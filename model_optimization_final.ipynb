{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAZYZgtHLqJm0jKYlO6v0z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/06unoh/model_optimization/blob/main/model_optimization_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx onnxscript onnxruntime\n",
        "\n",
        "# ONNX Runtime C++ 바이너리 다운로드\n",
        "!wget https://github.com/microsoft/onnxruntime/releases/download/v1.23.2/onnxruntime-linux-x64-1.23.2.tgz\n",
        "!tar -xvf onnxruntime-linux-x64-1.23.2.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fX8dLUFBEeb",
        "outputId": "f82b2857-300f-48db-a8be-305ff571b3c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting onnxscript\n",
            "  Downloading onnxscript-0.5.6-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.4)\n",
            "Collecting onnx_ir<2,>=0.1.12 (from onnxscript)\n",
            "  Downloading onnx_ir-0.1.12-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxscript) (25.0)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.9.23)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.14.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxscript-0.5.6-py3-none-any.whl (683 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m683.0/683.0 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx_ir-0.1.12-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, onnx, coloredlogs, onnxruntime, onnx_ir, onnxscript\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.19.1 onnx_ir-0.1.12 onnxruntime-1.23.2 onnxscript-0.5.6\n",
            "--2025-12-01 04:01:12--  https://github.com/microsoft/onnxruntime/releases/download/v1.23.2/onnxruntime-linux-x64-1.23.2.tgz\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/156939672/765f90c2-6271-4300-b1fa-fe488d5e2236?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-01T04%3A57%3A19Z&rscd=attachment%3B+filename%3Donnxruntime-linux-x64-1.23.2.tgz&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-01T03%3A56%3A45Z&ske=2025-12-01T04%3A57%3A19Z&sks=b&skv=2018-11-09&sig=76o%2FHSrQioM6ZVcic8HPu9JOAsW8N9Sj2DKJZ6hnm3c%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NDU2MTk3MywibmJmIjoxNzY0NTYxNjczLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.wk85JpOekXsyLiB_cN6oCrJ_l5uUP6xLWEcVlG6of6o&response-content-disposition=attachment%3B%20filename%3Donnxruntime-linux-x64-1.23.2.tgz&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-12-01 04:01:13--  https://release-assets.githubusercontent.com/github-production-release-asset/156939672/765f90c2-6271-4300-b1fa-fe488d5e2236?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-01T04%3A57%3A19Z&rscd=attachment%3B+filename%3Donnxruntime-linux-x64-1.23.2.tgz&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-01T03%3A56%3A45Z&ske=2025-12-01T04%3A57%3A19Z&sks=b&skv=2018-11-09&sig=76o%2FHSrQioM6ZVcic8HPu9JOAsW8N9Sj2DKJZ6hnm3c%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NDU2MTk3MywibmJmIjoxNzY0NTYxNjczLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.wk85JpOekXsyLiB_cN6oCrJ_l5uUP6xLWEcVlG6of6o&response-content-disposition=attachment%3B%20filename%3Donnxruntime-linux-x64-1.23.2.tgz&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8309231 (7.9M) [application/octet-stream]\n",
            "Saving to: ‘onnxruntime-linux-x64-1.23.2.tgz’\n",
            "\n",
            "onnxruntime-linux-x 100%[===================>]   7.92M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-12-01 04:01:13 (136 MB/s) - ‘onnxruntime-linux-x64-1.23.2.tgz’ saved [8309231/8309231]\n",
            "\n",
            "onnxruntime-linux-x64-1.23.2/\n",
            "onnxruntime-linux-x64-1.23.2/include/\n",
            "onnxruntime-linux-x64-1.23.2/include/onnxruntime_cxx_api.h\n",
            "onnxruntime-linux-x64-1.23.2/include/core/\n",
            "onnxruntime-linux-x64-1.23.2/include/core/providers/\n",
            "onnxruntime-linux-x64-1.23.2/include/core/providers/resource.h\n",
            "onnxruntime-linux-x64-1.23.2/include/core/providers/custom_op_context.h\n",
            "onnxruntime-linux-x64-1.23.2/include/onnxruntime_lite_custom_op.h\n",
            "onnxruntime-linux-x64-1.23.2/include/onnxruntime_session_options_config_keys.h\n",
            "onnxruntime-linux-x64-1.23.2/include/onnxruntime_c_api.h\n",
            "onnxruntime-linux-x64-1.23.2/include/onnxruntime_float16.h\n",
            "onnxruntime-linux-x64-1.23.2/include/onnxruntime_ep_c_api.h\n",
            "onnxruntime-linux-x64-1.23.2/include/provider_options.h\n",
            "onnxruntime-linux-x64-1.23.2/include/cpu_provider_factory.h\n",
            "onnxruntime-linux-x64-1.23.2/include/onnxruntime_ep_device_ep_metadata_keys.h\n",
            "onnxruntime-linux-x64-1.23.2/include/onnxruntime_run_options_config_keys.h\n",
            "onnxruntime-linux-x64-1.23.2/include/onnxruntime_cxx_inline.h\n",
            "onnxruntime-linux-x64-1.23.2/README.md\n",
            "onnxruntime-linux-x64-1.23.2/GIT_COMMIT_ID\n",
            "onnxruntime-linux-x64-1.23.2/Privacy.md\n",
            "onnxruntime-linux-x64-1.23.2/ThirdPartyNotices.txt\n",
            "onnxruntime-linux-x64-1.23.2/VERSION_NUMBER\n",
            "onnxruntime-linux-x64-1.23.2/LICENSE\n",
            "onnxruntime-linux-x64-1.23.2/lib/\n",
            "onnxruntime-linux-x64-1.23.2/lib/libonnxruntime.so.1\n",
            "onnxruntime-linux-x64-1.23.2/lib/libonnxruntime.so.1.23.2\n",
            "onnxruntime-linux-x64-1.23.2/lib/libonnxruntime_providers_shared.so\n",
            "onnxruntime-linux-x64-1.23.2/lib/libonnxruntime.so\n",
            "onnxruntime-linux-x64-1.23.2/lib/cmake/\n",
            "onnxruntime-linux-x64-1.23.2/lib/cmake/onnxruntime/\n",
            "onnxruntime-linux-x64-1.23.2/lib/cmake/onnxruntime/onnxruntimeTargets-release.cmake\n",
            "onnxruntime-linux-x64-1.23.2/lib/cmake/onnxruntime/onnxruntimeConfigVersion.cmake\n",
            "onnxruntime-linux-x64-1.23.2/lib/cmake/onnxruntime/onnxruntimeConfig.cmake\n",
            "onnxruntime-linux-x64-1.23.2/lib/cmake/onnxruntime/onnxruntimeTargets.cmake\n",
            "onnxruntime-linux-x64-1.23.2/lib/pkgconfig/\n",
            "onnxruntime-linux-x64-1.23.2/lib/pkgconfig/libonnxruntime.pc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OLTY_aSaA2jS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import onnx\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 학습 SwinIR Class\n",
        "def window_partition(x, window_size):\n",
        "  B, H, W, C=x.shape\n",
        "  x=x.view(B, H//window_size, window_size, W//window_size, window_size, -1)\n",
        "  windows=x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
        "  return windows\n",
        "\n",
        "def window_reverse(window, window_size, H, W):\n",
        "  B=int(window.shape[0]*window_size*window_size/(H*W))\n",
        "  x=window.view(B, H//window_size, W//window_size, window_size, window_size, -1)\n",
        "  x=x.permute(0,1,3,2,4,5).contiguous().view(B, H, W, -1)\n",
        "  return x\n",
        "\n",
        "def get_attn_mask(window_size, shift_size, H, W, device):\n",
        "  img_mask=torch.zeros((1, H, W, 1), device=device)\n",
        "  cnt=0\n",
        "\n",
        "  for h in (slice(-window_size),slice(-window_size,-shift_size),slice(-shift_size,None)):\n",
        "    for w in (slice(-window_size),slice(-window_size,-shift_size),slice(-shift_size,None)):\n",
        "      img_mask[:, h, w, :]=cnt\n",
        "      cnt+=1\n",
        "  img_window=window_partition(img_mask, window_size)\n",
        "  img_window=img_window.view(img_window.shape[0], -1)\n",
        "  attn_mask=img_window.unsqueeze(1)-img_window.unsqueeze(2)\n",
        "  attn_mask=attn_mask.masked_fill(attn_mask!=0, float(-100.0)).masked_fill(attn_mask==0, float(0.0))\n",
        "  return attn_mask\n",
        "\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "  def __init__(self, dim, num_heads, window_size):\n",
        "    super().__init__()\n",
        "    self.dim=dim\n",
        "    self.n_heads=num_heads\n",
        "    self.window_size=window_size\n",
        "    self.scale=(dim/self.n_heads)**-0.5\n",
        "\n",
        "    self.qkv=nn.Linear(dim, 3*dim)\n",
        "    self.proj=nn.Linear(dim, dim)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    B_, N, C=x.shape\n",
        "    qkv=self.qkv(x).view(B_, N, 3, self.n_heads,C//self.n_heads)\n",
        "    q, k, v=qkv.permute(2,0,3,1,4).contiguous()\n",
        "    attn=(q@k.transpose(-2, -1))*self.scale\n",
        "\n",
        "    if mask is not None:\n",
        "      nW=mask.shape[0]\n",
        "      attn=attn.view(B_//nW, nW, self.n_heads, N, N)\n",
        "      attn=attn+mask.unsqueeze(1).unsqueeze(0)\n",
        "      attn=attn.view(B_,self.n_heads,N,N)\n",
        "    attn=attn.softmax(dim=-1)\n",
        "    attn=(attn@v).permute(0,2,1,3).contiguous().view(B_, N, C)\n",
        "    return self.proj(attn)\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "  def __init__(self, drop_prob):\n",
        "    super().__init__()\n",
        "    self.drop_prob=drop_prob\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.drop_prob==0 or not self.training:\n",
        "      return x\n",
        "\n",
        "    keep_drop=1-self.drop_prob\n",
        "    B=x.shape[0]\n",
        "    shape=(B,)+(1,)*(x.ndim-1)\n",
        "    random_tensor=torch.rand(shape, dtype=x.dtype, device=x.device)+keep_drop\n",
        "    random_tensor.floor_()\n",
        "    return x.div(keep_drop)*random_tensor\n",
        "\n",
        "class SwinTFBlock(nn.Module):\n",
        "  def __init__(self, dim, num_heads, window_size, shift_size ,drop_prob=0.1):\n",
        "    super().__init__()\n",
        "    self.window_size=window_size\n",
        "    self.shift_size=shift_size\n",
        "\n",
        "    self.norm1=nn.LayerNorm(dim)\n",
        "    self.attn=WindowAttention(dim, num_heads, window_size)\n",
        "    self.drop_path1=DropPath(drop_prob)\n",
        "\n",
        "    self.norm2=nn.LayerNorm(dim)\n",
        "    self.mlp=nn.Sequential(\n",
        "        nn.Linear(dim, dim*4),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(dim*4, dim)\n",
        "    )\n",
        "    self.drop_path2=DropPath(drop_prob)\n",
        "\n",
        "  def forward(self, x, H, W):\n",
        "    B, N, C=x.shape\n",
        "    shortcut=x\n",
        "    x=self.norm1(x).view(x.shape[0], H, W, -1)\n",
        "\n",
        "    if self.shift_size>0:\n",
        "      x=torch.roll(x, shifts=(-self.shift_size, -self.shift_size),dims=(1,2))\n",
        "\n",
        "    x_windows=window_partition(x, self.window_size)\n",
        "    x_windows=x_windows.view(-1, self.window_size*self.window_size, C)    #(B_, N, C)\n",
        "\n",
        "    attn_mask=get_attn_mask(self.window_size, self.shift_size, H, W, x.device) if self.shift_size>0 else None\n",
        "    x_attn=self.attn(x_windows, mask=attn_mask)   #(B_,N,C)\n",
        "\n",
        "    x_attn=x_attn.view(-1,self.window_size, self.window_size, C)\n",
        "    x=window_reverse(x_attn, self.window_size, H, W)   # (B, H, W ,C)\n",
        "\n",
        "    if self.shift_size>0:\n",
        "      x=torch.roll(x, shifts=(self.shift_size, self.shift_size),dims=(1,2))\n",
        "\n",
        "    x=x.view(-1, H*W, C)\n",
        "    x=shortcut+self.drop_path1(x)\n",
        "    x=x+self.drop_path2(self.mlp(self.norm2(x)))\n",
        "    return x\n",
        "\n",
        "class RSTB(nn.Module):\n",
        "  def __init__(self, dim, num_heads, window_size, drop_prob, depth):\n",
        "    super().__init__()\n",
        "    self.blocks=nn.ModuleList([\n",
        "        SwinTFBlock(\n",
        "            dim,\n",
        "            num_heads,\n",
        "            window_size,\n",
        "            shift_size=0 if i%2==0 else window_size//2,\n",
        "            drop_prob=0.1)\n",
        "        for i in range(depth)\n",
        "    ])\n",
        "    self.conv=nn.Conv2d(dim, dim, 3, 1, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, H, W, C=x.shape\n",
        "    shortcut=x\n",
        "\n",
        "    x=x.view(B,H*W,C)\n",
        "    for blk in self.blocks:\n",
        "      x=blk(x, H, W)\n",
        "    x=x.view(B, H, W, C).permute(0,3,1,2).contiguous()\n",
        "    x=self.conv(x)\n",
        "    x=x.permute(0,2,3,1).contiguous()\n",
        "    x=shortcut+x\n",
        "    return x\n",
        "\n",
        "class SwinIR(nn.Module):\n",
        "  def __init__(self,img_dim=3, embed_dim=256, num_heads=8, window_size=8, drop_prob=0.1, depth=4, depths=3):\n",
        "    super().__init__()\n",
        "    self.conv_first=nn.Conv2d(img_dim, embed_dim, 3, 1, 1)\n",
        "    self.layers=nn.ModuleList([\n",
        "        RSTB(embed_dim, num_heads, window_size, drop_prob, depth)\n",
        "        for _ in range(depths)\n",
        "    ])\n",
        "    self.norm=nn.LayerNorm(embed_dim)\n",
        "    self.conv_after_body=nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n",
        "\n",
        "    self.upsample=nn.Sequential(\n",
        "        nn.Conv2d(embed_dim, embed_dim*4, 3, 1, 1),\n",
        "        nn.PixelShuffle(2),\n",
        "        nn.Conv2d(embed_dim, embed_dim*4, 3, 1, 1),\n",
        "        nn.PixelShuffle(2),\n",
        "        nn.Conv2d(embed_dim, img_dim, 3, 1, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, C, H, W=x.shape\n",
        "\n",
        "    x=self.conv_first(x)\n",
        "    x=x.permute(0,2,3,1).contiguous()\n",
        "\n",
        "    for layer in self.layers:   # 좋은데 기본 트랜스포머 구조 (B,L,C)\n",
        "      x=layer(x)\n",
        "\n",
        "    x=self.norm(x)\n",
        "    x=x.permute(0,3,1,2).contiguous()\n",
        "    x=self.conv_after_body(x)\n",
        "    return self.upsample(x)"
      ],
      "metadata": {
        "id": "skJQoXkLA6us"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ONNX Formatting\n",
        "model=SwinIR()\n",
        "checkpoint=torch.load('swinir_best.pth', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(checkpoint['model_state_dict'], strict=True)\n",
        "model.eval()\n",
        "\n",
        "dummy=torch.randn(1, 3, 64, 64)\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy,\n",
        "    'swinir_x4.onnx',\n",
        "    input_names=['input'],\n",
        "    output_names=['output'],\n",
        "    dynamic_axes={\n",
        "        \"input\": {0: \"batch\", 2: \"height\", 3: \"width\"},\n",
        "        \"output\": {0: \"batch\", 2: \"height\", 3: \"width\"},\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcAjc7s9A6sW",
        "outputId": "c2a257df-d5e0-4f90-f900-cd014d525cdb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-69745941.py:9: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
            "  torch.onnx.export(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.onnx] Obtain model graph for `SwinIR([...]` with `torch.export.export(..., strict=False)`...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-687436260.py:9: DeprecationWarning: The delegation of int() to __trunc__ is deprecated.\n",
            "  B=int(window.shape[0]*window_size*window_size/(H*W))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.onnx] Obtain model graph for `SwinIR([...]` with `torch.export.export(..., strict=False)`... ✅\n",
            "[torch.onnx] Run decomposition...\n",
            "[torch.onnx] Run decomposition... ✅\n",
            "[torch.onnx] Translate the graph into ONNX...\n",
            "[torch.onnx] Translate the graph into ONNX... ✅\n",
            "Applied 168 of general pattern rewrite rules.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ONNXProgram(\n",
              "    model=\n",
              "        <\n",
              "            ir_version=10,\n",
              "            opset_imports={'': 20},\n",
              "            producer_name='pytorch',\n",
              "            producer_version='2.9.0+cu126',\n",
              "            domain=None,\n",
              "            model_version=None,\n",
              "        >\n",
              "        graph(\n",
              "            name=main_graph,\n",
              "            inputs=(\n",
              "                %\"input\"<FLOAT,[s77,3,s53,s0]>\n",
              "            ),\n",
              "            outputs=(\n",
              "                %\"output\"<FLOAT,[1,3,4*s53,4*s0]>\n",
              "            ),\n",
              "            initializers=(\n",
              "                %\"conv_first.weight\"<FLOAT,[256,3,3,3]>{TorchTensor(...)},\n",
              "                %\"conv_first.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.0.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.0.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.0.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.0.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.0.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.0.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.0.mlp.0.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.0.mlp.2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.1.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.1.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.1.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.1.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.1.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.1.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.1.mlp.0.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.1.mlp.2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.2.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.2.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.2.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.2.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.2.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.2.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.2.mlp.0.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.2.mlp.2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.3.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.3.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.3.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.3.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.3.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.3.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.3.mlp.0.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
              "                %\"layers.0.blocks.3.mlp.2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.0.conv.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
              "                %\"layers.0.conv.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.0.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.0.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.0.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.0.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.0.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.0.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.0.mlp.0.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.0.mlp.2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.1.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.1.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.1.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.1.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.1.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.1.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.1.mlp.0.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.1.mlp.2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.2.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.2.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.2.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.2.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.2.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.2.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.2.mlp.0.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.2.mlp.2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.3.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.3.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.3.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.3.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.3.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.3.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.3.mlp.0.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
              "                %\"layers.1.blocks.3.mlp.2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.1.conv.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
              "                %\"layers.1.conv.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.0.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.0.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.0.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.0.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.0.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.0.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.0.mlp.0.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.0.mlp.2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.1.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.1.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.1.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.1.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.1.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.1.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.1.mlp.0.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.1.mlp.2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.2.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.2.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.2.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.2.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.2.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.2.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.2.mlp.0.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.2.mlp.2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.3.norm1.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.3.norm1.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.3.attn.qkv.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.3.attn.proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.3.norm2.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.3.norm2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.3.mlp.0.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
              "                %\"layers.2.blocks.3.mlp.2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"layers.2.conv.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
              "                %\"layers.2.conv.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"conv_after_body.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
              "                %\"conv_after_body.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
              "                %\"upsample.0.weight\"<FLOAT,[1024,256,3,3]>{TorchTensor(...)},\n",
              "                %\"upsample.0.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
              "                %\"upsample.2.weight\"<FLOAT,[1024,256,3,3]>{TorchTensor(...)},\n",
              "                %\"upsample.2.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
              "                %\"upsample.4.weight\"<FLOAT,[3,256,3,3]>{TorchTensor(...)},\n",
              "                %\"upsample.4.bias\"<FLOAT,[3]>{TorchTensor<FLOAT,[3]>(Parameter containing: tensor([-0.0473, -0.0088,  0.0368], requires_grad=True), name='upsample.4.bias')},\n",
              "                %\"val_35\"<INT64,[3]>{Tensor<INT64,[3]>(array([ -1,  64, 256]), name='val_35')},\n",
              "                %\"val_36\"<FLOAT,[256,768]>{Tensor(...)},\n",
              "                %\"val_57\"<FLOAT,[256,256]>{Tensor(...)},\n",
              "                %\"val_64\"<INT64,[4]>{Tensor<INT64,[4]>(array([ -1,   8,   8, 256]), name='val_64')},\n",
              "                %\"val_86\"<FLOAT,[256,1024]>{Tensor(...)},\n",
              "                %\"val_88\"<FLOAT,[1024,256]>{Tensor(...)},\n",
              "                %\"val_144\"<INT64,[1]>{Tensor<INT64,[1]>(array([0]), name='val_144')},\n",
              "                %\"val_148\"<INT64,[1]>{Tensor<INT64,[1]>(array([-8]), name='val_148')},\n",
              "                %\"val_152\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_152')},\n",
              "                %\"val_163\"<INT64,[1]>{Tensor<INT64,[1]>(array([2]), name='val_163')},\n",
              "                %\"value_0\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0., dtype=float32), name='value_0')},\n",
              "                %\"val_216\"<INT64,[1]>{Tensor<INT64,[1]>(array([-4]), name='val_216')},\n",
              "                %\"value_0_2\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name='value_0_2')},\n",
              "                %\"val_271\"<INT64,[1]>{Tensor<INT64,[1]>(array([9223372036854775807]), name='val_271')},\n",
              "                %\"value_0_3\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(2., dtype=float32), name='value_0_3')},\n",
              "                %\"value_0_4\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(3., dtype=float32), name='value_0_4')},\n",
              "                %\"val_403\"<FLOAT,[4,4,1,1]>{Tensor(...)},\n",
              "                %\"val_457\"<FLOAT,[4,4,1,1]>{Tensor(...)},\n",
              "                %\"value_0_7\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(6., dtype=float32), name='value_0_7')},\n",
              "                %\"val_565\"<FLOAT,[4,4,1,1]>{Tensor(...)},\n",
              "                %\"val_619\"<FLOAT,[4,4,1,1]>{Tensor(...)},\n",
              "                %\"val_647\"<INT64,[4]>{Tensor<INT64,[4]>(array([-1,  8,  8,  1]), name='val_647')},\n",
              "                %\"val_657\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(-100., dtype=float32), name='val_657')},\n",
              "                %\"val_660\"<FLOAT,[256,768]>{Tensor(...)},\n",
              "                %\"val_690\"<FLOAT,[256,256]>{Tensor(...)},\n",
              "                %\"val_740\"<FLOAT,[256,1024]>{Tensor(...)},\n",
              "                %\"val_742\"<FLOAT,[1024,256]>{Tensor(...)},\n",
              "                %\"val_771\"<FLOAT,[256,768]>{Tensor(...)},\n",
              "                %\"val_788\"<FLOAT,[256,256]>{Tensor(...)},\n",
              "                %\"val_817\"<FLOAT,[256,1024]>{Tensor(...)},\n",
              "                %\"val_819\"<FLOAT,[1024,256]>{Tensor(...)},\n",
              "                %\"val_1379\"<FLOAT,[256,768]>{Tensor(...)},\n",
              "                %\"val_1409\"<FLOAT,[256,256]>{Tensor(...)},\n",
              "                %\"val_1459\"<FLOAT,[256,1024]>{Tensor(...)},\n",
              "                %\"val_1461\"<FLOAT,[1024,256]>{Tensor(...)},\n",
              "                %\"val_1501\"<FLOAT,[256,768]>{Tensor(...)},\n",
              "                %\"val_1518\"<FLOAT,[256,256]>{Tensor(...)},\n",
              "                %\"val_1547\"<FLOAT,[256,1024]>{Tensor(...)},\n",
              "                %\"val_1549\"<FLOAT,[1024,256]>{Tensor(...)},\n",
              "                %\"val_2109\"<FLOAT,[256,768]>{Tensor(...)},\n",
              "                %\"val_2139\"<FLOAT,[256,256]>{Tensor(...)},\n",
              "                %\"val_2189\"<FLOAT,[256,1024]>{Tensor(...)},\n",
              "                %\"val_2191\"<FLOAT,[1024,256]>{Tensor(...)},\n",
              "                %\"val_2220\"<FLOAT,[256,768]>{Tensor(...)},\n",
              "                %\"val_2237\"<FLOAT,[256,256]>{Tensor(...)},\n",
              "                %\"val_2266\"<FLOAT,[256,1024]>{Tensor(...)},\n",
              "                %\"val_2268\"<FLOAT,[1024,256]>{Tensor(...)},\n",
              "                %\"val_2828\"<FLOAT,[256,768]>{Tensor(...)},\n",
              "                %\"val_2858\"<FLOAT,[256,256]>{Tensor(...)},\n",
              "                %\"val_2908\"<FLOAT,[256,1024]>{Tensor(...)},\n",
              "                %\"val_2910\"<FLOAT,[1024,256]>{Tensor(...)},\n",
              "                %\"val_2950\"<FLOAT,[256,768]>{Tensor(...)},\n",
              "                %\"val_2967\"<FLOAT,[256,256]>{Tensor(...)},\n",
              "                %\"val_2996\"<FLOAT,[256,1024]>{Tensor(...)},\n",
              "                %\"val_2998\"<FLOAT,[1024,256]>{Tensor(...)},\n",
              "                %\"val_3558\"<FLOAT,[256,768]>{Tensor(...)},\n",
              "                %\"val_3588\"<FLOAT,[256,256]>{Tensor(...)},\n",
              "                %\"val_3638\"<FLOAT,[256,1024]>{Tensor(...)},\n",
              "                %\"val_3640\"<FLOAT,[1024,256]>{Tensor(...)},\n",
              "                %\"val_3669\"<FLOAT,[256,768]>{Tensor(...)},\n",
              "                %\"val_3686\"<FLOAT,[256,256]>{Tensor(...)},\n",
              "                %\"val_3715\"<FLOAT,[256,1024]>{Tensor(...)},\n",
              "                %\"val_3717\"<FLOAT,[1024,256]>{Tensor(...)},\n",
              "                %\"val_4277\"<FLOAT,[256,768]>{Tensor(...)},\n",
              "                %\"val_4307\"<FLOAT,[256,256]>{Tensor(...)},\n",
              "                %\"val_4357\"<FLOAT,[256,1024]>{Tensor(...)},\n",
              "                %\"val_4359\"<FLOAT,[1024,256]>{Tensor(...)},\n",
              "                %\"val_3\"<INT64,[1]>{Tensor<INT64,[1]>(array([-1]), name='val_3')},\n",
              "                %\"val_6\"<INT64,[1]>{Tensor<INT64,[1]>(array([256]), name='val_6')},\n",
              "                %\"val_16\"<INT64,[]>{Tensor<INT64,[]>(array(8), name='val_16')},\n",
              "                %\"val_20\"<INT64,[1]>{Tensor<INT64,[1]>(array([8]), name='val_20')},\n",
              "                %\"val_38\"<INT64,[]>{Tensor<INT64,[]>(array(4), name='val_38')},\n",
              "                %\"val_39\"<INT64,[]>{Tensor<INT64,[]>(array(256), name='val_39')},\n",
              "                %\"val_42\"<INT64,[1]>{Tensor<INT64,[1]>(array([64]), name='val_42')},\n",
              "                %\"val_43\"<INT64,[1]>{Tensor<INT64,[1]>(array([3]), name='val_43')},\n",
              "                %\"val_45\"<INT64,[1]>{Tensor<INT64,[1]>(array([32]), name='val_45')},\n",
              "                %\"val_51\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.17677669, dtype=float32), name='val_51')},\n",
              "                %\"val_100\"<INT64,[1]>{Tensor<INT64,[1]>(array([4]), name='val_100')},\n",
              "                %\"val_141\"<INT64,[]>{Tensor<INT64,[]>(array(0), name='val_141')},\n",
              "                %\"val_149\"<INT64,[]>{Tensor<INT64,[]>(array(1), name='val_149')},\n",
              "                %\"val_160\"<INT64,[]>{Tensor<INT64,[]>(array(2), name='val_160')},\n",
              "                %\"val_648\"<INT64,[]>{Tensor<INT64,[]>(array(64), name='val_648')},\n",
              "                %\"val_4369\"<INT64,[2]>{Tensor<INT64,[2]>(array([0, 2]), name='val_4369')}\n",
              "            ),\n",
              "        ) {\n",
              "              0 |  # node_Shape_0\n",
              "                   %\"val_0\"<INT64,[1]> ⬅️ ::Shape(%\"input\") {end=1, start=0}\n",
              "              1 |  # node_sym_size_int_317\n",
              "                   %\"sym_size_int_317\"<INT64,[]> ⬅️ ::Squeeze(%\"val_0\")\n",
              "              2 |  # node_Shape_1\n",
              "                   %\"val_1\"<INT64,[1]> ⬅️ ::Shape(%\"input\") {end=3, start=2}\n",
              "              3 |  # node_sym_size_int_318\n",
              "                   %\"sym_size_int_318\"<INT64,[]> ⬅️ ::Squeeze(%\"val_1\")\n",
              "              4 |  # node_Shape_2\n",
              "                   %\"val_2\"<INT64,[1]> ⬅️ ::Shape(%\"input\") {end=4, start=3}\n",
              "              5 |  # node_sym_size_int_319\n",
              "                   %\"sym_size_int_319\"<INT64,[]> ⬅️ ::Squeeze(%\"val_2\")\n",
              "              6 |  # node_conv2d\n",
              "                   %\"conv2d\"<FLOAT,[s77,256,s53,s0]> ⬅️ ::Conv(%\"input\", %\"conv_first.weight\"{...}, %\"conv_first.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
              "              7 |  # node_permute\n",
              "                   %\"permute\"<FLOAT,[s77,s53,s0,256]> ⬅️ ::Transpose(%\"conv2d\") {perm=(0, 2, 3, 1)}\n",
              "              8 |  # node_mul_471\n",
              "                   %\"mul_471\"<INT64,[]> ⬅️ ::Mul(%\"sym_size_int_318\", %\"sym_size_int_319\")\n",
              "              9 |  # node_Reshape_5\n",
              "                   %\"val_5\"<INT64,[1]> ⬅️ ::Reshape(%\"mul_471\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "             10 |  # node_Concat_7\n",
              "                   %\"val_7\"<INT64,[3]> ⬅️ ::Concat(%\"val_0\", %\"val_5\", %\"val_6\"{[256]}) {axis=0}\n",
              "             11 |  # node_view\n",
              "                   %\"view\"<FLOAT,[s77,s0*s53,256]> ⬅️ ::Reshape(%\"permute\", %\"val_7\") {allowzero=1}\n",
              "             12 |  # node_layer_norm\n",
              "                   %\"layer_norm\"<FLOAT,[s77,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"view\", %\"layers.0.blocks.0.norm1.weight\"{...}, %\"layers.0.blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "             13 |  # node_Concat_13\n",
              "                   %\"val_15\"<INT64,[4]> ⬅️ ::Concat(%\"val_0\", %\"val_1\", %\"val_2\", %\"val_3\"{[-1]}) {axis=0}\n",
              "             14 |  # node_view_1\n",
              "                   %\"view_1\"<FLOAT,[s77,s53,s0,256]> ⬅️ ::Reshape(%\"layer_norm\", %\"val_15\") {allowzero=1}\n",
              "             15 |  # node_floordiv_388\n",
              "                   %\"floordiv_388\"<INT64,[]> ⬅️ ::Div(%\"sym_size_int_318\", %\"val_16\"{8})\n",
              "             16 |  # node_floordiv_389\n",
              "                   %\"floordiv_389\"<INT64,[]> ⬅️ ::Div(%\"sym_size_int_319\", %\"val_16\"{8})\n",
              "             17 |  # node_Reshape_17\n",
              "                   %\"val_19\"<INT64,[1]> ⬅️ ::Reshape(%\"floordiv_388\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "             18 |  # node_Reshape_19\n",
              "                   %\"val_21\"<INT64,[1]> ⬅️ ::Reshape(%\"floordiv_389\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "             19 |  # node_Concat_22\n",
              "                   %\"val_24\"<INT64,[6]> ⬅️ ::Concat(%\"val_0\", %\"val_19\", %\"val_20\"{[8]}, %\"val_21\", %\"val_20\"{[8]}, %\"val_3\"{[-1]}) {axis=0}\n",
              "             20 |  # node_view_2\n",
              "                   %\"view_2\"<FLOAT,[s77,(s53//8),(s53//((s53//8))),(s0//8),(s0//((s0//8))),256]> ⬅️ ::Reshape(%\"view_1\", %\"val_24\") {allowzero=1}\n",
              "             21 |  # node_permute_1\n",
              "                   %\"permute_1\"<FLOAT,[s77,(s53//8),(s0//8),(s53//((s53//8))),(s0//((s0//8))),256]> ⬅️ ::Transpose(%\"view_2\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "             22 |  # node_Reshape_4876\n",
              "                   %\"view_4\"<FLOAT,[((s0//8))*((s53//8)),64,256]> ⬅️ ::Reshape(%\"permute_1\", %\"val_35\"{[-1, 64, 256]})\n",
              "             23 |  # node_MatMul_35\n",
              "                   %\"val_37\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::MatMul(%\"view_4\", %\"val_36\"{...})\n",
              "             24 |  # node_linear\n",
              "                   %\"linear\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::Add(%\"val_37\", %\"layers.0.blocks.0.attn.qkv.bias\"{...})\n",
              "             25 |  # node_mul_523\n",
              "                   %\"mul_523\"<INT64,[]> ⬅️ ::Mul(%\"val_38\"{4}, %\"sym_size_int_319\")\n",
              "             26 |  # node_mul_524\n",
              "                   %\"mul_524\"<INT64,[]> ⬅️ ::Mul(%\"mul_523\", %\"sym_size_int_318\")\n",
              "             27 |  # node_mul_525\n",
              "                   %\"mul_525\"<INT64,[]> ⬅️ ::Mul(%\"floordiv_389\", %\"floordiv_388\")\n",
              "             28 |  # node_floordiv_390\n",
              "                   %\"floordiv_390\"<INT64,[]> ⬅️ ::Div(%\"mul_524\", %\"mul_525\")\n",
              "             29 |  # node_mul_526\n",
              "                   %\"mul_526\"<INT64,[]> ⬅️ ::Mul(%\"sym_size_int_317\", %\"floordiv_389\")\n",
              "             30 |  # node_mul_527\n",
              "                   %\"mul_527\"<INT64,[]> ⬅️ ::Mul(%\"mul_526\", %\"floordiv_388\")\n",
              "             31 |  # node_mul_528\n",
              "                   %\"mul_528\"<INT64,[]> ⬅️ ::Mul(%\"mul_527\", %\"floordiv_390\")\n",
              "             32 |  # node_floordiv_391\n",
              "                   %\"floordiv_391\"<INT64,[]> ⬅️ ::Div(%\"mul_528\", %\"val_39\"{256})\n",
              "             33 |  # node_Reshape_39\n",
              "                   %\"val_41\"<INT64,[1]> ⬅️ ::Reshape(%\"floordiv_391\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "             34 |  # node_Concat_44\n",
              "                   %\"val_46\"<INT64,[5]> ⬅️ ::Concat(%\"val_41\", %\"val_42\"{[64]}, %\"val_43\"{[3]}, %\"val_20\"{[8]}, %\"val_45\"{[32]}) {axis=0}\n",
              "             35 |  # node_view_5\n",
              "                   %\"view_5\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),3,8,32]> ⬅️ ::Reshape(%\"linear\", %\"val_46\") {allowzero=1}\n",
              "             36 |  # node_permute_2\n",
              "                   %\"permute_2\"<FLOAT,[3,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Transpose(%\"view_5\") {perm=(2, 0, 3, 1, 4)}\n",
              "             37 |  # node_Split_45\n",
              "                   %\"val_47\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_48\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_49\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Split(%\"permute_2\") {num_outputs=3, axis=0}\n",
              "             38 |  # node_unbind__0\n",
              "                   %\"getitem\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_47\", %\"val_144\"{[0]})\n",
              "             39 |  # node_unbind__1\n",
              "                   %\"getitem_1\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_48\", %\"val_144\"{[0]})\n",
              "             40 |  # node_unbind__2\n",
              "                   %\"getitem_2\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_49\", %\"val_144\"{[0]})\n",
              "             41 |  # node_transpose\n",
              "                   %\"transpose\"<FLOAT,[((s0//8))*((s53//8)),8,32,((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Transpose(%\"getitem_1\") {perm=(0, 1, 3, 2)}\n",
              "             42 |  # node_matmul\n",
              "                   %\"matmul\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::MatMul(%\"getitem\", %\"transpose\")\n",
              "             43 |  # node_mul_568\n",
              "                   %\"mul_568\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Mul(%\"matmul\", %\"val_51\"{0.1767766922712326})\n",
              "             44 |  # node_softmax\n",
              "                   %\"softmax\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Softmax(%\"mul_568\") {axis=-1}\n",
              "             45 |  # node_matmul_1\n",
              "                   %\"matmul_1\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::MatMul(%\"softmax\", %\"getitem_2\")\n",
              "             46 |  # node_permute_3\n",
              "                   %\"permute_3\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),8,32]> ⬅️ ::Transpose(%\"matmul_1\") {perm=(0, 2, 1, 3)}\n",
              "             47 |  # node_Concat_52\n",
              "                   %\"val_56\"<INT64,[3]> ⬅️ ::Concat(%\"val_41\", %\"val_42\"{[64]}, %\"val_6\"{[256]}) {axis=0}\n",
              "             48 |  # node_view_6\n",
              "                   %\"view_6\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Reshape(%\"permute_3\", %\"val_56\") {allowzero=1}\n",
              "             49 |  # node_MatMul_54\n",
              "                   %\"val_58\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::MatMul(%\"view_6\", %\"val_57\"{...})\n",
              "             50 |  # node_linear_1\n",
              "                   %\"linear_1\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Add(%\"val_58\", %\"layers.0.blocks.0.attn.proj.bias\"{...})\n",
              "             51 |  # node_view_7\n",
              "                   %\"view_7\"<FLOAT,[((s0//8))*((s53//8)),8,8,256]> ⬅️ ::Reshape(%\"linear_1\", %\"val_64\"{[-1, 8, 8, 256]}) {allowzero=1}\n",
              "             52 |  # node_Concat_68\n",
              "                   %\"val_72\"<INT64,[6]> ⬅️ ::Concat(%\"val_152\"{[1]}, %\"val_19\", %\"val_21\", %\"val_20\"{[8]}, %\"val_20\"{[8]}, %\"val_3\"{[-1]}) {axis=0}\n",
              "             53 |  # node_view_8\n",
              "                   %\"view_8\"<FLOAT,[1,(s53//8),(s0//8),8,((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Reshape(%\"view_7\", %\"val_72\") {allowzero=1}\n",
              "             54 |  # node_permute_4\n",
              "                   %\"permute_4\"<FLOAT,[1,(s53//8),8,(s0//8),((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Transpose(%\"view_8\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "             55 |  # node_Concat_74\n",
              "                   %\"val_78\"<INT64,[4]> ⬅️ ::Concat(%\"val_152\"{[1]}, %\"val_1\", %\"val_2\", %\"val_3\"{[-1]}) {axis=0}\n",
              "             56 |  # node_view_9\n",
              "                   %\"view_9\"<FLOAT,[1,8*((s53//8)),((s0//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"permute_4\", %\"val_78\") {allowzero=1}\n",
              "             57 |  # node_Concat_79\n",
              "                   %\"val_83\"<INT64,[3]> ⬅️ ::Concat(%\"val_3\"{[-1]}, %\"val_5\", %\"val_6\"{[256]}) {axis=0}\n",
              "             58 |  # node_view_10\n",
              "                   %\"view_10\"<FLOAT,[1,8*((s0//8))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"view_9\", %\"val_83\") {allowzero=1}\n",
              "             59 |  # node_add_171\n",
              "                   %\"add_171\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"view\", %\"view_10\")\n",
              "             60 |  # node_layer_norm_1\n",
              "                   %\"layer_norm_1\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_171\", %\"layers.0.blocks.0.norm2.weight\"{...}, %\"layers.0.blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "             61 |  # node_MatMul_81\n",
              "                   %\"val_87\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::MatMul(%\"layer_norm_1\", %\"val_86\"{...})\n",
              "             62 |  # node_linear_2\n",
              "                   %\"linear_2\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Add(%\"val_87\", %\"layers.0.blocks.0.mlp.0.bias\"{...})\n",
              "             63 |  # node_gelu\n",
              "                   %\"gelu\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Gelu(%\"linear_2\") {approximate='none'}\n",
              "             64 |  # node_MatMul_83\n",
              "                   %\"val_89\"<FLOAT,[1,s0*s53,256]> ⬅️ ::MatMul(%\"gelu\", %\"val_88\"{...})\n",
              "             65 |  # node_linear_3\n",
              "                   %\"linear_3\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"val_89\", %\"layers.0.blocks.0.mlp.2.bias\"{...})\n",
              "             66 |  # node_add_187\n",
              "                   %\"add_187\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_171\", %\"linear_3\")\n",
              "             67 |  # node_layer_norm_2\n",
              "                   %\"layer_norm_2\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_187\", %\"layers.0.blocks.1.norm1.weight\"{...}, %\"layers.0.blocks.1.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "             68 |  # node_view_11\n",
              "                   %\"view_11\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Reshape(%\"layer_norm_2\", %\"val_15\") {allowzero=1}\n",
              "             69 |  # node_Slice_94\n",
              "                   %\"val_102\"<FLOAT,[1,None,s0,256]> ⬅️ ::Slice(%\"view_11\", %\"val_144\"{[0]}, %\"val_100\"{[4]}, %\"val_152\"{[1]})\n",
              "             70 |  # node_Size_95\n",
              "                   %\"val_103\"<INT64,[]> ⬅️ ::Size(%\"view_11\")\n",
              "             71 |  # node_Reshape_96\n",
              "                   %\"val_104\"<INT64,[1]> ⬅️ ::Reshape(%\"val_103\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "             72 |  # node_Slice_97\n",
              "                   %\"val_105\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"view_11\", %\"val_100\"{[4]}, %\"val_104\", %\"val_152\"{[1]})\n",
              "             73 |  # node_Concat_98\n",
              "                   %\"val_106\"<FLOAT,[1,None,s0,256]> ⬅️ ::Concat(%\"val_105\", %\"val_102\") {axis=1}\n",
              "             74 |  # node_Slice_103\n",
              "                   %\"val_111\"<FLOAT,[1,None,None,256]> ⬅️ ::Slice(%\"val_106\", %\"val_144\"{[0]}, %\"val_100\"{[4]}, %\"val_163\"{[2]})\n",
              "             75 |  # node_Size_104\n",
              "                   %\"val_112\"<INT64,[]> ⬅️ ::Size(%\"val_106\")\n",
              "             76 |  # node_Reshape_105\n",
              "                   %\"val_113\"<INT64,[1]> ⬅️ ::Reshape(%\"val_112\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "             77 |  # node_Slice_106\n",
              "                   %\"val_114\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"val_106\", %\"val_100\"{[4]}, %\"val_113\", %\"val_163\"{[2]})\n",
              "             78 |  # node_roll\n",
              "                   %\"roll\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Concat(%\"val_114\", %\"val_111\") {axis=2}\n",
              "             79 |  # node_view_12\n",
              "                   %\"view_12\"<FLOAT,[1,(s53//8),(s53//((s53//8))),(s0//8),(s0//((s0//8))),256]> ⬅️ ::Reshape(%\"roll\", %\"val_24\") {allowzero=1}\n",
              "             80 |  # node_permute_5\n",
              "                   %\"permute_5\"<FLOAT,[1,(s53//8),(s0//8),(s53//((s53//8))),(s0//((s0//8))),256]> ⬅️ ::Transpose(%\"view_12\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "             81 |  # node_Reshape_4883\n",
              "                   %\"view_14\"<FLOAT,[((s0//8))*((s53//8)),64,256]> ⬅️ ::Reshape(%\"permute_5\", %\"val_35\"{[-1, 64, 256]})\n",
              "             82 |  # node_Concat_132\n",
              "                   %\"val_140\"<INT64,[4]> ⬅️ ::Concat(%\"val_152\"{[1]}, %\"val_1\", %\"val_2\", %\"val_152\"{[1]}) {axis=0}\n",
              "             83 |  # node_zeros\n",
              "                   %\"zeros\"<FLOAT,[1,s53,s0,1]> ⬅️ ::Expand(%\"value_0\"{0.0}, %\"val_140\")\n",
              "             84 |  # node_slice_1\n",
              "                   %\"slice_1\"<FLOAT,[1,s53 - 8,s0,1]> ⬅️ ::Slice(%\"zeros\", %\"val_144\"{[0]}, %\"val_148\"{[-8]}, %\"val_152\"{[1]}, %\"val_152\"{[1]})\n",
              "             85 |  # node_slice_2\n",
              "                   %\"slice_2\"<FLOAT,[1,s53 - 8,s0 - 8,1]> ⬅️ ::Slice(%\"slice_1\", %\"val_144\"{[0]}, %\"val_148\"{[-8]}, %\"val_163\"{[2]}, %\"val_152\"{[1]})\n",
              "             86 |  # n1\n",
              "                   %\"shape\"<INT64,[4]> ⬅️ ::Shape(%\"slice_2\")\n",
              "             87 |  # n2\n",
              "                   %\"fill\"<FLOAT,[1,s53 - 8,s0 - 8,1]> ⬅️ ::Expand(%\"value_0\"{0.0}, %\"shape\")\n",
              "             88 |  # node_Shape_168\n",
              "                   %\"val_176\"<INT64,[4]> ⬅️ ::Shape(%\"slice_1\") {start=0}\n",
              "             89 |  # node_Gather_169\n",
              "                   %\"val_177\"<INT64,[]> ⬅️ ::Gather(%\"val_176\", %\"val_160\"{2}) {axis=0}\n",
              "             90 |  # node_Range_170\n",
              "                   %\"val_178\"<INT64,[None]> ⬅️ ::Range(%\"val_141\"{0}, %\"val_177\", %\"val_149\"{1})\n",
              "             91 |  # node_Slice_174\n",
              "                   %\"val_182\"<INT64,[None]> ⬅️ ::Slice(%\"val_178\", %\"val_144\"{[0]}, %\"val_148\"{[-8]}, %\"val_144\"{[0]}, %\"val_152\"{[1]})\n",
              "             92 |  # node_Unsqueeze_176\n",
              "                   %\"val_184\"<INT64,[None,1]> ⬅️ ::Unsqueeze(%\"val_182\", %\"val_3\"{[-1]})\n",
              "             93 |  # node_Transpose_177\n",
              "                   %\"val_185\"<FLOAT,[s0 - 8,s53 - 8,1,1]> ⬅️ ::Transpose(%\"fill\") {perm=(2, 1, 0, 3)}\n",
              "             94 |  # node_Transpose_178\n",
              "                   %\"val_186\"<FLOAT,[s0,s53 - 8,1,1]> ⬅️ ::Transpose(%\"slice_1\") {perm=(2, 1, 0, 3)}\n",
              "             95 |  # node_ScatterND_179\n",
              "                   %\"val_187\"<FLOAT,[s0,s53 - 8,1,1]> ⬅️ ::ScatterND(%\"val_186\", %\"val_184\", %\"val_185\") {reduction='none'}\n",
              "             96 |  # node_Shape_181\n",
              "                   %\"val_189\"<INT64,[4]> ⬅️ ::Shape(%\"zeros\") {start=0}\n",
              "             97 |  # node_Gather_182\n",
              "                   %\"val_190\"<INT64,[]> ⬅️ ::Gather(%\"val_189\", %\"val_149\"{1}) {axis=0}\n",
              "             98 |  # node_Range_183\n",
              "                   %\"val_191\"<INT64,[None]> ⬅️ ::Range(%\"val_141\"{0}, %\"val_190\", %\"val_149\"{1})\n",
              "             99 |  # node_Slice_187\n",
              "                   %\"val_195\"<INT64,[None]> ⬅️ ::Slice(%\"val_191\", %\"val_144\"{[0]}, %\"val_148\"{[-8]}, %\"val_144\"{[0]}, %\"val_152\"{[1]})\n",
              "            100 |  # node_Unsqueeze_188\n",
              "                   %\"val_196\"<INT64,[None,1]> ⬅️ ::Unsqueeze(%\"val_195\", %\"val_3\"{[-1]})\n",
              "            101 |  # node_Transpose_4886\n",
              "                   %\"val_197\"<FLOAT,[s53 - 8,1,s0,1]> ⬅️ ::Transpose(%\"val_187\") {perm=(1, 2, 0, 3)}\n",
              "            102 |  # node_Transpose_190\n",
              "                   %\"val_198\"<FLOAT,[s53,1,s0,1]> ⬅️ ::Transpose(%\"zeros\") {perm=(1, 0, 2, 3)}\n",
              "            103 |  # node_ScatterND_191\n",
              "                   %\"val_199\"<FLOAT,[s53,1,s0,1]> ⬅️ ::ScatterND(%\"val_198\", %\"val_196\", %\"val_197\") {reduction='none'}\n",
              "            104 |  # node_slice_scatter_1\n",
              "                   %\"slice_scatter_1\"<FLOAT,[1,s53,s0,1]> ⬅️ ::Transpose(%\"val_199\") {perm=(1, 0, 2, 3)}\n",
              "            105 |  # node_slice_8\n",
              "                   %\"slice_8\"<FLOAT,[1,s53 - 8,s0,1]> ⬅️ ::Slice(%\"slice_scatter_1\", %\"val_144\"{[0]}, %\"val_148\"{[-8]}, %\"val_152\"{[1]}, %\"val_152\"{[1]})\n",
              "            106 |  # node_slice_9\n",
              "                   %\"slice_9\"<FLOAT,[1,s53 - 8,4,1]> ⬅️ ::Slice(%\"slice_8\", %\"val_148\"{[-8]}, %\"val_216\"{[-4]}, %\"val_163\"{[2]}, %\"val_152\"{[1]})\n",
              "            107 |  # n1_2\n",
              "                   %\"shape_2\"<INT64,[4]> ⬅️ ::Shape(%\"slice_9\")\n",
              "            108 |  # n2_2\n",
              "                   %\"fill_1\"<FLOAT,[1,s53 - 8,4,1]> ⬅️ ::Expand(%\"value_0_2\"{1.0}, %\"shape_2\")\n",
              "            109 |  # node_Shape_224\n",
              "                   %\"val_232\"<INT64,[4]> ⬅️ ::Shape(%\"slice_8\") {start=0}\n",
              "            110 |  # node_Gather_225\n",
              "                   %\"val_233\"<INT64,[]> ⬅️ ::Gather(%\"val_232\", %\"val_160\"{2}) {axis=0}\n",
              "            111 |  # node_Range_226\n",
              "                   %\"val_234\"<INT64,[None]> ⬅️ ::Range(%\"val_141\"{0}, %\"val_233\", %\"val_149\"{1})\n",
              "            112 |  # node_Slice_230\n",
              "                   %\"val_238\"<INT64,[None]> ⬅️ ::Slice(%\"val_234\", %\"val_148\"{[-8]}, %\"val_216\"{[-4]}, %\"val_144\"{[0]}, %\"val_152\"{[1]})\n",
              "            113 |  # node_Unsqueeze_231\n",
              "                   %\"val_239\"<INT64,[None,1]> ⬅️ ::Unsqueeze(%\"val_238\", %\"val_3\"{[-1]})\n",
              "            114 |  # node_Transpose_232\n",
              "                   %\"val_240\"<FLOAT,[4,s53 - 8,1,1]> ⬅️ ::Transpose(%\"fill_1\") {perm=(2, 1, 0, 3)}\n",
              "            115 |  # node_Transpose_233\n",
              "                   %\"val_241\"<FLOAT,[s0,s53 - 8,1,1]> ⬅️ ::Transpose(%\"slice_8\") {perm=(2, 1, 0, 3)}\n",
              "            116 |  # node_ScatterND_234\n",
              "                   %\"val_242\"<FLOAT,[s0,s53 - 8,1,1]> ⬅️ ::ScatterND(%\"val_241\", %\"val_239\", %\"val_240\") {reduction='none'}\n",
              "            117 |  # node_Shape_236\n",
              "                   %\"val_244\"<INT64,[4]> ⬅️ ::Shape(%\"slice_scatter_1\") {start=0}\n",
              "            118 |  # node_Gather_237\n",
              "                   %\"val_245\"<INT64,[]> ⬅️ ::Gather(%\"val_244\", %\"val_149\"{1}) {axis=0}\n",
              "            119 |  # node_Range_238\n",
              "                   %\"val_246\"<INT64,[None]> ⬅️ ::Range(%\"val_141\"{0}, %\"val_245\", %\"val_149\"{1})\n",
              "            120 |  # node_Slice_242\n",
              "                   %\"val_250\"<INT64,[None]> ⬅️ ::Slice(%\"val_246\", %\"val_144\"{[0]}, %\"val_148\"{[-8]}, %\"val_144\"{[0]}, %\"val_152\"{[1]})\n",
              "            121 |  # node_Unsqueeze_243\n",
              "                   %\"val_251\"<INT64,[None,1]> ⬅️ ::Unsqueeze(%\"val_250\", %\"val_3\"{[-1]})\n",
              "            122 |  # node_Transpose_4887\n",
              "                   %\"val_252\"<FLOAT,[s53 - 8,1,s0,1]> ⬅️ ::Transpose(%\"val_242\") {perm=(1, 2, 0, 3)}\n",
              "            123 |  # node_Transpose_245\n",
              "                   %\"val_253\"<FLOAT,[s53,1,s0,1]> ⬅️ ::Transpose(%\"slice_scatter_1\") {perm=(1, 0, 2, 3)}\n",
              "            124 |  # node_ScatterND_246\n",
              "                   %\"val_254\"<FLOAT,[s53,1,s0,1]> ⬅️ ::ScatterND(%\"val_253\", %\"val_251\", %\"val_252\") {reduction='none'}\n",
              "            125 |  # node_slice_scatter_3\n",
              "                   %\"slice_scatter_3\"<FLOAT,[1,s53,s0,1]> ⬅️ ::Transpose(%\"val_254\") {perm=(1, 0, 2, 3)}\n",
              "            126 |  # node_slice_15\n",
              "                   %\"slice_15\"<FLOAT,[1,s53 - 8,s0,1]> ⬅️ ::Slice(%\"slice_scatter_3\", %\"val_144\"{[0]}, %\"val_148\"{[-8]}, %\"val_152\"{[1]}, %\"val_152\"{[1]})\n",
              "            127 |  # node_slice_16\n",
              "                   %\"slice_16\"<FLOAT,[1,s53 - 8,4,1]> ⬅️ ::Slice(%\"slice_15\", %\"val_216\"{[-4]}, %\"val_271\"{[9223372036854775807]}, %\"val_163\"{[2]}, %\"val_152\"{[1]})\n",
              "            128 |  # n1_3\n",
              "                   %\"shape_3\"<INT64,[4]> ⬅️ ::Shape(%\"slice_16\")\n",
              "            129 |  # n2_3\n",
              "                   %\"fill_2\"<FLOAT,[1,s53 - 8,4,1]> ⬅️ ::Expand(%\"value_0_3\"{2.0}, %\"shape_3\")\n",
              "            130 |  # node_Shape_279\n",
              "                   %\"val_287\"<INT64,[4]> ⬅️ ::Shape(%\"slice_15\") {start=0}\n",
              "            131 |  # node_Gather_280\n",
              "                   %\"val_288\"<INT64,[]> ⬅️ ::Gather(%\"val_287\", %\"val_160\"{2}) {axis=0}\n",
              "            132 |  # node_Range_281\n",
              "                   %\"val_289\"<INT64,[None]> ⬅️ ::Range(%\"val_141\"{0}, %\"val_288\", %\"val_149\"{1})\n",
              "            133 |  # node_Slice_285\n",
              "                   %\"val_293\"<INT64,[None]> ⬅️ ::Slice(%\"val_289\", %\"val_216\"{[-4]}, %\"val_271\"{[9223372036854775807]}, %\"val_144\"{[0]}, %\"val_152\"{[1]})\n",
              "            134 |  # node_Unsqueeze_286\n",
              "                   %\"val_294\"<INT64,[None,1]> ⬅️ ::Unsqueeze(%\"val_293\", %\"val_3\"{[-1]})\n",
              "            135 |  # node_Transpose_287\n",
              "                   %\"val_295\"<FLOAT,[4,s53 - 8,1,1]> ⬅️ ::Transpose(%\"fill_2\") {perm=(2, 1, 0, 3)}\n",
              "            136 |  # node_Transpose_288\n",
              "                   %\"val_296\"<FLOAT,[s0,s53 - 8,1,1]> ⬅️ ::Transpose(%\"slice_15\") {perm=(2, 1, 0, 3)}\n",
              "            137 |  # node_ScatterND_289\n",
              "                   %\"val_297\"<FLOAT,[s0,s53 - 8,1,1]> ⬅️ ::ScatterND(%\"val_296\", %\"val_294\", %\"val_295\") {reduction='none'}\n",
              "            138 |  # node_Shape_291\n",
              "                   %\"val_299\"<INT64,[4]> ⬅️ ::Shape(%\"slice_scatter_3\") {start=0}\n",
              "            139 |  # node_Gather_292\n",
              "                   %\"val_300\"<INT64,[]> ⬅️ ::Gather(%\"val_299\", %\"val_149\"{1}) {axis=0}\n",
              "            140 |  # node_Range_293\n",
              "                   %\"val_301\"<INT64,[None]> ⬅️ ::Range(%\"val_141\"{0}, %\"val_300\", %\"val_149\"{1})\n",
              "            141 |  # node_Slice_297\n",
              "                   %\"val_305\"<INT64,[None]> ⬅️ ::Slice(%\"val_301\", %\"val_144\"{[0]}, %\"val_148\"{[-8]}, %\"val_144\"{[0]}, %\"val_152\"{[1]})\n",
              "            142 |  # node_Unsqueeze_298\n",
              "                   %\"val_306\"<INT64,[None,1]> ⬅️ ::Unsqueeze(%\"val_305\", %\"val_3\"{[-1]})\n",
              "            143 |  # node_Transpose_4888\n",
              "                   %\"val_307\"<FLOAT,[s53 - 8,1,s0,1]> ⬅️ ::Transpose(%\"val_297\") {perm=(1, 2, 0, 3)}\n",
              "            144 |  # node_Transpose_300\n",
              "                   %\"val_308\"<FLOAT,[s53,1,s0,1]> ⬅️ ::Transpose(%\"slice_scatter_3\") {perm=(1, 0, 2, 3)}\n",
              "            145 |  # node_ScatterND_301\n",
              "                   %\"val_309\"<FLOAT,[s53,1,s0,1]> ⬅️ ::ScatterND(%\"val_308\", %\"val_306\", %\"val_307\") {reduction='none'}\n",
              "            146 |  # node_slice_scatter_5\n",
              "                   %\"slice_scatter_5\"<FLOAT,[1,s53,s0,1]> ⬅️ ::Transpose(%\"val_309\") {perm=(1, 0, 2, 3)}\n",
              "            147 |  # node_slice_22\n",
              "                   %\"slice_22\"<FLOAT,[1,4,s0,1]> ⬅️ ::Slice(%\"slice_scatter_5\", %\"val_148\"{[-8]}, %\"val_216\"{[-4]}, %\"val_152\"{[1]}, %\"val_152\"{[1]})\n",
              "            148 |  # node_slice_23\n",
              "                   %\"slice_23\"<FLOAT,[1,4,s0 - 8,1]> ⬅️ ::Slice(%\"slice_22\", %\"val_144\"{[0]}, %\"val_148\"{[-8]}, %\"val_163\"{[2]}, %\"val_152\"{[1]})\n",
              "            149 |  # n1_4\n",
              "                   %\"shape_4\"<INT64,[4]> ⬅️ ::Shape(%\"slice_23\")\n",
              "            150 |  # n2_4\n",
              "                   %\"fill_3\"<FLOAT,[1,4,s0 - 8,1]> ⬅️ ::Expand(%\"value_0_4\"{3.0}, %\"shape_4\")\n",
              "            151 |  # node_Shape_333\n",
              "                   %\"val_341\"<INT64,[4]> ⬅️ ::Shape(%\"slice_22\") {start=0}\n",
              "            152 |  # node_Gather_334\n",
              "                   %\"val_342\"<INT64,[]> ⬅️ ::Gather(%\"val_341\", %\"val_160\"{2}) {axis=0}\n",
              "            153 |  # node_Range_335\n",
              "                   %\"val_343\"<INT64,[None]> ⬅️ ::Range(%\"val_141\"{0}, %\"val_342\", %\"val_149\"{1})\n",
              "            154 |  # node_Slice_339\n",
              "                   %\"val_347\"<INT64,[None]> ⬅️ ::Slice(%\"val_343\", %\"val_144\"{[0]}, %\"val_148\"{[-8]}, %\"val_144\"{[0]}, %\"val_152\"{[1]})\n",
              "            155 |  # node_Unsqueeze_340\n",
              "                   %\"val_348\"<INT64,[None,1]> ⬅️ ::Unsqueeze(%\"val_347\", %\"val_3\"{[-1]})\n",
              "            156 |  # node_Transpose_341\n",
              "                   %\"val_349\"<FLOAT,[s0 - 8,4,1,1]> ⬅️ ::Transpose(%\"fill_3\") {perm=(2, 1, 0, 3)}\n",
              "            157 |  # node_Transpose_342\n",
              "                   %\"val_350\"<FLOAT,[s0,4,1,1]> ⬅️ ::Transpose(%\"slice_22\") {perm=(2, 1, 0, 3)}\n",
              "            158 |  # node_ScatterND_343\n",
              "                   %\"val_351\"<FLOAT,[s0,4,1,1]> ⬅️ ::ScatterND(%\"val_350\", %\"val_348\", %\"val_349\") {reduction='none'}\n",
              "            159 |  # node_Shape_345\n",
              "                   %\"val_353\"<INT64,[4]> ⬅️ ::Shape(%\"slice_scatter_5\") {start=0}\n",
              "            160 |  # node_Gather_346\n",
              "                   %\"val_354\"<INT64,[]> ⬅️ ::Gather(%\"val_353\", %\"val_149\"{1}) {axis=0}\n",
              "            161 |  # node_Range_347\n",
              "                   %\"val_355\"<INT64,[None]> ⬅️ ::Range(%\"val_141\"{0}, %\"val_354\", %\"val_149\"{1})\n",
              "            162 |  # node_Slice_351\n",
              "                   %\"val_359\"<INT64,[None]> ⬅️ ::Slice(%\"val_355\", %\"val_148\"{[-8]}, %\"val_216\"{[-4]}, %\"val_144\"{[0]}, %\"val_152\"{[1]})\n",
              "            163 |  # node_Unsqueeze_352\n",
              "                   %\"val_360\"<INT64,[None,1]> ⬅️ ::Unsqueeze(%\"val_359\", %\"val_3\"{[-1]})\n",
              "            164 |  # node_Transpose_4889\n",
              "                   %\"val_361\"<FLOAT,[4,1,s0,1]> ⬅️ ::Transpose(%\"val_351\") {perm=(1, 2, 0, 3)}\n",
              "            165 |  # node_Transpose_354\n",
              "                   %\"val_362\"<FLOAT,[s53,1,s0,1]> ⬅️ ::Transpose(%\"slice_scatter_5\") {perm=(1, 0, 2, 3)}\n",
              "            166 |  # node_ScatterND_355\n",
              "                   %\"val_363\"<FLOAT,[s53,1,s0,1]> ⬅️ ::ScatterND(%\"val_362\", %\"val_360\", %\"val_361\") {reduction='none'}\n",
              "            167 |  # node_slice_scatter_7\n",
              "                   %\"slice_scatter_7\"<FLOAT,[1,s53,s0,1]> ⬅️ ::Transpose(%\"val_363\") {perm=(1, 0, 2, 3)}\n",
              "            168 |  # node_slice_31\n",
              "                   %\"slice_31\"<FLOAT,[1,4,s0,1]> ⬅️ ::Slice(%\"slice_scatter_7\", %\"val_148\"{[-8]}, %\"val_216\"{[-4]}, %\"val_152\"{[1]}, %\"val_152\"{[1]})\n",
              "            169 |  # node_Shape_387\n",
              "                   %\"val_395\"<INT64,[4]> ⬅️ ::Shape(%\"slice_31\") {start=0}\n",
              "            170 |  # node_Gather_388\n",
              "                   %\"val_396\"<INT64,[]> ⬅️ ::Gather(%\"val_395\", %\"val_160\"{2}) {axis=0}\n",
              "            171 |  # node_Range_389\n",
              "                   %\"val_397\"<INT64,[None]> ⬅️ ::Range(%\"val_141\"{0}, %\"val_396\", %\"val_149\"{1})\n",
              "            172 |  # node_Slice_393\n",
              "                   %\"val_401\"<INT64,[None]> ⬅️ ::Slice(%\"val_397\", %\"val_148\"{[-8]}, %\"val_216\"{[-4]}, %\"val_144\"{[0]}, %\"val_152\"{[1]})\n",
              "            173 |  # node_Unsqueeze_394\n",
              "                   %\"val_402\"<INT64,[None,1]> ⬅️ ::Unsqueeze(%\"val_401\", %\"val_3\"{[-1]})\n",
              "            174 |  # node_Transpose_396\n",
              "                   %\"val_404\"<FLOAT,[s0,4,1,1]> ⬅️ ::Transpose(%\"slice_31\") {perm=(2, 1, 0, 3)}\n",
              "            175 |  # node_ScatterND_397\n",
              "                   %\"val_405\"<FLOAT,[s0,4,1,1]> ⬅️ ::ScatterND(%\"val_404\", %\"val_402\", %\"val_403\"{...}) {reduction='none'}\n",
              "            176 |  # node_Shape_399\n",
              "                   %\"val_407\"<INT64,[4]> ⬅️ ::Shape(%\"slice_scatter_7\") {start=0}\n",
              "            177 |  # node_Gather_400\n",
              "                   %\"val_408\"<INT64,[]> ⬅️ ::Gather(%\"val_407\", %\"val_149\"{1}) {axis=0}\n",
              "            178 |  # node_Range_401\n",
              "                   %\"val_409\"<INT64,[None]> ⬅️ ::Range(%\"val_141\"{0}, %\"val_408\", %\"val_149\"{1})\n",
              "            179 |  # node_Slice_405\n",
              "                   %\"val_413\"<INT64,[None]> ⬅️ ::Slice(%\"val_409\", %\"val_148\"{[-8]}, %\"val_216\"{[-4]}, %\"val_144\"{[0]}, %\"val_152\"{[1]})\n",
              "            180 |  # node_Unsqueeze_406\n",
              "                   %\"val_414\"<INT64,[None,1]> ⬅️ ::Unsqueeze(%\"val_413\", %\"val_3\"{[-1]})\n",
              "            181 |  # node_Transpose_4890\n",
              "                   %\"val_415\"<FLOAT,[4,1,s0,1]> ⬅️ ::Transpose(%\"val_405\") {perm=(1, 2, 0, 3)}\n",
              "            182 |  # node_Transpose_408\n",
              "                   %\"val_416\"<FLOAT,[s53,1,s0,1]> ⬅️ ::Transpose(%\"slice_scatter_7\") {perm=(1, 0, 2, 3)}\n",
              "            183 |  # node_ScatterND_409\n",
              "                   %\"val_417\"<FLOAT,[s53,1,s0,1]> ⬅️ ::ScatterND(%\"val_416\", %\"val_414\", %\"val_415\") {reduction='none'}\n",
              "            184 |  # node_slice_scatter_9\n",
              "                   %\"slice_scatter_9\"<FLOAT,[1,s53,s0,1]> ⬅️ ::Transpose(%\"val_417\") {perm=(1, 0, 2, 3)}\n",
              "            185 |  # node_slice_38\n",
              "                   %\"slice_38\"<FLOAT,[1,4,s0,1]> ⬅️ ::Slice(%\"slice_scatter_9\", %\"val_148\"{[-8]}, %\"val_216\"{[-4]}, %\"val_152\"{[1]}, %\"val_152\"{[1]})\n",
              "            186 |  # node_Shape_441\n",
              "                   %\"val_449\"<INT64,[4]> ⬅️ ::Shape(%\"slice_38\") {start=0}\n",
              "            187 |  # node_Gather_442\n",
              "                   %\"val_450\"<INT64,[]> ⬅️ ::Gather(%\"val_449\", %\"val_160\"{2}) {axis=0}\n",
              "            188 |  # node_Range_443\n",
              "                   %\"val_451\"<INT64,[None]> ⬅️ ::Range(%\"val_141\"{0}, %\"val_450\", %\"val_149\"{1})\n",
              "            189 |  # node_Slice_447\n",
              "                   %\"val_455\"<INT64,[None]> ⬅️ ::Slice(%\"val_451\", %\"val_216\"{[-4]}, %\"val_271\"{[9223372036854775807]}, %\"val_144\"{[0]}, %\"val_152\"{[1]})\n",
              "            190 |  # node_Unsqueeze_448\n",
              "                   %\"val_456\"<INT64,[None,1]> ⬅️ ::Unsqueeze(%\"val_455\", %\"val_3\"{[-1]})\n",
              "            191 |  # node_Transpose_450\n",
              "                   %\"val_458\"<FLOAT,[s0,4,1,1]> ⬅️ ::Transpose(%\"slice_38\") {perm=(2, 1, 0, 3)}\n",
              "            192 |  # node_ScatterND_451\n",
              "                   %\"val_459\"<FLOAT,[s0,4,1,1]> ⬅️ ::ScatterND(%\"val_458\", %\"val_456\", %\"val_457\"{...}) {reduction='none'}\n",
              "            193 |  # node_Shape_453\n",
              "                   %\"val_461\"<INT64,[4]> ⬅️ ::Shape(%\"slice_scatter_9\") {start=0}\n",
              "            194 |  # node_Gather_454\n",
              "                   %\"val_462\"<INT64,[]> ⬅️ ::Gather(%\"val_461\", %\"val_149\"{1}) {axis=0}\n",
              "            195 |  # node_Range_455\n",
              "                   %\"val_463\"<INT64,[None]> ⬅️ ::Range(%\"val_141\"{0}, %\"val_462\", %\"val_149\"{1})\n",
              "            196 |  # node_Slice_459\n",
              "                   %\"val_467\"<INT64,[None]> ⬅️ ::Slice(%\"val_463\", %\"val_148\"{[-8]}, %\"val_216\"{[-4]}, %\"val_144\"{[0]}, %\"val_152\"{[1]})\n",
              "            197 |  # node_Unsqueeze_460\n",
              "                   %\"val_468\"<INT64,[None,1]> ⬅️ ::Unsqueeze(%\"val_467\", %\"val_3\"{[-1]})\n",
              "            198 |  # node_Transpose_4891\n",
              "                   %\"val_469\"<FLOAT,[4,1,s0,1]> ⬅️ ::Transpose(%\"val_459\") {perm=(1, 2, 0, 3)}\n",
              "            199 |  # node_Transpose_462\n",
              "                   %\"val_470\"<FLOAT,[s53,1,s0,1]> ⬅️ ::Transpose(%\"slice_scatter_9\") {perm=(1, 0, 2, 3)}\n",
              "            200 |  # node_ScatterND_463\n",
              "                   %\"val_471\"<FLOAT,[s53,1,s0,1]> ⬅️ ::ScatterND(%\"val_470\", %\"val_468\", %\"val_469\") {reduction='none'}\n",
              "            201 |  # node_slice_scatter_11\n",
              "                   %\"slice_scatter_11\"<FLOAT,[1,s53,s0,1]> ⬅️ ::Transpose(%\"val_471\") {perm=(1, 0, 2, 3)}\n",
              "            202 |  # node_slice_43\n",
              "                   %\"slice_43\"<FLOAT,[1,4,s0,1]> ⬅️ ::Slice(%\"slice_scatter_11\", %\"val_216\"{[-4]}, %\"val_271\"{[9223372036854775807]}, %\"val_152\"{[1]}, %\"val_152\"{[1]})\n",
              "            203 |  # node_slice_44\n",
              "                   %\"slice_44\"<FLOAT,[1,4,s0 - 8,1]> ⬅️ ::Slice(%\"slice_43\", %\"val_144\"{[0]}, %\"val_148\"{[-8]}, %\"val_163\"{[2]}, %\"val_152\"{[1]})\n",
              "            204 |  # n1_7\n",
              "                   %\"shape_7\"<INT64,[4]> ⬅️ ::Shape(%\"slice_44\")\n",
              "            205 |  # n2_7\n",
              "                   %\"fill_6\"<FLOAT,[1,4,s0 - 8,1]> ⬅️ ::Expand(%\"value_0_7\"{6.0}, %\"shape_7\")\n",
              "            206 |  # node_Shape_495\n",
              "                   %\"val_503\"<INT64,[4]> ⬅️ ::Shape(%\"slice_43\") {start=0}\n",
              "            207 |  # node_Gather_496\n",
              "                   %\"val_504\"<INT64,[]> ⬅️ ::Gather(%\"val_503\", %\"val_160\"{2}) {axis=0}\n",
              "            208 |  # node_Range_497\n",
              "                   %\"val_505\"<INT64,[None]> ⬅️ ::Range(%\"val_141\"{0}, %\"val_504\", %\"val_149\"{1})\n",
              "            209 |  # node_Slice_501\n",
              "                   %\"val_509\"<INT64,[None]> ⬅️ ::Slice(%\"val_505\", %\"val_144\"{[0]}, %\"val_148\"{[-8]}, %\"val_144\"{[0]}, %\"val_152\"{[1]})\n",
              "            210 |  # node_Unsqueeze_502\n",
              "                   %\"val_510\"<INT64,[None,1]> ⬅️ ::Unsqueeze(%\"val_509\", %\"val_3\"{[-1]})\n",
              "            211 |  # node_Transpose_503\n",
              "                   %\"val_511\"<FLOAT,[s0 - 8,4,1,1]> ⬅️ ::Transpose(%\"fill_6\") {perm=(2, 1, 0, 3)}\n",
              "            212 |  # node_Transpose_504\n",
              "                   %\"val_512\"<FLOAT,[s0,4,1,1]> ⬅️ ::Transpose(%\"slice_43\") {perm=(2, 1, 0, 3)}\n",
              "            213 |  # node_ScatterND_505\n",
              "                   %\"val_513\"<FLOAT,[s0,4,1,1]> ⬅️ ::ScatterND(%\"val_512\", %\"val_510\", %\"val_511\") {reduction='none'}\n",
              "            214 |  # node_Shape_507\n",
              "                   %\"val_515\"<INT64,[4]> ⬅️ ::Shape(%\"slice_scatter_11\") {start=0}\n",
              "            215 |  # node_Gather_508\n",
              "                   %\"val_516\"<INT64,[]> ⬅️ ::Gather(%\"val_515\", %\"val_149\"{1}) {axis=0}\n",
              "            216 |  # node_Range_509\n",
              "                   %\"val_517\"<INT64,[None]> ⬅️ ::Range(%\"val_141\"{0}, %\"val_516\", %\"val_149\"{1})\n",
              "            217 |  # node_Slice_513\n",
              "                   %\"val_521\"<INT64,[None]> ⬅️ ::Slice(%\"val_517\", %\"val_216\"{[-4]}, %\"val_271\"{[9223372036854775807]}, %\"val_144\"{[0]}, %\"val_152\"{[1]})\n",
              "            218 |  # node_Unsqueeze_514\n",
              "                   %\"val_522\"<INT64,[None,1]> ⬅️ ::Unsqueeze(%\"val_521\", %\"val_3\"{[-1]})\n",
              "            219 |  # node_Transpose_4892\n",
              "                   %\"val_523\"<FLOAT,[4,1,s0,1]> ⬅️ ::Transpose(%\"val_513\") {perm=(1, 2, 0, 3)}\n",
              "            220 |  # node_Transpose_516\n",
              "                   %\"val_524\"<FLOAT,[s53,1,s0,1]> ⬅️ ::Transpose(%\"slice_scatter_11\") {perm=(1, 0, 2, 3)}\n",
              "            221 |  # node_ScatterND_517\n",
              "                   %\"val_525\"<FLOAT,[s53,1,s0,1]> ⬅️ ::ScatterND(%\"val_524\", %\"val_522\", %\"val_523\") {reduction='none'}\n",
              "            222 |  # node_slice_scatter_13\n",
              "                   %\"slice_scatter_13\"<FLOAT,[1,s53,s0,1]> ⬅️ ::Transpose(%\"val_525\") {perm=(1, 0, 2, 3)}\n",
              "            223 |  # node_slice_52\n",
              "                   %\"slice_52\"<FLOAT,[1,4,s0,1]> ⬅️ ::Slice(%\"slice_scatter_13\", %\"val_216\"{[-4]}, %\"val_271\"{[9223372036854775807]}, %\"val_152\"{[1]}, %\"val_152\"{[1]})\n",
              "            224 |  # node_Shape_549\n",
              "                   %\"val_557\"<INT64,[4]> ⬅️ ::Shape(%\"slice_52\") {start=0}\n",
              "            225 |  # node_Gather_550\n",
              "                   %\"val_558\"<INT64,[]> ⬅️ ::Gather(%\"val_557\", %\"val_160\"{2}) {axis=0}\n",
              "            226 |  # node_Range_551\n",
              "                   %\"val_559\"<INT64,[None]> ⬅️ ::Range(%\"val_141\"{0}, %\"val_558\", %\"val_149\"{1})\n",
              "            227 |  # node_Slice_555\n",
              "                   %\"val_563\"<INT64,[None]> ⬅️ ::Slice(%\"val_559\", %\"val_148\"{[-8]}, %\"val_216\"{[-4]}, %\"val_144\"{[0]}, %\"val_152\"{[1]})\n",
              "            228 |  # node_Unsqueeze_556\n",
              "                   %\"val_564\"<INT64,[None,1]> ⬅️ ::Unsqueeze(%\"val_563\", %\"val_3\"{[-1]})\n",
              "            229 |  # node_Transpose_558\n",
              "                   %\"val_566\"<FLOAT,[s0,4,1,1]> ⬅️ ::Transpose(%\"slice_52\") {perm=(2, 1, 0, 3)}\n",
              "            230 |  # node_ScatterND_559\n",
              "                   %\"val_567\"<FLOAT,[s0,4,1,1]> ⬅️ ::ScatterND(%\"val_566\", %\"val_564\", %\"val_565\"{...}) {reduction='none'}\n",
              "            231 |  # node_Shape_561\n",
              "                   %\"val_569\"<INT64,[4]> ⬅️ ::Shape(%\"slice_scatter_13\") {start=0}\n",
              "            232 |  # node_Gather_562\n",
              "                   %\"val_570\"<INT64,[]> ⬅️ ::Gather(%\"val_569\", %\"val_149\"{1}) {axis=0}\n",
              "            233 |  # node_Range_563\n",
              "                   %\"val_571\"<INT64,[None]> ⬅️ ::Range(%\"val_141\"{0}, %\"val_570\", %\"val_149\"{1})\n",
              "            234 |  # node_Slice_567\n",
              "                   %\"val_575\"<INT64,[None]> ⬅️ ::Slice(%\"val_571\", %\"val_216\"{[-4]}, %\"val_271\"{[9223372036854775807]}, %\"val_144\"{[0]}, %\"val_152\"{[1]})\n",
              "            235 |  # node_Unsqueeze_568\n",
              "                   %\"val_576\"<INT64,[None,1]> ⬅️ ::Unsqueeze(%\"val_575\", %\"val_3\"{[-1]})\n",
              "            236 |  # node_Transpose_4893\n",
              "                   %\"val_577\"<FLOAT,[4,1,s0,1]> ⬅️ ::Transpose(%\"val_567\") {perm=(1, 2, 0, 3)}\n",
              "            237 |  # node_Transpose_570\n",
              "                   %\"val_578\"<FLOAT,[s53,1,s0,1]> ⬅️ ::Transpose(%\"slice_scatter_13\") {perm=(1, 0, 2, 3)}\n",
              "            238 |  # node_ScatterND_571\n",
              "                   %\"val_579\"<FLOAT,[s53,1,s0,1]> ⬅️ ::ScatterND(%\"val_578\", %\"val_576\", %\"val_577\") {reduction='none'}\n",
              "            239 |  # node_slice_scatter_15\n",
              "                   %\"slice_scatter_15\"<FLOAT,[1,s53,s0,1]> ⬅️ ::Transpose(%\"val_579\") {perm=(1, 0, 2, 3)}\n",
              "            240 |  # node_slice_59\n",
              "                   %\"slice_59\"<FLOAT,[1,4,s0,1]> ⬅️ ::Slice(%\"slice_scatter_15\", %\"val_216\"{[-4]}, %\"val_271\"{[9223372036854775807]}, %\"val_152\"{[1]}, %\"val_152\"{[1]})\n",
              "            241 |  # node_Shape_603\n",
              "                   %\"val_611\"<INT64,[4]> ⬅️ ::Shape(%\"slice_59\") {start=0}\n",
              "            242 |  # node_Gather_604\n",
              "                   %\"val_612\"<INT64,[]> ⬅️ ::Gather(%\"val_611\", %\"val_160\"{2}) {axis=0}\n",
              "            243 |  # node_Range_605\n",
              "                   %\"val_613\"<INT64,[None]> ⬅️ ::Range(%\"val_141\"{0}, %\"val_612\", %\"val_149\"{1})\n",
              "            244 |  # node_Slice_609\n",
              "                   %\"val_617\"<INT64,[None]> ⬅️ ::Slice(%\"val_613\", %\"val_216\"{[-4]}, %\"val_271\"{[9223372036854775807]}, %\"val_144\"{[0]}, %\"val_152\"{[1]})\n",
              "            245 |  # node_Unsqueeze_610\n",
              "                   %\"val_618\"<INT64,[None,1]> ⬅️ ::Unsqueeze(%\"val_617\", %\"val_3\"{[-1]})\n",
              "            246 |  # node_Transpose_612\n",
              "                   %\"val_620\"<FLOAT,[s0,4,1,1]> ⬅️ ::Transpose(%\"slice_59\") {perm=(2, 1, 0, 3)}\n",
              "            247 |  # node_ScatterND_613\n",
              "                   %\"val_621\"<FLOAT,[s0,4,1,1]> ⬅️ ::ScatterND(%\"val_620\", %\"val_618\", %\"val_619\"{...}) {reduction='none'}\n",
              "            248 |  # node_Shape_615\n",
              "                   %\"val_623\"<INT64,[4]> ⬅️ ::Shape(%\"slice_scatter_15\") {start=0}\n",
              "            249 |  # node_Gather_616\n",
              "                   %\"val_624\"<INT64,[]> ⬅️ ::Gather(%\"val_623\", %\"val_149\"{1}) {axis=0}\n",
              "            250 |  # node_Range_617\n",
              "                   %\"val_625\"<INT64,[None]> ⬅️ ::Range(%\"val_141\"{0}, %\"val_624\", %\"val_149\"{1})\n",
              "            251 |  # node_Slice_621\n",
              "                   %\"val_629\"<INT64,[None]> ⬅️ ::Slice(%\"val_625\", %\"val_216\"{[-4]}, %\"val_271\"{[9223372036854775807]}, %\"val_144\"{[0]}, %\"val_152\"{[1]})\n",
              "            252 |  # node_Unsqueeze_622\n",
              "                   %\"val_630\"<INT64,[None,1]> ⬅️ ::Unsqueeze(%\"val_629\", %\"val_3\"{[-1]})\n",
              "            253 |  # node_Transpose_4894\n",
              "                   %\"val_631\"<FLOAT,[4,1,s0,1]> ⬅️ ::Transpose(%\"val_621\") {perm=(1, 2, 0, 3)}\n",
              "            254 |  # node_Transpose_624\n",
              "                   %\"val_632\"<FLOAT,[s53,1,s0,1]> ⬅️ ::Transpose(%\"slice_scatter_15\") {perm=(1, 0, 2, 3)}\n",
              "            255 |  # node_ScatterND_625\n",
              "                   %\"val_633\"<FLOAT,[s53,1,s0,1]> ⬅️ ::ScatterND(%\"val_632\", %\"val_630\", %\"val_631\") {reduction='none'}\n",
              "            256 |  # node_slice_scatter_17\n",
              "                   %\"slice_scatter_17\"<FLOAT,[1,s53,s0,1]> ⬅️ ::Transpose(%\"val_633\") {perm=(1, 0, 2, 3)}\n",
              "            257 |  # node_Concat_633\n",
              "                   %\"val_641\"<INT64,[6]> ⬅️ ::Concat(%\"val_152\"{[1]}, %\"val_19\", %\"val_20\"{[8]}, %\"val_21\", %\"val_20\"{[8]}, %\"val_3\"{[-1]}) {axis=0}\n",
              "            258 |  # node_view_16\n",
              "                   %\"view_16\"<FLOAT,[1,(s53//8),8,(s0//8),8,((s0*s53)//(64*((s0//8))*((s53//8))))]> ⬅️ ::Reshape(%\"slice_scatter_17\", %\"val_641\") {allowzero=1}\n",
              "            259 |  # node_permute_7\n",
              "                   %\"permute_7\"<FLOAT,[1,(s53//8),(s0//8),8,8,((s0*s53)//(64*((s0//8))*((s53//8))))]> ⬅️ ::Transpose(%\"view_16\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            260 |  # node_view_17\n",
              "                   %\"view_17\"<FLOAT,[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))),8,8,1]> ⬅️ ::Reshape(%\"permute_7\", %\"val_647\"{[-1, 8, 8, 1]}) {allowzero=1}\n",
              "            261 |  # node_mul_843\n",
              "                   %\"mul_843\"<INT64,[]> ⬅️ ::Mul(%\"val_648\"{64}, %\"floordiv_389\")\n",
              "            262 |  # node_mul_844\n",
              "                   %\"mul_844\"<INT64,[]> ⬅️ ::Mul(%\"mul_843\", %\"floordiv_388\")\n",
              "            263 |  # node_floordiv_392\n",
              "                   %\"floordiv_392\"<INT64,[]> ⬅️ ::Div(%\"mul_471\", %\"mul_844\")\n",
              "            264 |  # node_mul_846\n",
              "                   %\"mul_846\"<INT64,[]> ⬅️ ::Mul(%\"mul_525\", %\"floordiv_392\")\n",
              "            265 |  # node_Reshape_642\n",
              "                   %\"val_650\"<INT64,[1]> ⬅️ ::Reshape(%\"mul_846\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            266 |  # node_Concat_644\n",
              "                   %\"val_652\"<INT64,[2]> ⬅️ ::Concat(%\"val_650\", %\"val_3\"{[-1]}) {axis=0}\n",
              "            267 |  # node_view_18\n",
              "                   %\"view_18\"<FLOAT,[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))),64]> ⬅️ ::Reshape(%\"view_17\", %\"val_652\") {allowzero=1}\n",
              "            268 |  # node_unsqueeze\n",
              "                   %\"unsqueeze\"<FLOAT,[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))),1,64]> ⬅️ ::Unsqueeze(%\"view_18\", %\"val_152\"{[1]})\n",
              "            269 |  # node_unsqueeze_1\n",
              "                   %\"unsqueeze_1\"<FLOAT,[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))),64,1]> ⬅️ ::Unsqueeze(%\"view_18\", %\"val_163\"{[2]})\n",
              "            270 |  # node_sub_158\n",
              "                   %\"sub_158\"<FLOAT,[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))),64,64]> ⬅️ ::Sub(%\"unsqueeze\", %\"unsqueeze_1\")\n",
              "            271 |  # node_Equal_647\n",
              "                   %\"val_655\"<BOOL,[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))),64,64]> ⬅️ ::Equal(%\"sub_158\", %\"value_0\"{0.0})\n",
              "            272 |  # node_ne_36\n",
              "                   %\"ne_36\"<BOOL,[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))),64,64]> ⬅️ ::Not(%\"val_655\")\n",
              "            273 |  # node_masked_fill\n",
              "                   %\"masked_fill\"<FLOAT,[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))),64,64]> ⬅️ ::Where(%\"ne_36\", %\"val_657\"{-100.0}, %\"sub_158\")\n",
              "            274 |  # node_masked_fill_1\n",
              "                   %\"masked_fill_1\"<FLOAT,[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))),64,64]> ⬅️ ::Where(%\"val_655\", %\"value_0\"{0.0}, %\"masked_fill\")\n",
              "            275 |  # node_MatMul_653\n",
              "                   %\"val_661\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::MatMul(%\"view_14\", %\"val_660\"{...})\n",
              "            276 |  # node_linear_4\n",
              "                   %\"linear_4\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::Add(%\"val_661\", %\"layers.0.blocks.1.attn.qkv.bias\"{...})\n",
              "            277 |  # node_view_19\n",
              "                   %\"view_19\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),3,8,32]> ⬅️ ::Reshape(%\"linear_4\", %\"val_46\") {allowzero=1}\n",
              "            278 |  # node_permute_8\n",
              "                   %\"permute_8\"<FLOAT,[3,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Transpose(%\"view_19\") {perm=(2, 0, 3, 1, 4)}\n",
              "            279 |  # node_Split_661\n",
              "                   %\"val_669\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_670\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_671\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Split(%\"permute_8\") {num_outputs=3, axis=0}\n",
              "            280 |  # node_unbind_1__0\n",
              "                   %\"getitem_3\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_669\", %\"val_144\"{[0]})\n",
              "            281 |  # node_unbind_1__1\n",
              "                   %\"getitem_4\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_670\", %\"val_144\"{[0]})\n",
              "            282 |  # node_unbind_1__2\n",
              "                   %\"getitem_5\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_671\", %\"val_144\"{[0]})\n",
              "            283 |  # node_transpose_1\n",
              "                   %\"transpose_1\"<FLOAT,[((s0//8))*((s53//8)),8,32,((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Transpose(%\"getitem_4\") {perm=(0, 1, 3, 2)}\n",
              "            284 |  # node_matmul_2\n",
              "                   %\"matmul_2\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::MatMul(%\"getitem_3\", %\"transpose_1\")\n",
              "            285 |  # node_mul_903\n",
              "                   %\"mul_903\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Mul(%\"matmul_2\", %\"val_51\"{0.1767766922712326})\n",
              "            286 |  # node_floordiv_393\n",
              "                   %\"floordiv_393\"<INT64,[]> ⬅️ ::Div(%\"floordiv_391\", %\"mul_846\")\n",
              "            287 |  # node_Reshape_663\n",
              "                   %\"val_673\"<INT64,[1]> ⬅️ ::Reshape(%\"floordiv_393\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            288 |  # node_Concat_668\n",
              "                   %\"val_678\"<INT64,[5]> ⬅️ ::Concat(%\"val_673\", %\"val_650\", %\"val_20\"{[8]}, %\"val_42\"{[64]}, %\"val_42\"{[64]}) {axis=0}\n",
              "            289 |  # node_view_20\n",
              "                   %\"view_20\"<FLOAT,[((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))),((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Reshape(%\"mul_903\", %\"val_678\") {allowzero=1}\n",
              "            290 |  # node_Unsqueeze_4896\n",
              "                   %\"unsqueeze_3\"<FLOAT,[1,((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))),1,64,64]> ⬅️ ::Unsqueeze(%\"masked_fill_1\", %\"val_4369\"{[0, 2]})\n",
              "            291 |  # node_add_498\n",
              "                   %\"add_498\"<FLOAT,[1,((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,64,64]> ⬅️ ::Add(%\"view_20\", %\"unsqueeze_3\")\n",
              "            292 |  # node_Concat_674\n",
              "                   %\"val_684\"<INT64,[4]> ⬅️ ::Concat(%\"val_41\", %\"val_20\"{[8]}, %\"val_42\"{[64]}, %\"val_42\"{[64]}) {axis=0}\n",
              "            293 |  # node_view_21\n",
              "                   %\"view_21\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Reshape(%\"add_498\", %\"val_684\") {allowzero=1}\n",
              "            294 |  # node_softmax_1\n",
              "                   %\"softmax_1\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Softmax(%\"view_21\") {axis=-1}\n",
              "            295 |  # node_matmul_3\n",
              "                   %\"matmul_3\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::MatMul(%\"softmax_1\", %\"getitem_5\")\n",
              "            296 |  # node_permute_9\n",
              "                   %\"permute_9\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),8,32]> ⬅️ ::Transpose(%\"matmul_3\") {perm=(0, 2, 1, 3)}\n",
              "            297 |  # node_view_22\n",
              "                   %\"view_22\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Reshape(%\"permute_9\", %\"val_56\") {allowzero=1}\n",
              "            298 |  # node_MatMul_681\n",
              "                   %\"val_691\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::MatMul(%\"view_22\", %\"val_690\"{...})\n",
              "            299 |  # node_linear_5\n",
              "                   %\"linear_5\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Add(%\"val_691\", %\"layers.0.blocks.1.attn.proj.bias\"{...})\n",
              "            300 |  # node_view_23\n",
              "                   %\"view_23\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,8,256]> ⬅️ ::Reshape(%\"linear_5\", %\"val_64\"{[-1, 8, 8, 256]}) {allowzero=1}\n",
              "            301 |  # node_view_24\n",
              "                   %\"view_24\"<FLOAT,[1,(s53//8),(s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))),8,((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Reshape(%\"view_23\", %\"val_72\") {allowzero=1}\n",
              "            302 |  # node_permute_10\n",
              "                   %\"permute_10\"<FLOAT,[1,(s53//8),8,(s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))),((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Transpose(%\"view_24\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            303 |  # node_view_25\n",
              "                   %\"view_25\"<FLOAT,[1,8*((s53//8)),((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"permute_10\", %\"val_78\") {allowzero=1}\n",
              "            304 |  # node_Shape_704\n",
              "                   %\"val_714\"<INT64,[1]> ⬅️ ::Shape(%\"view_25\") {end=2, start=1}\n",
              "            305 |  # node_Sub_706\n",
              "                   %\"val_716\"<INT64,[1]> ⬅️ ::Sub(%\"val_714\", %\"val_100\"{[4]})\n",
              "            306 |  # node_Slice_708\n",
              "                   %\"val_718\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"view_25\", %\"val_144\"{[0]}, %\"val_716\", %\"val_152\"{[1]})\n",
              "            307 |  # node_Size_709\n",
              "                   %\"val_719\"<INT64,[]> ⬅️ ::Size(%\"view_25\")\n",
              "            308 |  # node_Reshape_710\n",
              "                   %\"val_720\"<INT64,[1]> ⬅️ ::Reshape(%\"val_719\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            309 |  # node_Slice_711\n",
              "                   %\"val_721\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"view_25\", %\"val_716\", %\"val_720\", %\"val_152\"{[1]})\n",
              "            310 |  # node_Concat_712\n",
              "                   %\"val_722\"<FLOAT,[None,None,None,None]> ⬅️ ::Concat(%\"val_721\", %\"val_718\") {axis=1}\n",
              "            311 |  # node_Shape_715\n",
              "                   %\"val_725\"<INT64,[1]> ⬅️ ::Shape(%\"val_722\") {end=3, start=2}\n",
              "            312 |  # node_Sub_717\n",
              "                   %\"val_727\"<INT64,[1]> ⬅️ ::Sub(%\"val_725\", %\"val_100\"{[4]})\n",
              "            313 |  # node_Slice_719\n",
              "                   %\"val_729\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"val_722\", %\"val_144\"{[0]}, %\"val_727\", %\"val_163\"{[2]})\n",
              "            314 |  # node_Size_720\n",
              "                   %\"val_730\"<INT64,[]> ⬅️ ::Size(%\"val_722\")\n",
              "            315 |  # node_Reshape_721\n",
              "                   %\"val_731\"<INT64,[1]> ⬅️ ::Reshape(%\"val_730\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            316 |  # node_Slice_722\n",
              "                   %\"val_732\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"val_722\", %\"val_727\", %\"val_731\", %\"val_163\"{[2]})\n",
              "            317 |  # node_roll_1\n",
              "                   %\"roll_1\"<FLOAT,[1,8*((s53//8)),((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Concat(%\"val_732\", %\"val_729\") {axis=2}\n",
              "            318 |  # node_view_26\n",
              "                   %\"view_26\"<FLOAT,[1,8*((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"roll_1\", %\"val_83\") {allowzero=1}\n",
              "            319 |  # node_add_571\n",
              "                   %\"add_571\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_187\", %\"view_26\")\n",
              "            320 |  # node_layer_norm_3\n",
              "                   %\"layer_norm_3\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_571\", %\"layers.0.blocks.1.norm2.weight\"{...}, %\"layers.0.blocks.1.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            321 |  # node_MatMul_729\n",
              "                   %\"val_741\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::MatMul(%\"layer_norm_3\", %\"val_740\"{...})\n",
              "            322 |  # node_linear_6\n",
              "                   %\"linear_6\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Add(%\"val_741\", %\"layers.0.blocks.1.mlp.0.bias\"{...})\n",
              "            323 |  # node_gelu_1\n",
              "                   %\"gelu_1\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Gelu(%\"linear_6\") {approximate='none'}\n",
              "            324 |  # node_MatMul_731\n",
              "                   %\"val_743\"<FLOAT,[1,s0*s53,256]> ⬅️ ::MatMul(%\"gelu_1\", %\"val_742\"{...})\n",
              "            325 |  # node_linear_7\n",
              "                   %\"linear_7\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"val_743\", %\"layers.0.blocks.1.mlp.2.bias\"{...})\n",
              "            326 |  # node_add_587\n",
              "                   %\"add_587\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_571\", %\"linear_7\")\n",
              "            327 |  # node_layer_norm_4\n",
              "                   %\"layer_norm_4\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_587\", %\"layers.0.blocks.2.norm1.weight\"{...}, %\"layers.0.blocks.2.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            328 |  # node_view_27\n",
              "                   %\"view_27\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Reshape(%\"layer_norm_4\", %\"val_15\") {allowzero=1}\n",
              "            329 |  # node_view_28\n",
              "                   %\"view_28\"<FLOAT,[1,(s53//8),(s53//((s53//8))),(s0//8),(s0//((s0//8))),256]> ⬅️ ::Reshape(%\"view_27\", %\"val_24\") {allowzero=1}\n",
              "            330 |  # node_permute_11\n",
              "                   %\"permute_11\"<FLOAT,[1,(s53//8),(s0//8),(s53//((s53//8))),(s0//((s0//8))),256]> ⬅️ ::Transpose(%\"view_28\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            331 |  # node_Reshape_4903\n",
              "                   %\"view_30\"<FLOAT,[((s0//8))*((s53//8)),64,256]> ⬅️ ::Reshape(%\"permute_11\", %\"val_35\"{[-1, 64, 256]})\n",
              "            332 |  # node_MatMul_758\n",
              "                   %\"val_772\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::MatMul(%\"view_30\", %\"val_771\"{...})\n",
              "            333 |  # node_linear_8\n",
              "                   %\"linear_8\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::Add(%\"val_772\", %\"layers.0.blocks.2.attn.qkv.bias\"{...})\n",
              "            334 |  # node_view_31\n",
              "                   %\"view_31\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),3,8,32]> ⬅️ ::Reshape(%\"linear_8\", %\"val_46\") {allowzero=1}\n",
              "            335 |  # node_permute_12\n",
              "                   %\"permute_12\"<FLOAT,[3,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Transpose(%\"view_31\") {perm=(2, 0, 3, 1, 4)}\n",
              "            336 |  # node_Split_766\n",
              "                   %\"val_780\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_781\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_782\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Split(%\"permute_12\") {num_outputs=3, axis=0}\n",
              "            337 |  # node_unbind_2__0\n",
              "                   %\"getitem_6\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_780\", %\"val_144\"{[0]})\n",
              "            338 |  # node_unbind_2__1\n",
              "                   %\"getitem_7\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_781\", %\"val_144\"{[0]})\n",
              "            339 |  # node_unbind_2__2\n",
              "                   %\"getitem_8\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_782\", %\"val_144\"{[0]})\n",
              "            340 |  # node_transpose_2\n",
              "                   %\"transpose_2\"<FLOAT,[((s0//8))*((s53//8)),8,32,((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Transpose(%\"getitem_7\") {perm=(0, 1, 3, 2)}\n",
              "            341 |  # node_matmul_4\n",
              "                   %\"matmul_4\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::MatMul(%\"getitem_6\", %\"transpose_2\")\n",
              "            342 |  # node_mul_1100\n",
              "                   %\"mul_1100\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Mul(%\"matmul_4\", %\"val_51\"{0.1767766922712326})\n",
              "            343 |  # node_softmax_2\n",
              "                   %\"softmax_2\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Softmax(%\"mul_1100\") {axis=-1}\n",
              "            344 |  # node_matmul_5\n",
              "                   %\"matmul_5\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::MatMul(%\"softmax_2\", %\"getitem_8\")\n",
              "            345 |  # node_permute_13\n",
              "                   %\"permute_13\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),8,32]> ⬅️ ::Transpose(%\"matmul_5\") {perm=(0, 2, 1, 3)}\n",
              "            346 |  # node_view_32\n",
              "                   %\"view_32\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Reshape(%\"permute_13\", %\"val_56\") {allowzero=1}\n",
              "            347 |  # node_MatMul_773\n",
              "                   %\"val_789\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::MatMul(%\"view_32\", %\"val_788\"{...})\n",
              "            348 |  # node_linear_9\n",
              "                   %\"linear_9\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Add(%\"val_789\", %\"layers.0.blocks.2.attn.proj.bias\"{...})\n",
              "            349 |  # node_view_33\n",
              "                   %\"view_33\"<FLOAT,[((s0//8))*((s53//8)),8,8,256]> ⬅️ ::Reshape(%\"linear_9\", %\"val_64\"{[-1, 8, 8, 256]}) {allowzero=1}\n",
              "            350 |  # node_view_34\n",
              "                   %\"view_34\"<FLOAT,[1,(s53//8),(s0//8),8,((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Reshape(%\"view_33\", %\"val_72\") {allowzero=1}\n",
              "            351 |  # node_permute_14\n",
              "                   %\"permute_14\"<FLOAT,[1,(s53//8),8,(s0//8),((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Transpose(%\"view_34\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            352 |  # node_view_35\n",
              "                   %\"view_35\"<FLOAT,[1,8*((s53//8)),((s0//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"permute_14\", %\"val_78\") {allowzero=1}\n",
              "            353 |  # node_view_36\n",
              "                   %\"view_36\"<FLOAT,[1,8*((s0//8))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"view_35\", %\"val_83\") {allowzero=1}\n",
              "            354 |  # node_add_734\n",
              "                   %\"add_734\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_587\", %\"view_36\")\n",
              "            355 |  # node_layer_norm_5\n",
              "                   %\"layer_norm_5\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_734\", %\"layers.0.blocks.2.norm2.weight\"{...}, %\"layers.0.blocks.2.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            356 |  # node_MatMul_800\n",
              "                   %\"val_818\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::MatMul(%\"layer_norm_5\", %\"val_817\"{...})\n",
              "            357 |  # node_linear_10\n",
              "                   %\"linear_10\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Add(%\"val_818\", %\"layers.0.blocks.2.mlp.0.bias\"{...})\n",
              "            358 |  # node_gelu_2\n",
              "                   %\"gelu_2\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Gelu(%\"linear_10\") {approximate='none'}\n",
              "            359 |  # node_MatMul_802\n",
              "                   %\"val_820\"<FLOAT,[1,s0*s53,256]> ⬅️ ::MatMul(%\"gelu_2\", %\"val_819\"{...})\n",
              "            360 |  # node_linear_11\n",
              "                   %\"linear_11\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"val_820\", %\"layers.0.blocks.2.mlp.2.bias\"{...})\n",
              "            361 |  # node_add_750\n",
              "                   %\"add_750\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_734\", %\"linear_11\")\n",
              "            362 |  # node_layer_norm_6\n",
              "                   %\"layer_norm_6\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_750\", %\"layers.0.blocks.3.norm1.weight\"{...}, %\"layers.0.blocks.3.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            363 |  # node_view_37\n",
              "                   %\"view_37\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Reshape(%\"layer_norm_6\", %\"val_15\") {allowzero=1}\n",
              "            364 |  # node_Slice_813\n",
              "                   %\"val_833\"<FLOAT,[1,None,s0,256]> ⬅️ ::Slice(%\"view_37\", %\"val_144\"{[0]}, %\"val_100\"{[4]}, %\"val_152\"{[1]})\n",
              "            365 |  # node_Size_814\n",
              "                   %\"val_834\"<INT64,[]> ⬅️ ::Size(%\"view_37\")\n",
              "            366 |  # node_Reshape_815\n",
              "                   %\"val_835\"<INT64,[1]> ⬅️ ::Reshape(%\"val_834\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            367 |  # node_Slice_816\n",
              "                   %\"val_836\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"view_37\", %\"val_100\"{[4]}, %\"val_835\", %\"val_152\"{[1]})\n",
              "            368 |  # node_Concat_817\n",
              "                   %\"val_837\"<FLOAT,[1,None,s0,256]> ⬅️ ::Concat(%\"val_836\", %\"val_833\") {axis=1}\n",
              "            369 |  # node_Slice_822\n",
              "                   %\"val_842\"<FLOAT,[1,None,None,256]> ⬅️ ::Slice(%\"val_837\", %\"val_144\"{[0]}, %\"val_100\"{[4]}, %\"val_163\"{[2]})\n",
              "            370 |  # node_Size_823\n",
              "                   %\"val_843\"<INT64,[]> ⬅️ ::Size(%\"val_837\")\n",
              "            371 |  # node_Reshape_824\n",
              "                   %\"val_844\"<INT64,[1]> ⬅️ ::Reshape(%\"val_843\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            372 |  # node_Slice_825\n",
              "                   %\"val_845\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"val_837\", %\"val_100\"{[4]}, %\"val_844\", %\"val_163\"{[2]})\n",
              "            373 |  # node_roll_2\n",
              "                   %\"roll_2\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Concat(%\"val_845\", %\"val_842\") {axis=2}\n",
              "            374 |  # node_view_38\n",
              "                   %\"view_38\"<FLOAT,[1,(s53//8),(s53//((s53//8))),(s0//8),(s0//((s0//8))),256]> ⬅️ ::Reshape(%\"roll_2\", %\"val_24\") {allowzero=1}\n",
              "            375 |  # node_permute_15\n",
              "                   %\"permute_15\"<FLOAT,[1,(s53//8),(s0//8),(s53//((s53//8))),(s0//((s0//8))),256]> ⬅️ ::Transpose(%\"view_38\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            376 |  # node_Reshape_4910\n",
              "                   %\"view_40\"<FLOAT,[((s0//8))*((s53//8)),64,256]> ⬅️ ::Reshape(%\"permute_15\", %\"val_35\"{[-1, 64, 256]})\n",
              "            377 |  # node_MatMul_1360\n",
              "                   %\"val_1380\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::MatMul(%\"view_40\", %\"val_1379\"{...})\n",
              "            378 |  # node_linear_12\n",
              "                   %\"linear_12\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::Add(%\"val_1380\", %\"layers.0.blocks.3.attn.qkv.bias\"{...})\n",
              "            379 |  # node_view_45\n",
              "                   %\"view_45\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),3,8,32]> ⬅️ ::Reshape(%\"linear_12\", %\"val_46\") {allowzero=1}\n",
              "            380 |  # node_permute_18\n",
              "                   %\"permute_18\"<FLOAT,[3,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Transpose(%\"view_45\") {perm=(2, 0, 3, 1, 4)}\n",
              "            381 |  # node_Split_1368\n",
              "                   %\"val_1388\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_1389\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_1390\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Split(%\"permute_18\") {num_outputs=3, axis=0}\n",
              "            382 |  # node_unbind_3__0\n",
              "                   %\"getitem_9\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_1388\", %\"val_144\"{[0]})\n",
              "            383 |  # node_unbind_3__1\n",
              "                   %\"getitem_10\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_1389\", %\"val_144\"{[0]})\n",
              "            384 |  # node_unbind_3__2\n",
              "                   %\"getitem_11\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_1390\", %\"val_144\"{[0]})\n",
              "            385 |  # node_transpose_3\n",
              "                   %\"transpose_3\"<FLOAT,[((s0//8))*((s53//8)),8,32,((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Transpose(%\"getitem_10\") {perm=(0, 1, 3, 2)}\n",
              "            386 |  # node_matmul_6\n",
              "                   %\"matmul_6\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::MatMul(%\"getitem_9\", %\"transpose_3\")\n",
              "            387 |  # node_mul_1431\n",
              "                   %\"mul_1431\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Mul(%\"matmul_6\", %\"val_51\"{0.1767766922712326})\n",
              "            388 |  # node_view_46\n",
              "                   %\"view_46\"<FLOAT,[((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))),((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Reshape(%\"mul_1431\", %\"val_678\") {allowzero=1}\n",
              "            389 |  # node_add_1061\n",
              "                   %\"add_1061\"<FLOAT,[1,((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,64,64]> ⬅️ ::Add(%\"view_46\", %\"unsqueeze_3\")\n",
              "            390 |  # node_view_47\n",
              "                   %\"view_47\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Reshape(%\"add_1061\", %\"val_684\") {allowzero=1}\n",
              "            391 |  # node_softmax_3\n",
              "                   %\"softmax_3\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Softmax(%\"view_47\") {axis=-1}\n",
              "            392 |  # node_matmul_7\n",
              "                   %\"matmul_7\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::MatMul(%\"softmax_3\", %\"getitem_11\")\n",
              "            393 |  # node_permute_19\n",
              "                   %\"permute_19\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),8,32]> ⬅️ ::Transpose(%\"matmul_7\") {perm=(0, 2, 1, 3)}\n",
              "            394 |  # node_view_48\n",
              "                   %\"view_48\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Reshape(%\"permute_19\", %\"val_56\") {allowzero=1}\n",
              "            395 |  # node_MatMul_1388\n",
              "                   %\"val_1410\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::MatMul(%\"view_48\", %\"val_1409\"{...})\n",
              "            396 |  # node_linear_13\n",
              "                   %\"linear_13\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Add(%\"val_1410\", %\"layers.0.blocks.3.attn.proj.bias\"{...})\n",
              "            397 |  # node_view_49\n",
              "                   %\"view_49\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,8,256]> ⬅️ ::Reshape(%\"linear_13\", %\"val_64\"{[-1, 8, 8, 256]}) {allowzero=1}\n",
              "            398 |  # node_view_50\n",
              "                   %\"view_50\"<FLOAT,[1,(s53//8),(s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))),8,((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Reshape(%\"view_49\", %\"val_72\") {allowzero=1}\n",
              "            399 |  # node_permute_20\n",
              "                   %\"permute_20\"<FLOAT,[1,(s53//8),8,(s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))),((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Transpose(%\"view_50\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            400 |  # node_view_51\n",
              "                   %\"view_51\"<FLOAT,[1,8*((s53//8)),((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"permute_20\", %\"val_78\") {allowzero=1}\n",
              "            401 |  # node_Shape_1411\n",
              "                   %\"val_1433\"<INT64,[1]> ⬅️ ::Shape(%\"view_51\") {end=2, start=1}\n",
              "            402 |  # node_Sub_1413\n",
              "                   %\"val_1435\"<INT64,[1]> ⬅️ ::Sub(%\"val_1433\", %\"val_100\"{[4]})\n",
              "            403 |  # node_Slice_1415\n",
              "                   %\"val_1437\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"view_51\", %\"val_144\"{[0]}, %\"val_1435\", %\"val_152\"{[1]})\n",
              "            404 |  # node_Size_1416\n",
              "                   %\"val_1438\"<INT64,[]> ⬅️ ::Size(%\"view_51\")\n",
              "            405 |  # node_Reshape_1417\n",
              "                   %\"val_1439\"<INT64,[1]> ⬅️ ::Reshape(%\"val_1438\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            406 |  # node_Slice_1418\n",
              "                   %\"val_1440\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"view_51\", %\"val_1435\", %\"val_1439\", %\"val_152\"{[1]})\n",
              "            407 |  # node_Concat_1419\n",
              "                   %\"val_1441\"<FLOAT,[None,None,None,None]> ⬅️ ::Concat(%\"val_1440\", %\"val_1437\") {axis=1}\n",
              "            408 |  # node_Shape_1422\n",
              "                   %\"val_1444\"<INT64,[1]> ⬅️ ::Shape(%\"val_1441\") {end=3, start=2}\n",
              "            409 |  # node_Sub_1424\n",
              "                   %\"val_1446\"<INT64,[1]> ⬅️ ::Sub(%\"val_1444\", %\"val_100\"{[4]})\n",
              "            410 |  # node_Slice_1426\n",
              "                   %\"val_1448\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"val_1441\", %\"val_144\"{[0]}, %\"val_1446\", %\"val_163\"{[2]})\n",
              "            411 |  # node_Size_1427\n",
              "                   %\"val_1449\"<INT64,[]> ⬅️ ::Size(%\"val_1441\")\n",
              "            412 |  # node_Reshape_1428\n",
              "                   %\"val_1450\"<INT64,[1]> ⬅️ ::Reshape(%\"val_1449\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            413 |  # node_Slice_1429\n",
              "                   %\"val_1451\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"val_1441\", %\"val_1446\", %\"val_1450\", %\"val_163\"{[2]})\n",
              "            414 |  # node_roll_3\n",
              "                   %\"roll_3\"<FLOAT,[1,8*((s53//8)),((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Concat(%\"val_1451\", %\"val_1448\") {axis=2}\n",
              "            415 |  # node_view_52\n",
              "                   %\"view_52\"<FLOAT,[1,8*((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"roll_3\", %\"val_83\") {allowzero=1}\n",
              "            416 |  # node_add_1134\n",
              "                   %\"add_1134\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_750\", %\"view_52\")\n",
              "            417 |  # node_layer_norm_7\n",
              "                   %\"layer_norm_7\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1134\", %\"layers.0.blocks.3.norm2.weight\"{...}, %\"layers.0.blocks.3.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            418 |  # node_MatMul_1436\n",
              "                   %\"val_1460\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::MatMul(%\"layer_norm_7\", %\"val_1459\"{...})\n",
              "            419 |  # node_linear_14\n",
              "                   %\"linear_14\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Add(%\"val_1460\", %\"layers.0.blocks.3.mlp.0.bias\"{...})\n",
              "            420 |  # node_gelu_3\n",
              "                   %\"gelu_3\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Gelu(%\"linear_14\") {approximate='none'}\n",
              "            421 |  # node_MatMul_1438\n",
              "                   %\"val_1462\"<FLOAT,[1,s0*s53,256]> ⬅️ ::MatMul(%\"gelu_3\", %\"val_1461\"{...})\n",
              "            422 |  # node_linear_15\n",
              "                   %\"linear_15\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"val_1462\", %\"layers.0.blocks.3.mlp.2.bias\"{...})\n",
              "            423 |  # node_add_1150\n",
              "                   %\"add_1150\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_1134\", %\"linear_15\")\n",
              "            424 |  # node_Concat_1444\n",
              "                   %\"val_1468\"<INT64,[4]> ⬅️ ::Concat(%\"val_0\", %\"val_1\", %\"val_2\", %\"val_6\"{[256]}) {axis=0}\n",
              "            425 |  # node_view_53\n",
              "                   %\"view_53\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Reshape(%\"add_1150\", %\"val_1468\") {allowzero=1}\n",
              "            426 |  # node_permute_21\n",
              "                   %\"permute_21\"<FLOAT,[1,256,s53,s0]> ⬅️ ::Transpose(%\"view_53\") {perm=(0, 3, 1, 2)}\n",
              "            427 |  # node_conv2d_1\n",
              "                   %\"conv2d_1\"<FLOAT,[1,256,s53,s0]> ⬅️ ::Conv(%\"permute_21\", %\"layers.0.conv.weight\"{...}, %\"layers.0.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
              "            428 |  # node_permute_22\n",
              "                   %\"permute_22\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Transpose(%\"conv2d_1\") {perm=(0, 2, 3, 1)}\n",
              "            429 |  # node_add_1177\n",
              "                   %\"add_1177\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Add(%\"permute\", %\"permute_22\")\n",
              "            430 |  # node_view_54\n",
              "                   %\"view_54\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Reshape(%\"add_1177\", %\"val_7\") {allowzero=1}\n",
              "            431 |  # node_layer_norm_8\n",
              "                   %\"layer_norm_8\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"view_54\", %\"layers.1.blocks.0.norm1.weight\"{...}, %\"layers.1.blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            432 |  # node_view_55\n",
              "                   %\"view_55\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Reshape(%\"layer_norm_8\", %\"val_15\") {allowzero=1}\n",
              "            433 |  # node_view_56\n",
              "                   %\"view_56\"<FLOAT,[1,(s53//8),(s53//((s53//8))),(s0//8),(s0//((s0//8))),256]> ⬅️ ::Reshape(%\"view_55\", %\"val_24\") {allowzero=1}\n",
              "            434 |  # node_permute_23\n",
              "                   %\"permute_23\"<FLOAT,[1,(s53//8),(s0//8),(s53//((s53//8))),(s0//((s0//8))),256]> ⬅️ ::Transpose(%\"view_56\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            435 |  # node_Reshape_4934\n",
              "                   %\"view_58\"<FLOAT,[((s0//8))*((s53//8)),64,256]> ⬅️ ::Reshape(%\"permute_23\", %\"val_35\"{[-1, 64, 256]})\n",
              "            436 |  # node_MatMul_1476\n",
              "                   %\"val_1502\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::MatMul(%\"view_58\", %\"val_1501\"{...})\n",
              "            437 |  # node_linear_16\n",
              "                   %\"linear_16\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::Add(%\"val_1502\", %\"layers.1.blocks.0.attn.qkv.bias\"{...})\n",
              "            438 |  # node_view_59\n",
              "                   %\"view_59\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),3,8,32]> ⬅️ ::Reshape(%\"linear_16\", %\"val_46\") {allowzero=1}\n",
              "            439 |  # node_permute_24\n",
              "                   %\"permute_24\"<FLOAT,[3,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Transpose(%\"view_59\") {perm=(2, 0, 3, 1, 4)}\n",
              "            440 |  # node_Split_1484\n",
              "                   %\"val_1510\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_1511\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_1512\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Split(%\"permute_24\") {num_outputs=3, axis=0}\n",
              "            441 |  # node_unbind_4__0\n",
              "                   %\"getitem_12\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_1510\", %\"val_144\"{[0]})\n",
              "            442 |  # node_unbind_4__1\n",
              "                   %\"getitem_13\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_1511\", %\"val_144\"{[0]})\n",
              "            443 |  # node_unbind_4__2\n",
              "                   %\"getitem_14\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_1512\", %\"val_144\"{[0]})\n",
              "            444 |  # node_transpose_4\n",
              "                   %\"transpose_4\"<FLOAT,[((s0//8))*((s53//8)),8,32,((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Transpose(%\"getitem_13\") {perm=(0, 1, 3, 2)}\n",
              "            445 |  # node_matmul_8\n",
              "                   %\"matmul_8\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::MatMul(%\"getitem_12\", %\"transpose_4\")\n",
              "            446 |  # node_mul_1667\n",
              "                   %\"mul_1667\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Mul(%\"matmul_8\", %\"val_51\"{0.1767766922712326})\n",
              "            447 |  # node_softmax_4\n",
              "                   %\"softmax_4\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Softmax(%\"mul_1667\") {axis=-1}\n",
              "            448 |  # node_matmul_9\n",
              "                   %\"matmul_9\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::MatMul(%\"softmax_4\", %\"getitem_14\")\n",
              "            449 |  # node_permute_25\n",
              "                   %\"permute_25\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),8,32]> ⬅️ ::Transpose(%\"matmul_9\") {perm=(0, 2, 1, 3)}\n",
              "            450 |  # node_view_60\n",
              "                   %\"view_60\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Reshape(%\"permute_25\", %\"val_56\") {allowzero=1}\n",
              "            451 |  # node_MatMul_1491\n",
              "                   %\"val_1519\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::MatMul(%\"view_60\", %\"val_1518\"{...})\n",
              "            452 |  # node_linear_17\n",
              "                   %\"linear_17\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Add(%\"val_1519\", %\"layers.1.blocks.0.attn.proj.bias\"{...})\n",
              "            453 |  # node_view_61\n",
              "                   %\"view_61\"<FLOAT,[((s0//8))*((s53//8)),8,8,256]> ⬅️ ::Reshape(%\"linear_17\", %\"val_64\"{[-1, 8, 8, 256]}) {allowzero=1}\n",
              "            454 |  # node_view_62\n",
              "                   %\"view_62\"<FLOAT,[1,(s53//8),(s0//8),8,((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Reshape(%\"view_61\", %\"val_72\") {allowzero=1}\n",
              "            455 |  # node_permute_26\n",
              "                   %\"permute_26\"<FLOAT,[1,(s53//8),8,(s0//8),((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Transpose(%\"view_62\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            456 |  # node_view_63\n",
              "                   %\"view_63\"<FLOAT,[1,8*((s53//8)),((s0//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"permute_26\", %\"val_78\") {allowzero=1}\n",
              "            457 |  # node_view_64\n",
              "                   %\"view_64\"<FLOAT,[1,8*((s0//8))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"view_63\", %\"val_83\") {allowzero=1}\n",
              "            458 |  # node_add_1328\n",
              "                   %\"add_1328\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"view_54\", %\"view_64\")\n",
              "            459 |  # node_layer_norm_9\n",
              "                   %\"layer_norm_9\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1328\", %\"layers.1.blocks.0.norm2.weight\"{...}, %\"layers.1.blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            460 |  # node_MatMul_1518\n",
              "                   %\"val_1548\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::MatMul(%\"layer_norm_9\", %\"val_1547\"{...})\n",
              "            461 |  # node_linear_18\n",
              "                   %\"linear_18\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Add(%\"val_1548\", %\"layers.1.blocks.0.mlp.0.bias\"{...})\n",
              "            462 |  # node_gelu_4\n",
              "                   %\"gelu_4\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Gelu(%\"linear_18\") {approximate='none'}\n",
              "            463 |  # node_MatMul_1520\n",
              "                   %\"val_1550\"<FLOAT,[1,s0*s53,256]> ⬅️ ::MatMul(%\"gelu_4\", %\"val_1549\"{...})\n",
              "            464 |  # node_linear_19\n",
              "                   %\"linear_19\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"val_1550\", %\"layers.1.blocks.0.mlp.2.bias\"{...})\n",
              "            465 |  # node_add_1344\n",
              "                   %\"add_1344\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_1328\", %\"linear_19\")\n",
              "            466 |  # node_layer_norm_10\n",
              "                   %\"layer_norm_10\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1344\", %\"layers.1.blocks.1.norm1.weight\"{...}, %\"layers.1.blocks.1.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            467 |  # node_view_65\n",
              "                   %\"view_65\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Reshape(%\"layer_norm_10\", %\"val_15\") {allowzero=1}\n",
              "            468 |  # node_Slice_1531\n",
              "                   %\"val_1563\"<FLOAT,[1,None,s0,256]> ⬅️ ::Slice(%\"view_65\", %\"val_144\"{[0]}, %\"val_100\"{[4]}, %\"val_152\"{[1]})\n",
              "            469 |  # node_Size_1532\n",
              "                   %\"val_1564\"<INT64,[]> ⬅️ ::Size(%\"view_65\")\n",
              "            470 |  # node_Reshape_1533\n",
              "                   %\"val_1565\"<INT64,[1]> ⬅️ ::Reshape(%\"val_1564\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            471 |  # node_Slice_1534\n",
              "                   %\"val_1566\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"view_65\", %\"val_100\"{[4]}, %\"val_1565\", %\"val_152\"{[1]})\n",
              "            472 |  # node_Concat_1535\n",
              "                   %\"val_1567\"<FLOAT,[1,None,s0,256]> ⬅️ ::Concat(%\"val_1566\", %\"val_1563\") {axis=1}\n",
              "            473 |  # node_Slice_1540\n",
              "                   %\"val_1572\"<FLOAT,[1,None,None,256]> ⬅️ ::Slice(%\"val_1567\", %\"val_144\"{[0]}, %\"val_100\"{[4]}, %\"val_163\"{[2]})\n",
              "            474 |  # node_Size_1541\n",
              "                   %\"val_1573\"<INT64,[]> ⬅️ ::Size(%\"val_1567\")\n",
              "            475 |  # node_Reshape_1542\n",
              "                   %\"val_1574\"<INT64,[1]> ⬅️ ::Reshape(%\"val_1573\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            476 |  # node_Slice_1543\n",
              "                   %\"val_1575\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"val_1567\", %\"val_100\"{[4]}, %\"val_1574\", %\"val_163\"{[2]})\n",
              "            477 |  # node_roll_4\n",
              "                   %\"roll_4\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Concat(%\"val_1575\", %\"val_1572\") {axis=2}\n",
              "            478 |  # node_view_66\n",
              "                   %\"view_66\"<FLOAT,[1,(s53//8),(s53//((s53//8))),(s0//8),(s0//((s0//8))),256]> ⬅️ ::Reshape(%\"roll_4\", %\"val_24\") {allowzero=1}\n",
              "            479 |  # node_permute_27\n",
              "                   %\"permute_27\"<FLOAT,[1,(s53//8),(s0//8),(s53//((s53//8))),(s0//((s0//8))),256]> ⬅️ ::Transpose(%\"view_66\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            480 |  # node_Reshape_4941\n",
              "                   %\"view_68\"<FLOAT,[((s0//8))*((s53//8)),64,256]> ⬅️ ::Reshape(%\"permute_27\", %\"val_35\"{[-1, 64, 256]})\n",
              "            481 |  # node_MatMul_2078\n",
              "                   %\"val_2110\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::MatMul(%\"view_68\", %\"val_2109\"{...})\n",
              "            482 |  # node_linear_20\n",
              "                   %\"linear_20\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::Add(%\"val_2110\", %\"layers.1.blocks.1.attn.qkv.bias\"{...})\n",
              "            483 |  # node_view_73\n",
              "                   %\"view_73\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),3,8,32]> ⬅️ ::Reshape(%\"linear_20\", %\"val_46\") {allowzero=1}\n",
              "            484 |  # node_permute_30\n",
              "                   %\"permute_30\"<FLOAT,[3,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Transpose(%\"view_73\") {perm=(2, 0, 3, 1, 4)}\n",
              "            485 |  # node_Split_2086\n",
              "                   %\"val_2118\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_2119\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_2120\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Split(%\"permute_30\") {num_outputs=3, axis=0}\n",
              "            486 |  # node_unbind_5__0\n",
              "                   %\"getitem_15\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_2118\", %\"val_144\"{[0]})\n",
              "            487 |  # node_unbind_5__1\n",
              "                   %\"getitem_16\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_2119\", %\"val_144\"{[0]})\n",
              "            488 |  # node_unbind_5__2\n",
              "                   %\"getitem_17\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_2120\", %\"val_144\"{[0]})\n",
              "            489 |  # node_transpose_5\n",
              "                   %\"transpose_5\"<FLOAT,[((s0//8))*((s53//8)),8,32,((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Transpose(%\"getitem_16\") {perm=(0, 1, 3, 2)}\n",
              "            490 |  # node_matmul_10\n",
              "                   %\"matmul_10\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::MatMul(%\"getitem_15\", %\"transpose_5\")\n",
              "            491 |  # node_mul_1998\n",
              "                   %\"mul_1998\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Mul(%\"matmul_10\", %\"val_51\"{0.1767766922712326})\n",
              "            492 |  # node_view_74\n",
              "                   %\"view_74\"<FLOAT,[((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))),((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Reshape(%\"mul_1998\", %\"val_678\") {allowzero=1}\n",
              "            493 |  # node_add_1655\n",
              "                   %\"add_1655\"<FLOAT,[1,((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,64,64]> ⬅️ ::Add(%\"view_74\", %\"unsqueeze_3\")\n",
              "            494 |  # node_view_75\n",
              "                   %\"view_75\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Reshape(%\"add_1655\", %\"val_684\") {allowzero=1}\n",
              "            495 |  # node_softmax_5\n",
              "                   %\"softmax_5\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Softmax(%\"view_75\") {axis=-1}\n",
              "            496 |  # node_matmul_11\n",
              "                   %\"matmul_11\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::MatMul(%\"softmax_5\", %\"getitem_17\")\n",
              "            497 |  # node_permute_31\n",
              "                   %\"permute_31\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),8,32]> ⬅️ ::Transpose(%\"matmul_11\") {perm=(0, 2, 1, 3)}\n",
              "            498 |  # node_view_76\n",
              "                   %\"view_76\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Reshape(%\"permute_31\", %\"val_56\") {allowzero=1}\n",
              "            499 |  # node_MatMul_2106\n",
              "                   %\"val_2140\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::MatMul(%\"view_76\", %\"val_2139\"{...})\n",
              "            500 |  # node_linear_21\n",
              "                   %\"linear_21\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Add(%\"val_2140\", %\"layers.1.blocks.1.attn.proj.bias\"{...})\n",
              "            501 |  # node_view_77\n",
              "                   %\"view_77\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,8,256]> ⬅️ ::Reshape(%\"linear_21\", %\"val_64\"{[-1, 8, 8, 256]}) {allowzero=1}\n",
              "            502 |  # node_view_78\n",
              "                   %\"view_78\"<FLOAT,[1,(s53//8),(s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))),8,((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Reshape(%\"view_77\", %\"val_72\") {allowzero=1}\n",
              "            503 |  # node_permute_32\n",
              "                   %\"permute_32\"<FLOAT,[1,(s53//8),8,(s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))),((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Transpose(%\"view_78\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            504 |  # node_view_79\n",
              "                   %\"view_79\"<FLOAT,[1,8*((s53//8)),((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"permute_32\", %\"val_78\") {allowzero=1}\n",
              "            505 |  # node_Shape_2129\n",
              "                   %\"val_2163\"<INT64,[1]> ⬅️ ::Shape(%\"view_79\") {end=2, start=1}\n",
              "            506 |  # node_Sub_2131\n",
              "                   %\"val_2165\"<INT64,[1]> ⬅️ ::Sub(%\"val_2163\", %\"val_100\"{[4]})\n",
              "            507 |  # node_Slice_2133\n",
              "                   %\"val_2167\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"view_79\", %\"val_144\"{[0]}, %\"val_2165\", %\"val_152\"{[1]})\n",
              "            508 |  # node_Size_2134\n",
              "                   %\"val_2168\"<INT64,[]> ⬅️ ::Size(%\"view_79\")\n",
              "            509 |  # node_Reshape_2135\n",
              "                   %\"val_2169\"<INT64,[1]> ⬅️ ::Reshape(%\"val_2168\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            510 |  # node_Slice_2136\n",
              "                   %\"val_2170\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"view_79\", %\"val_2165\", %\"val_2169\", %\"val_152\"{[1]})\n",
              "            511 |  # node_Concat_2137\n",
              "                   %\"val_2171\"<FLOAT,[None,None,None,None]> ⬅️ ::Concat(%\"val_2170\", %\"val_2167\") {axis=1}\n",
              "            512 |  # node_Shape_2140\n",
              "                   %\"val_2174\"<INT64,[1]> ⬅️ ::Shape(%\"val_2171\") {end=3, start=2}\n",
              "            513 |  # node_Sub_2142\n",
              "                   %\"val_2176\"<INT64,[1]> ⬅️ ::Sub(%\"val_2174\", %\"val_100\"{[4]})\n",
              "            514 |  # node_Slice_2144\n",
              "                   %\"val_2178\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"val_2171\", %\"val_144\"{[0]}, %\"val_2176\", %\"val_163\"{[2]})\n",
              "            515 |  # node_Size_2145\n",
              "                   %\"val_2179\"<INT64,[]> ⬅️ ::Size(%\"val_2171\")\n",
              "            516 |  # node_Reshape_2146\n",
              "                   %\"val_2180\"<INT64,[1]> ⬅️ ::Reshape(%\"val_2179\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            517 |  # node_Slice_2147\n",
              "                   %\"val_2181\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"val_2171\", %\"val_2176\", %\"val_2180\", %\"val_163\"{[2]})\n",
              "            518 |  # node_roll_5\n",
              "                   %\"roll_5\"<FLOAT,[1,8*((s53//8)),((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Concat(%\"val_2181\", %\"val_2178\") {axis=2}\n",
              "            519 |  # node_view_80\n",
              "                   %\"view_80\"<FLOAT,[1,8*((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"roll_5\", %\"val_83\") {allowzero=1}\n",
              "            520 |  # node_add_1728\n",
              "                   %\"add_1728\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_1344\", %\"view_80\")\n",
              "            521 |  # node_layer_norm_11\n",
              "                   %\"layer_norm_11\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1728\", %\"layers.1.blocks.1.norm2.weight\"{...}, %\"layers.1.blocks.1.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            522 |  # node_MatMul_2154\n",
              "                   %\"val_2190\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::MatMul(%\"layer_norm_11\", %\"val_2189\"{...})\n",
              "            523 |  # node_linear_22\n",
              "                   %\"linear_22\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Add(%\"val_2190\", %\"layers.1.blocks.1.mlp.0.bias\"{...})\n",
              "            524 |  # node_gelu_5\n",
              "                   %\"gelu_5\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Gelu(%\"linear_22\") {approximate='none'}\n",
              "            525 |  # node_MatMul_2156\n",
              "                   %\"val_2192\"<FLOAT,[1,s0*s53,256]> ⬅️ ::MatMul(%\"gelu_5\", %\"val_2191\"{...})\n",
              "            526 |  # node_linear_23\n",
              "                   %\"linear_23\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"val_2192\", %\"layers.1.blocks.1.mlp.2.bias\"{...})\n",
              "            527 |  # node_add_1744\n",
              "                   %\"add_1744\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_1728\", %\"linear_23\")\n",
              "            528 |  # node_layer_norm_12\n",
              "                   %\"layer_norm_12\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1744\", %\"layers.1.blocks.2.norm1.weight\"{...}, %\"layers.1.blocks.2.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            529 |  # node_view_81\n",
              "                   %\"view_81\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Reshape(%\"layer_norm_12\", %\"val_15\") {allowzero=1}\n",
              "            530 |  # node_view_82\n",
              "                   %\"view_82\"<FLOAT,[1,(s53//8),(s53//((s53//8))),(s0//8),(s0//((s0//8))),256]> ⬅️ ::Reshape(%\"view_81\", %\"val_24\") {allowzero=1}\n",
              "            531 |  # node_permute_33\n",
              "                   %\"permute_33\"<FLOAT,[1,(s53//8),(s0//8),(s53//((s53//8))),(s0//((s0//8))),256]> ⬅️ ::Transpose(%\"view_82\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            532 |  # node_Reshape_4961\n",
              "                   %\"view_84\"<FLOAT,[((s0//8))*((s53//8)),64,256]> ⬅️ ::Reshape(%\"permute_33\", %\"val_35\"{[-1, 64, 256]})\n",
              "            533 |  # node_MatMul_2183\n",
              "                   %\"val_2221\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::MatMul(%\"view_84\", %\"val_2220\"{...})\n",
              "            534 |  # node_linear_24\n",
              "                   %\"linear_24\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::Add(%\"val_2221\", %\"layers.1.blocks.2.attn.qkv.bias\"{...})\n",
              "            535 |  # node_view_85\n",
              "                   %\"view_85\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),3,8,32]> ⬅️ ::Reshape(%\"linear_24\", %\"val_46\") {allowzero=1}\n",
              "            536 |  # node_permute_34\n",
              "                   %\"permute_34\"<FLOAT,[3,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Transpose(%\"view_85\") {perm=(2, 0, 3, 1, 4)}\n",
              "            537 |  # node_Split_2191\n",
              "                   %\"val_2229\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_2230\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_2231\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Split(%\"permute_34\") {num_outputs=3, axis=0}\n",
              "            538 |  # node_unbind_6__0\n",
              "                   %\"getitem_18\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_2229\", %\"val_144\"{[0]})\n",
              "            539 |  # node_unbind_6__1\n",
              "                   %\"getitem_19\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_2230\", %\"val_144\"{[0]})\n",
              "            540 |  # node_unbind_6__2\n",
              "                   %\"getitem_20\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_2231\", %\"val_144\"{[0]})\n",
              "            541 |  # node_transpose_6\n",
              "                   %\"transpose_6\"<FLOAT,[((s0//8))*((s53//8)),8,32,((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Transpose(%\"getitem_19\") {perm=(0, 1, 3, 2)}\n",
              "            542 |  # node_matmul_12\n",
              "                   %\"matmul_12\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::MatMul(%\"getitem_18\", %\"transpose_6\")\n",
              "            543 |  # node_mul_2195\n",
              "                   %\"mul_2195\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Mul(%\"matmul_12\", %\"val_51\"{0.1767766922712326})\n",
              "            544 |  # node_softmax_6\n",
              "                   %\"softmax_6\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Softmax(%\"mul_2195\") {axis=-1}\n",
              "            545 |  # node_matmul_13\n",
              "                   %\"matmul_13\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::MatMul(%\"softmax_6\", %\"getitem_20\")\n",
              "            546 |  # node_permute_35\n",
              "                   %\"permute_35\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),8,32]> ⬅️ ::Transpose(%\"matmul_13\") {perm=(0, 2, 1, 3)}\n",
              "            547 |  # node_view_86\n",
              "                   %\"view_86\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Reshape(%\"permute_35\", %\"val_56\") {allowzero=1}\n",
              "            548 |  # node_MatMul_2198\n",
              "                   %\"val_2238\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::MatMul(%\"view_86\", %\"val_2237\"{...})\n",
              "            549 |  # node_linear_25\n",
              "                   %\"linear_25\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Add(%\"val_2238\", %\"layers.1.blocks.2.attn.proj.bias\"{...})\n",
              "            550 |  # node_view_87\n",
              "                   %\"view_87\"<FLOAT,[((s0//8))*((s53//8)),8,8,256]> ⬅️ ::Reshape(%\"linear_25\", %\"val_64\"{[-1, 8, 8, 256]}) {allowzero=1}\n",
              "            551 |  # node_view_88\n",
              "                   %\"view_88\"<FLOAT,[1,(s53//8),(s0//8),8,((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Reshape(%\"view_87\", %\"val_72\") {allowzero=1}\n",
              "            552 |  # node_permute_36\n",
              "                   %\"permute_36\"<FLOAT,[1,(s53//8),8,(s0//8),((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Transpose(%\"view_88\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            553 |  # node_view_89\n",
              "                   %\"view_89\"<FLOAT,[1,8*((s53//8)),((s0//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"permute_36\", %\"val_78\") {allowzero=1}\n",
              "            554 |  # node_view_90\n",
              "                   %\"view_90\"<FLOAT,[1,8*((s0//8))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"view_89\", %\"val_83\") {allowzero=1}\n",
              "            555 |  # node_add_1891\n",
              "                   %\"add_1891\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_1744\", %\"view_90\")\n",
              "            556 |  # node_layer_norm_13\n",
              "                   %\"layer_norm_13\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1891\", %\"layers.1.blocks.2.norm2.weight\"{...}, %\"layers.1.blocks.2.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            557 |  # node_MatMul_2225\n",
              "                   %\"val_2267\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::MatMul(%\"layer_norm_13\", %\"val_2266\"{...})\n",
              "            558 |  # node_linear_26\n",
              "                   %\"linear_26\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Add(%\"val_2267\", %\"layers.1.blocks.2.mlp.0.bias\"{...})\n",
              "            559 |  # node_gelu_6\n",
              "                   %\"gelu_6\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Gelu(%\"linear_26\") {approximate='none'}\n",
              "            560 |  # node_MatMul_2227\n",
              "                   %\"val_2269\"<FLOAT,[1,s0*s53,256]> ⬅️ ::MatMul(%\"gelu_6\", %\"val_2268\"{...})\n",
              "            561 |  # node_linear_27\n",
              "                   %\"linear_27\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"val_2269\", %\"layers.1.blocks.2.mlp.2.bias\"{...})\n",
              "            562 |  # node_add_1907\n",
              "                   %\"add_1907\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_1891\", %\"linear_27\")\n",
              "            563 |  # node_layer_norm_14\n",
              "                   %\"layer_norm_14\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1907\", %\"layers.1.blocks.3.norm1.weight\"{...}, %\"layers.1.blocks.3.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            564 |  # node_view_91\n",
              "                   %\"view_91\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Reshape(%\"layer_norm_14\", %\"val_15\") {allowzero=1}\n",
              "            565 |  # node_Slice_2238\n",
              "                   %\"val_2282\"<FLOAT,[1,None,s0,256]> ⬅️ ::Slice(%\"view_91\", %\"val_144\"{[0]}, %\"val_100\"{[4]}, %\"val_152\"{[1]})\n",
              "            566 |  # node_Size_2239\n",
              "                   %\"val_2283\"<INT64,[]> ⬅️ ::Size(%\"view_91\")\n",
              "            567 |  # node_Reshape_2240\n",
              "                   %\"val_2284\"<INT64,[1]> ⬅️ ::Reshape(%\"val_2283\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            568 |  # node_Slice_2241\n",
              "                   %\"val_2285\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"view_91\", %\"val_100\"{[4]}, %\"val_2284\", %\"val_152\"{[1]})\n",
              "            569 |  # node_Concat_2242\n",
              "                   %\"val_2286\"<FLOAT,[1,None,s0,256]> ⬅️ ::Concat(%\"val_2285\", %\"val_2282\") {axis=1}\n",
              "            570 |  # node_Slice_2247\n",
              "                   %\"val_2291\"<FLOAT,[1,None,None,256]> ⬅️ ::Slice(%\"val_2286\", %\"val_144\"{[0]}, %\"val_100\"{[4]}, %\"val_163\"{[2]})\n",
              "            571 |  # node_Size_2248\n",
              "                   %\"val_2292\"<INT64,[]> ⬅️ ::Size(%\"val_2286\")\n",
              "            572 |  # node_Reshape_2249\n",
              "                   %\"val_2293\"<INT64,[1]> ⬅️ ::Reshape(%\"val_2292\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            573 |  # node_Slice_2250\n",
              "                   %\"val_2294\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"val_2286\", %\"val_100\"{[4]}, %\"val_2293\", %\"val_163\"{[2]})\n",
              "            574 |  # node_roll_6\n",
              "                   %\"roll_6\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Concat(%\"val_2294\", %\"val_2291\") {axis=2}\n",
              "            575 |  # node_view_92\n",
              "                   %\"view_92\"<FLOAT,[1,(s53//8),(s53//((s53//8))),(s0//8),(s0//((s0//8))),256]> ⬅️ ::Reshape(%\"roll_6\", %\"val_24\") {allowzero=1}\n",
              "            576 |  # node_permute_37\n",
              "                   %\"permute_37\"<FLOAT,[1,(s53//8),(s0//8),(s53//((s53//8))),(s0//((s0//8))),256]> ⬅️ ::Transpose(%\"view_92\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            577 |  # node_Reshape_4968\n",
              "                   %\"view_94\"<FLOAT,[((s0//8))*((s53//8)),64,256]> ⬅️ ::Reshape(%\"permute_37\", %\"val_35\"{[-1, 64, 256]})\n",
              "            578 |  # node_MatMul_2785\n",
              "                   %\"val_2829\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::MatMul(%\"view_94\", %\"val_2828\"{...})\n",
              "            579 |  # node_linear_28\n",
              "                   %\"linear_28\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::Add(%\"val_2829\", %\"layers.1.blocks.3.attn.qkv.bias\"{...})\n",
              "            580 |  # node_view_99\n",
              "                   %\"view_99\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),3,8,32]> ⬅️ ::Reshape(%\"linear_28\", %\"val_46\") {allowzero=1}\n",
              "            581 |  # node_permute_40\n",
              "                   %\"permute_40\"<FLOAT,[3,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Transpose(%\"view_99\") {perm=(2, 0, 3, 1, 4)}\n",
              "            582 |  # node_Split_2793\n",
              "                   %\"val_2837\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_2838\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_2839\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Split(%\"permute_40\") {num_outputs=3, axis=0}\n",
              "            583 |  # node_unbind_7__0\n",
              "                   %\"getitem_21\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_2837\", %\"val_144\"{[0]})\n",
              "            584 |  # node_unbind_7__1\n",
              "                   %\"getitem_22\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_2838\", %\"val_144\"{[0]})\n",
              "            585 |  # node_unbind_7__2\n",
              "                   %\"getitem_23\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_2839\", %\"val_144\"{[0]})\n",
              "            586 |  # node_transpose_7\n",
              "                   %\"transpose_7\"<FLOAT,[((s0//8))*((s53//8)),8,32,((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Transpose(%\"getitem_22\") {perm=(0, 1, 3, 2)}\n",
              "            587 |  # node_matmul_14\n",
              "                   %\"matmul_14\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::MatMul(%\"getitem_21\", %\"transpose_7\")\n",
              "            588 |  # node_mul_2526\n",
              "                   %\"mul_2526\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Mul(%\"matmul_14\", %\"val_51\"{0.1767766922712326})\n",
              "            589 |  # node_view_100\n",
              "                   %\"view_100\"<FLOAT,[((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))),((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Reshape(%\"mul_2526\", %\"val_678\") {allowzero=1}\n",
              "            590 |  # node_add_2218\n",
              "                   %\"add_2218\"<FLOAT,[1,((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,64,64]> ⬅️ ::Add(%\"view_100\", %\"unsqueeze_3\")\n",
              "            591 |  # node_view_101\n",
              "                   %\"view_101\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Reshape(%\"add_2218\", %\"val_684\") {allowzero=1}\n",
              "            592 |  # node_softmax_7\n",
              "                   %\"softmax_7\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Softmax(%\"view_101\") {axis=-1}\n",
              "            593 |  # node_matmul_15\n",
              "                   %\"matmul_15\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::MatMul(%\"softmax_7\", %\"getitem_23\")\n",
              "            594 |  # node_permute_41\n",
              "                   %\"permute_41\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),8,32]> ⬅️ ::Transpose(%\"matmul_15\") {perm=(0, 2, 1, 3)}\n",
              "            595 |  # node_view_102\n",
              "                   %\"view_102\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Reshape(%\"permute_41\", %\"val_56\") {allowzero=1}\n",
              "            596 |  # node_MatMul_2813\n",
              "                   %\"val_2859\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::MatMul(%\"view_102\", %\"val_2858\"{...})\n",
              "            597 |  # node_linear_29\n",
              "                   %\"linear_29\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Add(%\"val_2859\", %\"layers.1.blocks.3.attn.proj.bias\"{...})\n",
              "            598 |  # node_view_103\n",
              "                   %\"view_103\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,8,256]> ⬅️ ::Reshape(%\"linear_29\", %\"val_64\"{[-1, 8, 8, 256]}) {allowzero=1}\n",
              "            599 |  # node_view_104\n",
              "                   %\"view_104\"<FLOAT,[1,(s53//8),(s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))),8,((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Reshape(%\"view_103\", %\"val_72\") {allowzero=1}\n",
              "            600 |  # node_permute_42\n",
              "                   %\"permute_42\"<FLOAT,[1,(s53//8),8,(s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))),((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Transpose(%\"view_104\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            601 |  # node_view_105\n",
              "                   %\"view_105\"<FLOAT,[1,8*((s53//8)),((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"permute_42\", %\"val_78\") {allowzero=1}\n",
              "            602 |  # node_Shape_2836\n",
              "                   %\"val_2882\"<INT64,[1]> ⬅️ ::Shape(%\"view_105\") {end=2, start=1}\n",
              "            603 |  # node_Sub_2838\n",
              "                   %\"val_2884\"<INT64,[1]> ⬅️ ::Sub(%\"val_2882\", %\"val_100\"{[4]})\n",
              "            604 |  # node_Slice_2840\n",
              "                   %\"val_2886\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"view_105\", %\"val_144\"{[0]}, %\"val_2884\", %\"val_152\"{[1]})\n",
              "            605 |  # node_Size_2841\n",
              "                   %\"val_2887\"<INT64,[]> ⬅️ ::Size(%\"view_105\")\n",
              "            606 |  # node_Reshape_2842\n",
              "                   %\"val_2888\"<INT64,[1]> ⬅️ ::Reshape(%\"val_2887\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            607 |  # node_Slice_2843\n",
              "                   %\"val_2889\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"view_105\", %\"val_2884\", %\"val_2888\", %\"val_152\"{[1]})\n",
              "            608 |  # node_Concat_2844\n",
              "                   %\"val_2890\"<FLOAT,[None,None,None,None]> ⬅️ ::Concat(%\"val_2889\", %\"val_2886\") {axis=1}\n",
              "            609 |  # node_Shape_2847\n",
              "                   %\"val_2893\"<INT64,[1]> ⬅️ ::Shape(%\"val_2890\") {end=3, start=2}\n",
              "            610 |  # node_Sub_2849\n",
              "                   %\"val_2895\"<INT64,[1]> ⬅️ ::Sub(%\"val_2893\", %\"val_100\"{[4]})\n",
              "            611 |  # node_Slice_2851\n",
              "                   %\"val_2897\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"val_2890\", %\"val_144\"{[0]}, %\"val_2895\", %\"val_163\"{[2]})\n",
              "            612 |  # node_Size_2852\n",
              "                   %\"val_2898\"<INT64,[]> ⬅️ ::Size(%\"val_2890\")\n",
              "            613 |  # node_Reshape_2853\n",
              "                   %\"val_2899\"<INT64,[1]> ⬅️ ::Reshape(%\"val_2898\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            614 |  # node_Slice_2854\n",
              "                   %\"val_2900\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"val_2890\", %\"val_2895\", %\"val_2899\", %\"val_163\"{[2]})\n",
              "            615 |  # node_roll_7\n",
              "                   %\"roll_7\"<FLOAT,[1,8*((s53//8)),((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Concat(%\"val_2900\", %\"val_2897\") {axis=2}\n",
              "            616 |  # node_view_106\n",
              "                   %\"view_106\"<FLOAT,[1,8*((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"roll_7\", %\"val_83\") {allowzero=1}\n",
              "            617 |  # node_add_2291\n",
              "                   %\"add_2291\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_1907\", %\"view_106\")\n",
              "            618 |  # node_layer_norm_15\n",
              "                   %\"layer_norm_15\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_2291\", %\"layers.1.blocks.3.norm2.weight\"{...}, %\"layers.1.blocks.3.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            619 |  # node_MatMul_2861\n",
              "                   %\"val_2909\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::MatMul(%\"layer_norm_15\", %\"val_2908\"{...})\n",
              "            620 |  # node_linear_30\n",
              "                   %\"linear_30\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Add(%\"val_2909\", %\"layers.1.blocks.3.mlp.0.bias\"{...})\n",
              "            621 |  # node_gelu_7\n",
              "                   %\"gelu_7\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Gelu(%\"linear_30\") {approximate='none'}\n",
              "            622 |  # node_MatMul_2863\n",
              "                   %\"val_2911\"<FLOAT,[1,s0*s53,256]> ⬅️ ::MatMul(%\"gelu_7\", %\"val_2910\"{...})\n",
              "            623 |  # node_linear_31\n",
              "                   %\"linear_31\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"val_2911\", %\"layers.1.blocks.3.mlp.2.bias\"{...})\n",
              "            624 |  # node_add_2307\n",
              "                   %\"add_2307\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_2291\", %\"linear_31\")\n",
              "            625 |  # node_view_107\n",
              "                   %\"view_107\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Reshape(%\"add_2307\", %\"val_1468\") {allowzero=1}\n",
              "            626 |  # node_permute_43\n",
              "                   %\"permute_43\"<FLOAT,[1,256,s53,s0]> ⬅️ ::Transpose(%\"view_107\") {perm=(0, 3, 1, 2)}\n",
              "            627 |  # node_conv2d_2\n",
              "                   %\"conv2d_2\"<FLOAT,[1,256,s53,s0]> ⬅️ ::Conv(%\"permute_43\", %\"layers.1.conv.weight\"{...}, %\"layers.1.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
              "            628 |  # node_permute_44\n",
              "                   %\"permute_44\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Transpose(%\"conv2d_2\") {perm=(0, 2, 3, 1)}\n",
              "            629 |  # node_add_2334\n",
              "                   %\"add_2334\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Add(%\"add_1177\", %\"permute_44\")\n",
              "            630 |  # node_view_108\n",
              "                   %\"view_108\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Reshape(%\"add_2334\", %\"val_7\") {allowzero=1}\n",
              "            631 |  # node_layer_norm_16\n",
              "                   %\"layer_norm_16\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"view_108\", %\"layers.2.blocks.0.norm1.weight\"{...}, %\"layers.2.blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            632 |  # node_view_109\n",
              "                   %\"view_109\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Reshape(%\"layer_norm_16\", %\"val_15\") {allowzero=1}\n",
              "            633 |  # node_view_110\n",
              "                   %\"view_110\"<FLOAT,[1,(s53//8),(s53//((s53//8))),(s0//8),(s0//((s0//8))),256]> ⬅️ ::Reshape(%\"view_109\", %\"val_24\") {allowzero=1}\n",
              "            634 |  # node_permute_45\n",
              "                   %\"permute_45\"<FLOAT,[1,(s53//8),(s0//8),(s53//((s53//8))),(s0//((s0//8))),256]> ⬅️ ::Transpose(%\"view_110\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            635 |  # node_Reshape_4992\n",
              "                   %\"view_112\"<FLOAT,[((s0//8))*((s53//8)),64,256]> ⬅️ ::Reshape(%\"permute_45\", %\"val_35\"{[-1, 64, 256]})\n",
              "            636 |  # node_MatMul_2901\n",
              "                   %\"val_2951\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::MatMul(%\"view_112\", %\"val_2950\"{...})\n",
              "            637 |  # node_linear_32\n",
              "                   %\"linear_32\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::Add(%\"val_2951\", %\"layers.2.blocks.0.attn.qkv.bias\"{...})\n",
              "            638 |  # node_view_113\n",
              "                   %\"view_113\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),3,8,32]> ⬅️ ::Reshape(%\"linear_32\", %\"val_46\") {allowzero=1}\n",
              "            639 |  # node_permute_46\n",
              "                   %\"permute_46\"<FLOAT,[3,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Transpose(%\"view_113\") {perm=(2, 0, 3, 1, 4)}\n",
              "            640 |  # node_Split_2909\n",
              "                   %\"val_2959\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_2960\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_2961\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Split(%\"permute_46\") {num_outputs=3, axis=0}\n",
              "            641 |  # node_unbind_8__0\n",
              "                   %\"getitem_24\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_2959\", %\"val_144\"{[0]})\n",
              "            642 |  # node_unbind_8__1\n",
              "                   %\"getitem_25\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_2960\", %\"val_144\"{[0]})\n",
              "            643 |  # node_unbind_8__2\n",
              "                   %\"getitem_26\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_2961\", %\"val_144\"{[0]})\n",
              "            644 |  # node_transpose_8\n",
              "                   %\"transpose_8\"<FLOAT,[((s0//8))*((s53//8)),8,32,((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Transpose(%\"getitem_25\") {perm=(0, 1, 3, 2)}\n",
              "            645 |  # node_matmul_16\n",
              "                   %\"matmul_16\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::MatMul(%\"getitem_24\", %\"transpose_8\")\n",
              "            646 |  # node_mul_2762\n",
              "                   %\"mul_2762\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Mul(%\"matmul_16\", %\"val_51\"{0.1767766922712326})\n",
              "            647 |  # node_softmax_8\n",
              "                   %\"softmax_8\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Softmax(%\"mul_2762\") {axis=-1}\n",
              "            648 |  # node_matmul_17\n",
              "                   %\"matmul_17\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::MatMul(%\"softmax_8\", %\"getitem_26\")\n",
              "            649 |  # node_permute_47\n",
              "                   %\"permute_47\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),8,32]> ⬅️ ::Transpose(%\"matmul_17\") {perm=(0, 2, 1, 3)}\n",
              "            650 |  # node_view_114\n",
              "                   %\"view_114\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Reshape(%\"permute_47\", %\"val_56\") {allowzero=1}\n",
              "            651 |  # node_MatMul_2916\n",
              "                   %\"val_2968\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::MatMul(%\"view_114\", %\"val_2967\"{...})\n",
              "            652 |  # node_linear_33\n",
              "                   %\"linear_33\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Add(%\"val_2968\", %\"layers.2.blocks.0.attn.proj.bias\"{...})\n",
              "            653 |  # node_view_115\n",
              "                   %\"view_115\"<FLOAT,[((s0//8))*((s53//8)),8,8,256]> ⬅️ ::Reshape(%\"linear_33\", %\"val_64\"{[-1, 8, 8, 256]}) {allowzero=1}\n",
              "            654 |  # node_view_116\n",
              "                   %\"view_116\"<FLOAT,[1,(s53//8),(s0//8),8,((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Reshape(%\"view_115\", %\"val_72\") {allowzero=1}\n",
              "            655 |  # node_permute_48\n",
              "                   %\"permute_48\"<FLOAT,[1,(s53//8),8,(s0//8),((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Transpose(%\"view_116\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            656 |  # node_view_117\n",
              "                   %\"view_117\"<FLOAT,[1,8*((s53//8)),((s0//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"permute_48\", %\"val_78\") {allowzero=1}\n",
              "            657 |  # node_view_118\n",
              "                   %\"view_118\"<FLOAT,[1,8*((s0//8))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"view_117\", %\"val_83\") {allowzero=1}\n",
              "            658 |  # node_add_2485\n",
              "                   %\"add_2485\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"view_108\", %\"view_118\")\n",
              "            659 |  # node_layer_norm_17\n",
              "                   %\"layer_norm_17\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_2485\", %\"layers.2.blocks.0.norm2.weight\"{...}, %\"layers.2.blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            660 |  # node_MatMul_2943\n",
              "                   %\"val_2997\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::MatMul(%\"layer_norm_17\", %\"val_2996\"{...})\n",
              "            661 |  # node_linear_34\n",
              "                   %\"linear_34\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Add(%\"val_2997\", %\"layers.2.blocks.0.mlp.0.bias\"{...})\n",
              "            662 |  # node_gelu_8\n",
              "                   %\"gelu_8\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Gelu(%\"linear_34\") {approximate='none'}\n",
              "            663 |  # node_MatMul_2945\n",
              "                   %\"val_2999\"<FLOAT,[1,s0*s53,256]> ⬅️ ::MatMul(%\"gelu_8\", %\"val_2998\"{...})\n",
              "            664 |  # node_linear_35\n",
              "                   %\"linear_35\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"val_2999\", %\"layers.2.blocks.0.mlp.2.bias\"{...})\n",
              "            665 |  # node_add_2501\n",
              "                   %\"add_2501\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_2485\", %\"linear_35\")\n",
              "            666 |  # node_layer_norm_18\n",
              "                   %\"layer_norm_18\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_2501\", %\"layers.2.blocks.1.norm1.weight\"{...}, %\"layers.2.blocks.1.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            667 |  # node_view_119\n",
              "                   %\"view_119\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Reshape(%\"layer_norm_18\", %\"val_15\") {allowzero=1}\n",
              "            668 |  # node_Slice_2956\n",
              "                   %\"val_3012\"<FLOAT,[1,None,s0,256]> ⬅️ ::Slice(%\"view_119\", %\"val_144\"{[0]}, %\"val_100\"{[4]}, %\"val_152\"{[1]})\n",
              "            669 |  # node_Size_2957\n",
              "                   %\"val_3013\"<INT64,[]> ⬅️ ::Size(%\"view_119\")\n",
              "            670 |  # node_Reshape_2958\n",
              "                   %\"val_3014\"<INT64,[1]> ⬅️ ::Reshape(%\"val_3013\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            671 |  # node_Slice_2959\n",
              "                   %\"val_3015\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"view_119\", %\"val_100\"{[4]}, %\"val_3014\", %\"val_152\"{[1]})\n",
              "            672 |  # node_Concat_2960\n",
              "                   %\"val_3016\"<FLOAT,[1,None,s0,256]> ⬅️ ::Concat(%\"val_3015\", %\"val_3012\") {axis=1}\n",
              "            673 |  # node_Slice_2965\n",
              "                   %\"val_3021\"<FLOAT,[1,None,None,256]> ⬅️ ::Slice(%\"val_3016\", %\"val_144\"{[0]}, %\"val_100\"{[4]}, %\"val_163\"{[2]})\n",
              "            674 |  # node_Size_2966\n",
              "                   %\"val_3022\"<INT64,[]> ⬅️ ::Size(%\"val_3016\")\n",
              "            675 |  # node_Reshape_2967\n",
              "                   %\"val_3023\"<INT64,[1]> ⬅️ ::Reshape(%\"val_3022\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            676 |  # node_Slice_2968\n",
              "                   %\"val_3024\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"val_3016\", %\"val_100\"{[4]}, %\"val_3023\", %\"val_163\"{[2]})\n",
              "            677 |  # node_roll_8\n",
              "                   %\"roll_8\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Concat(%\"val_3024\", %\"val_3021\") {axis=2}\n",
              "            678 |  # node_view_120\n",
              "                   %\"view_120\"<FLOAT,[1,(s53//8),(s53//((s53//8))),(s0//8),(s0//((s0//8))),256]> ⬅️ ::Reshape(%\"roll_8\", %\"val_24\") {allowzero=1}\n",
              "            679 |  # node_permute_49\n",
              "                   %\"permute_49\"<FLOAT,[1,(s53//8),(s0//8),(s53//((s53//8))),(s0//((s0//8))),256]> ⬅️ ::Transpose(%\"view_120\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            680 |  # node_Reshape_4999\n",
              "                   %\"view_122\"<FLOAT,[((s0//8))*((s53//8)),64,256]> ⬅️ ::Reshape(%\"permute_49\", %\"val_35\"{[-1, 64, 256]})\n",
              "            681 |  # node_MatMul_3503\n",
              "                   %\"val_3559\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::MatMul(%\"view_122\", %\"val_3558\"{...})\n",
              "            682 |  # node_linear_36\n",
              "                   %\"linear_36\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::Add(%\"val_3559\", %\"layers.2.blocks.1.attn.qkv.bias\"{...})\n",
              "            683 |  # node_view_127\n",
              "                   %\"view_127\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),3,8,32]> ⬅️ ::Reshape(%\"linear_36\", %\"val_46\") {allowzero=1}\n",
              "            684 |  # node_permute_52\n",
              "                   %\"permute_52\"<FLOAT,[3,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Transpose(%\"view_127\") {perm=(2, 0, 3, 1, 4)}\n",
              "            685 |  # node_Split_3511\n",
              "                   %\"val_3567\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_3568\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_3569\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Split(%\"permute_52\") {num_outputs=3, axis=0}\n",
              "            686 |  # node_unbind_9__0\n",
              "                   %\"getitem_27\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_3567\", %\"val_144\"{[0]})\n",
              "            687 |  # node_unbind_9__1\n",
              "                   %\"getitem_28\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_3568\", %\"val_144\"{[0]})\n",
              "            688 |  # node_unbind_9__2\n",
              "                   %\"getitem_29\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_3569\", %\"val_144\"{[0]})\n",
              "            689 |  # node_transpose_9\n",
              "                   %\"transpose_9\"<FLOAT,[((s0//8))*((s53//8)),8,32,((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Transpose(%\"getitem_28\") {perm=(0, 1, 3, 2)}\n",
              "            690 |  # node_matmul_18\n",
              "                   %\"matmul_18\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::MatMul(%\"getitem_27\", %\"transpose_9\")\n",
              "            691 |  # node_mul_3093\n",
              "                   %\"mul_3093\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Mul(%\"matmul_18\", %\"val_51\"{0.1767766922712326})\n",
              "            692 |  # node_view_128\n",
              "                   %\"view_128\"<FLOAT,[((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))),((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Reshape(%\"mul_3093\", %\"val_678\") {allowzero=1}\n",
              "            693 |  # node_add_2812\n",
              "                   %\"add_2812\"<FLOAT,[1,((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,64,64]> ⬅️ ::Add(%\"view_128\", %\"unsqueeze_3\")\n",
              "            694 |  # node_view_129\n",
              "                   %\"view_129\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Reshape(%\"add_2812\", %\"val_684\") {allowzero=1}\n",
              "            695 |  # node_softmax_9\n",
              "                   %\"softmax_9\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Softmax(%\"view_129\") {axis=-1}\n",
              "            696 |  # node_matmul_19\n",
              "                   %\"matmul_19\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::MatMul(%\"softmax_9\", %\"getitem_29\")\n",
              "            697 |  # node_permute_53\n",
              "                   %\"permute_53\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),8,32]> ⬅️ ::Transpose(%\"matmul_19\") {perm=(0, 2, 1, 3)}\n",
              "            698 |  # node_view_130\n",
              "                   %\"view_130\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Reshape(%\"permute_53\", %\"val_56\") {allowzero=1}\n",
              "            699 |  # node_MatMul_3531\n",
              "                   %\"val_3589\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::MatMul(%\"view_130\", %\"val_3588\"{...})\n",
              "            700 |  # node_linear_37\n",
              "                   %\"linear_37\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Add(%\"val_3589\", %\"layers.2.blocks.1.attn.proj.bias\"{...})\n",
              "            701 |  # node_view_131\n",
              "                   %\"view_131\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,8,256]> ⬅️ ::Reshape(%\"linear_37\", %\"val_64\"{[-1, 8, 8, 256]}) {allowzero=1}\n",
              "            702 |  # node_view_132\n",
              "                   %\"view_132\"<FLOAT,[1,(s53//8),(s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))),8,((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Reshape(%\"view_131\", %\"val_72\") {allowzero=1}\n",
              "            703 |  # node_permute_54\n",
              "                   %\"permute_54\"<FLOAT,[1,(s53//8),8,(s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))),((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Transpose(%\"view_132\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            704 |  # node_view_133\n",
              "                   %\"view_133\"<FLOAT,[1,8*((s53//8)),((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"permute_54\", %\"val_78\") {allowzero=1}\n",
              "            705 |  # node_Shape_3554\n",
              "                   %\"val_3612\"<INT64,[1]> ⬅️ ::Shape(%\"view_133\") {end=2, start=1}\n",
              "            706 |  # node_Sub_3556\n",
              "                   %\"val_3614\"<INT64,[1]> ⬅️ ::Sub(%\"val_3612\", %\"val_100\"{[4]})\n",
              "            707 |  # node_Slice_3558\n",
              "                   %\"val_3616\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"view_133\", %\"val_144\"{[0]}, %\"val_3614\", %\"val_152\"{[1]})\n",
              "            708 |  # node_Size_3559\n",
              "                   %\"val_3617\"<INT64,[]> ⬅️ ::Size(%\"view_133\")\n",
              "            709 |  # node_Reshape_3560\n",
              "                   %\"val_3618\"<INT64,[1]> ⬅️ ::Reshape(%\"val_3617\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            710 |  # node_Slice_3561\n",
              "                   %\"val_3619\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"view_133\", %\"val_3614\", %\"val_3618\", %\"val_152\"{[1]})\n",
              "            711 |  # node_Concat_3562\n",
              "                   %\"val_3620\"<FLOAT,[None,None,None,None]> ⬅️ ::Concat(%\"val_3619\", %\"val_3616\") {axis=1}\n",
              "            712 |  # node_Shape_3565\n",
              "                   %\"val_3623\"<INT64,[1]> ⬅️ ::Shape(%\"val_3620\") {end=3, start=2}\n",
              "            713 |  # node_Sub_3567\n",
              "                   %\"val_3625\"<INT64,[1]> ⬅️ ::Sub(%\"val_3623\", %\"val_100\"{[4]})\n",
              "            714 |  # node_Slice_3569\n",
              "                   %\"val_3627\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"val_3620\", %\"val_144\"{[0]}, %\"val_3625\", %\"val_163\"{[2]})\n",
              "            715 |  # node_Size_3570\n",
              "                   %\"val_3628\"<INT64,[]> ⬅️ ::Size(%\"val_3620\")\n",
              "            716 |  # node_Reshape_3571\n",
              "                   %\"val_3629\"<INT64,[1]> ⬅️ ::Reshape(%\"val_3628\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            717 |  # node_Slice_3572\n",
              "                   %\"val_3630\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"val_3620\", %\"val_3625\", %\"val_3629\", %\"val_163\"{[2]})\n",
              "            718 |  # node_roll_9\n",
              "                   %\"roll_9\"<FLOAT,[1,8*((s53//8)),((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Concat(%\"val_3630\", %\"val_3627\") {axis=2}\n",
              "            719 |  # node_view_134\n",
              "                   %\"view_134\"<FLOAT,[1,8*((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"roll_9\", %\"val_83\") {allowzero=1}\n",
              "            720 |  # node_add_2885\n",
              "                   %\"add_2885\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_2501\", %\"view_134\")\n",
              "            721 |  # node_layer_norm_19\n",
              "                   %\"layer_norm_19\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_2885\", %\"layers.2.blocks.1.norm2.weight\"{...}, %\"layers.2.blocks.1.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            722 |  # node_MatMul_3579\n",
              "                   %\"val_3639\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::MatMul(%\"layer_norm_19\", %\"val_3638\"{...})\n",
              "            723 |  # node_linear_38\n",
              "                   %\"linear_38\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Add(%\"val_3639\", %\"layers.2.blocks.1.mlp.0.bias\"{...})\n",
              "            724 |  # node_gelu_9\n",
              "                   %\"gelu_9\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Gelu(%\"linear_38\") {approximate='none'}\n",
              "            725 |  # node_MatMul_3581\n",
              "                   %\"val_3641\"<FLOAT,[1,s0*s53,256]> ⬅️ ::MatMul(%\"gelu_9\", %\"val_3640\"{...})\n",
              "            726 |  # node_linear_39\n",
              "                   %\"linear_39\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"val_3641\", %\"layers.2.blocks.1.mlp.2.bias\"{...})\n",
              "            727 |  # node_add_2901\n",
              "                   %\"add_2901\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_2885\", %\"linear_39\")\n",
              "            728 |  # node_layer_norm_20\n",
              "                   %\"layer_norm_20\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_2901\", %\"layers.2.blocks.2.norm1.weight\"{...}, %\"layers.2.blocks.2.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            729 |  # node_view_135\n",
              "                   %\"view_135\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Reshape(%\"layer_norm_20\", %\"val_15\") {allowzero=1}\n",
              "            730 |  # node_view_136\n",
              "                   %\"view_136\"<FLOAT,[1,(s53//8),(s53//((s53//8))),(s0//8),(s0//((s0//8))),256]> ⬅️ ::Reshape(%\"view_135\", %\"val_24\") {allowzero=1}\n",
              "            731 |  # node_permute_55\n",
              "                   %\"permute_55\"<FLOAT,[1,(s53//8),(s0//8),(s53//((s53//8))),(s0//((s0//8))),256]> ⬅️ ::Transpose(%\"view_136\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            732 |  # node_Reshape_5019\n",
              "                   %\"view_138\"<FLOAT,[((s0//8))*((s53//8)),64,256]> ⬅️ ::Reshape(%\"permute_55\", %\"val_35\"{[-1, 64, 256]})\n",
              "            733 |  # node_MatMul_3608\n",
              "                   %\"val_3670\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::MatMul(%\"view_138\", %\"val_3669\"{...})\n",
              "            734 |  # node_linear_40\n",
              "                   %\"linear_40\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::Add(%\"val_3670\", %\"layers.2.blocks.2.attn.qkv.bias\"{...})\n",
              "            735 |  # node_view_139\n",
              "                   %\"view_139\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),3,8,32]> ⬅️ ::Reshape(%\"linear_40\", %\"val_46\") {allowzero=1}\n",
              "            736 |  # node_permute_56\n",
              "                   %\"permute_56\"<FLOAT,[3,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Transpose(%\"view_139\") {perm=(2, 0, 3, 1, 4)}\n",
              "            737 |  # node_Split_3616\n",
              "                   %\"val_3678\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_3679\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_3680\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Split(%\"permute_56\") {num_outputs=3, axis=0}\n",
              "            738 |  # node_unbind_10__0\n",
              "                   %\"getitem_30\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_3678\", %\"val_144\"{[0]})\n",
              "            739 |  # node_unbind_10__1\n",
              "                   %\"getitem_31\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_3679\", %\"val_144\"{[0]})\n",
              "            740 |  # node_unbind_10__2\n",
              "                   %\"getitem_32\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_3680\", %\"val_144\"{[0]})\n",
              "            741 |  # node_transpose_10\n",
              "                   %\"transpose_10\"<FLOAT,[((s0//8))*((s53//8)),8,32,((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Transpose(%\"getitem_31\") {perm=(0, 1, 3, 2)}\n",
              "            742 |  # node_matmul_20\n",
              "                   %\"matmul_20\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::MatMul(%\"getitem_30\", %\"transpose_10\")\n",
              "            743 |  # node_mul_3290\n",
              "                   %\"mul_3290\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Mul(%\"matmul_20\", %\"val_51\"{0.1767766922712326})\n",
              "            744 |  # node_softmax_10\n",
              "                   %\"softmax_10\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Softmax(%\"mul_3290\") {axis=-1}\n",
              "            745 |  # node_matmul_21\n",
              "                   %\"matmul_21\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::MatMul(%\"softmax_10\", %\"getitem_32\")\n",
              "            746 |  # node_permute_57\n",
              "                   %\"permute_57\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),8,32]> ⬅️ ::Transpose(%\"matmul_21\") {perm=(0, 2, 1, 3)}\n",
              "            747 |  # node_view_140\n",
              "                   %\"view_140\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Reshape(%\"permute_57\", %\"val_56\") {allowzero=1}\n",
              "            748 |  # node_MatMul_3623\n",
              "                   %\"val_3687\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::MatMul(%\"view_140\", %\"val_3686\"{...})\n",
              "            749 |  # node_linear_41\n",
              "                   %\"linear_41\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Add(%\"val_3687\", %\"layers.2.blocks.2.attn.proj.bias\"{...})\n",
              "            750 |  # node_view_141\n",
              "                   %\"view_141\"<FLOAT,[((s0//8))*((s53//8)),8,8,256]> ⬅️ ::Reshape(%\"linear_41\", %\"val_64\"{[-1, 8, 8, 256]}) {allowzero=1}\n",
              "            751 |  # node_view_142\n",
              "                   %\"view_142\"<FLOAT,[1,(s53//8),(s0//8),8,((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Reshape(%\"view_141\", %\"val_72\") {allowzero=1}\n",
              "            752 |  # node_permute_58\n",
              "                   %\"permute_58\"<FLOAT,[1,(s53//8),8,(s0//8),((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Transpose(%\"view_142\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            753 |  # node_view_143\n",
              "                   %\"view_143\"<FLOAT,[1,8*((s53//8)),((s0//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"permute_58\", %\"val_78\") {allowzero=1}\n",
              "            754 |  # node_view_144\n",
              "                   %\"view_144\"<FLOAT,[1,8*((s0//8))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"view_143\", %\"val_83\") {allowzero=1}\n",
              "            755 |  # node_add_3048\n",
              "                   %\"add_3048\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_2901\", %\"view_144\")\n",
              "            756 |  # node_layer_norm_21\n",
              "                   %\"layer_norm_21\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_3048\", %\"layers.2.blocks.2.norm2.weight\"{...}, %\"layers.2.blocks.2.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            757 |  # node_MatMul_3650\n",
              "                   %\"val_3716\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::MatMul(%\"layer_norm_21\", %\"val_3715\"{...})\n",
              "            758 |  # node_linear_42\n",
              "                   %\"linear_42\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Add(%\"val_3716\", %\"layers.2.blocks.2.mlp.0.bias\"{...})\n",
              "            759 |  # node_gelu_10\n",
              "                   %\"gelu_10\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Gelu(%\"linear_42\") {approximate='none'}\n",
              "            760 |  # node_MatMul_3652\n",
              "                   %\"val_3718\"<FLOAT,[1,s0*s53,256]> ⬅️ ::MatMul(%\"gelu_10\", %\"val_3717\"{...})\n",
              "            761 |  # node_linear_43\n",
              "                   %\"linear_43\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"val_3718\", %\"layers.2.blocks.2.mlp.2.bias\"{...})\n",
              "            762 |  # node_add_3064\n",
              "                   %\"add_3064\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_3048\", %\"linear_43\")\n",
              "            763 |  # node_layer_norm_22\n",
              "                   %\"layer_norm_22\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_3064\", %\"layers.2.blocks.3.norm1.weight\"{...}, %\"layers.2.blocks.3.norm1.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            764 |  # node_view_145\n",
              "                   %\"view_145\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Reshape(%\"layer_norm_22\", %\"val_15\") {allowzero=1}\n",
              "            765 |  # node_Slice_3663\n",
              "                   %\"val_3731\"<FLOAT,[1,None,s0,256]> ⬅️ ::Slice(%\"view_145\", %\"val_144\"{[0]}, %\"val_100\"{[4]}, %\"val_152\"{[1]})\n",
              "            766 |  # node_Size_3664\n",
              "                   %\"val_3732\"<INT64,[]> ⬅️ ::Size(%\"view_145\")\n",
              "            767 |  # node_Reshape_3665\n",
              "                   %\"val_3733\"<INT64,[1]> ⬅️ ::Reshape(%\"val_3732\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            768 |  # node_Slice_3666\n",
              "                   %\"val_3734\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"view_145\", %\"val_100\"{[4]}, %\"val_3733\", %\"val_152\"{[1]})\n",
              "            769 |  # node_Concat_3667\n",
              "                   %\"val_3735\"<FLOAT,[1,None,s0,256]> ⬅️ ::Concat(%\"val_3734\", %\"val_3731\") {axis=1}\n",
              "            770 |  # node_Slice_3672\n",
              "                   %\"val_3740\"<FLOAT,[1,None,None,256]> ⬅️ ::Slice(%\"val_3735\", %\"val_144\"{[0]}, %\"val_100\"{[4]}, %\"val_163\"{[2]})\n",
              "            771 |  # node_Size_3673\n",
              "                   %\"val_3741\"<INT64,[]> ⬅️ ::Size(%\"val_3735\")\n",
              "            772 |  # node_Reshape_3674\n",
              "                   %\"val_3742\"<INT64,[1]> ⬅️ ::Reshape(%\"val_3741\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            773 |  # node_Slice_3675\n",
              "                   %\"val_3743\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"val_3735\", %\"val_100\"{[4]}, %\"val_3742\", %\"val_163\"{[2]})\n",
              "            774 |  # node_roll_10\n",
              "                   %\"roll_10\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Concat(%\"val_3743\", %\"val_3740\") {axis=2}\n",
              "            775 |  # node_view_146\n",
              "                   %\"view_146\"<FLOAT,[1,(s53//8),(s53//((s53//8))),(s0//8),(s0//((s0//8))),256]> ⬅️ ::Reshape(%\"roll_10\", %\"val_24\") {allowzero=1}\n",
              "            776 |  # node_permute_59\n",
              "                   %\"permute_59\"<FLOAT,[1,(s53//8),(s0//8),(s53//((s53//8))),(s0//((s0//8))),256]> ⬅️ ::Transpose(%\"view_146\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            777 |  # node_Reshape_5026\n",
              "                   %\"view_148\"<FLOAT,[((s0//8))*((s53//8)),64,256]> ⬅️ ::Reshape(%\"permute_59\", %\"val_35\"{[-1, 64, 256]})\n",
              "            778 |  # node_MatMul_4210\n",
              "                   %\"val_4278\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::MatMul(%\"view_148\", %\"val_4277\"{...})\n",
              "            779 |  # node_linear_44\n",
              "                   %\"linear_44\"<FLOAT,[((s0//8))*((s53//8)),64,768]> ⬅️ ::Add(%\"val_4278\", %\"layers.2.blocks.3.attn.qkv.bias\"{...})\n",
              "            780 |  # node_view_153\n",
              "                   %\"view_153\"<FLOAT,[((s0//8))*((s53//8)),((s0//((s0//8))))*((s53//((s53//8)))),3,8,32]> ⬅️ ::Reshape(%\"linear_44\", %\"val_46\") {allowzero=1}\n",
              "            781 |  # node_permute_62\n",
              "                   %\"permute_62\"<FLOAT,[3,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Transpose(%\"view_153\") {perm=(2, 0, 3, 1, 4)}\n",
              "            782 |  # node_Split_4218\n",
              "                   %\"val_4286\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_4287\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]>, %\"val_4288\"<FLOAT,[1,((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Split(%\"permute_62\") {num_outputs=3, axis=0}\n",
              "            783 |  # node_unbind_11__0\n",
              "                   %\"getitem_33\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_4286\", %\"val_144\"{[0]})\n",
              "            784 |  # node_unbind_11__1\n",
              "                   %\"getitem_34\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_4287\", %\"val_144\"{[0]})\n",
              "            785 |  # node_unbind_11__2\n",
              "                   %\"getitem_35\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::Squeeze(%\"val_4288\", %\"val_144\"{[0]})\n",
              "            786 |  # node_transpose_11\n",
              "                   %\"transpose_11\"<FLOAT,[((s0//8))*((s53//8)),8,32,((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Transpose(%\"getitem_34\") {perm=(0, 1, 3, 2)}\n",
              "            787 |  # node_matmul_22\n",
              "                   %\"matmul_22\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::MatMul(%\"getitem_33\", %\"transpose_11\")\n",
              "            788 |  # node_mul_3621\n",
              "                   %\"mul_3621\"<FLOAT,[((s0//8))*((s53//8)),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Mul(%\"matmul_22\", %\"val_51\"{0.1767766922712326})\n",
              "            789 |  # node_view_154\n",
              "                   %\"view_154\"<FLOAT,[((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))),((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Reshape(%\"mul_3621\", %\"val_678\") {allowzero=1}\n",
              "            790 |  # node_add_3375\n",
              "                   %\"add_3375\"<FLOAT,[1,((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,64,64]> ⬅️ ::Add(%\"view_154\", %\"unsqueeze_3\")\n",
              "            791 |  # node_view_155\n",
              "                   %\"view_155\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Reshape(%\"add_3375\", %\"val_684\") {allowzero=1}\n",
              "            792 |  # node_softmax_11\n",
              "                   %\"softmax_11\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),((s0//((s0//8))))*((s53//((s53//8))))]> ⬅️ ::Softmax(%\"view_155\") {axis=-1}\n",
              "            793 |  # node_matmul_23\n",
              "                   %\"matmul_23\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,((s0//((s0//8))))*((s53//((s53//8)))),32]> ⬅️ ::MatMul(%\"softmax_11\", %\"getitem_35\")\n",
              "            794 |  # node_permute_63\n",
              "                   %\"permute_63\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),8,32]> ⬅️ ::Transpose(%\"matmul_23\") {perm=(0, 2, 1, 3)}\n",
              "            795 |  # node_view_156\n",
              "                   %\"view_156\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Reshape(%\"permute_63\", %\"val_56\") {allowzero=1}\n",
              "            796 |  # node_MatMul_4238\n",
              "                   %\"val_4308\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::MatMul(%\"view_156\", %\"val_4307\"{...})\n",
              "            797 |  # node_linear_45\n",
              "                   %\"linear_45\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),((s0//((s0//8))))*((s53//((s53//8)))),256]> ⬅️ ::Add(%\"val_4308\", %\"layers.2.blocks.3.attn.proj.bias\"{...})\n",
              "            798 |  # node_view_157\n",
              "                   %\"view_157\"<FLOAT,[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))),8,8,256]> ⬅️ ::Reshape(%\"linear_45\", %\"val_64\"{[-1, 8, 8, 256]}) {allowzero=1}\n",
              "            799 |  # node_view_158\n",
              "                   %\"view_158\"<FLOAT,[1,(s53//8),(s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))),8,((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Reshape(%\"view_157\", %\"val_72\") {allowzero=1}\n",
              "            800 |  # node_permute_64\n",
              "                   %\"permute_64\"<FLOAT,[1,(s53//8),8,(s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))),((((s0//((s0//8))))*((s53//((s53//8)))))//8),256]> ⬅️ ::Transpose(%\"view_158\") {perm=(0, 1, 3, 2, 4, 5)}\n",
              "            801 |  # node_view_159\n",
              "                   %\"view_159\"<FLOAT,[1,8*((s53//8)),((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"permute_64\", %\"val_78\") {allowzero=1}\n",
              "            802 |  # node_Shape_4261\n",
              "                   %\"val_4331\"<INT64,[1]> ⬅️ ::Shape(%\"view_159\") {end=2, start=1}\n",
              "            803 |  # node_Sub_4263\n",
              "                   %\"val_4333\"<INT64,[1]> ⬅️ ::Sub(%\"val_4331\", %\"val_100\"{[4]})\n",
              "            804 |  # node_Slice_4265\n",
              "                   %\"val_4335\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"view_159\", %\"val_144\"{[0]}, %\"val_4333\", %\"val_152\"{[1]})\n",
              "            805 |  # node_Size_4266\n",
              "                   %\"val_4336\"<INT64,[]> ⬅️ ::Size(%\"view_159\")\n",
              "            806 |  # node_Reshape_4267\n",
              "                   %\"val_4337\"<INT64,[1]> ⬅️ ::Reshape(%\"val_4336\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            807 |  # node_Slice_4268\n",
              "                   %\"val_4338\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"view_159\", %\"val_4333\", %\"val_4337\", %\"val_152\"{[1]})\n",
              "            808 |  # node_Concat_4269\n",
              "                   %\"val_4339\"<FLOAT,[None,None,None,None]> ⬅️ ::Concat(%\"val_4338\", %\"val_4335\") {axis=1}\n",
              "            809 |  # node_Shape_4272\n",
              "                   %\"val_4342\"<INT64,[1]> ⬅️ ::Shape(%\"val_4339\") {end=3, start=2}\n",
              "            810 |  # node_Sub_4274\n",
              "                   %\"val_4344\"<INT64,[1]> ⬅️ ::Sub(%\"val_4342\", %\"val_100\"{[4]})\n",
              "            811 |  # node_Slice_4276\n",
              "                   %\"val_4346\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"val_4339\", %\"val_144\"{[0]}, %\"val_4344\", %\"val_163\"{[2]})\n",
              "            812 |  # node_Size_4277\n",
              "                   %\"val_4347\"<INT64,[]> ⬅️ ::Size(%\"val_4339\")\n",
              "            813 |  # node_Reshape_4278\n",
              "                   %\"val_4348\"<INT64,[1]> ⬅️ ::Reshape(%\"val_4347\", %\"val_3\"{[-1]}) {allowzero=0}\n",
              "            814 |  # node_Slice_4279\n",
              "                   %\"val_4349\"<FLOAT,[None,None,None,None]> ⬅️ ::Slice(%\"val_4339\", %\"val_4344\", %\"val_4348\", %\"val_163\"{[2]})\n",
              "            815 |  # node_roll_11\n",
              "                   %\"roll_11\"<FLOAT,[1,8*((s53//8)),((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Concat(%\"val_4349\", %\"val_4346\") {axis=2}\n",
              "            816 |  # node_view_160\n",
              "                   %\"view_160\"<FLOAT,[1,8*((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)),256]> ⬅️ ::Reshape(%\"roll_11\", %\"val_83\") {allowzero=1}\n",
              "            817 |  # node_add_3448\n",
              "                   %\"add_3448\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_3064\", %\"view_160\")\n",
              "            818 |  # node_layer_norm_23\n",
              "                   %\"layer_norm_23\"<FLOAT,[1,s0*s53,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_3448\", %\"layers.2.blocks.3.norm2.weight\"{...}, %\"layers.2.blocks.3.norm2.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            819 |  # node_MatMul_4286\n",
              "                   %\"val_4358\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::MatMul(%\"layer_norm_23\", %\"val_4357\"{...})\n",
              "            820 |  # node_linear_46\n",
              "                   %\"linear_46\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Add(%\"val_4358\", %\"layers.2.blocks.3.mlp.0.bias\"{...})\n",
              "            821 |  # node_gelu_11\n",
              "                   %\"gelu_11\"<FLOAT,[1,s0*s53,1024]> ⬅️ ::Gelu(%\"linear_46\") {approximate='none'}\n",
              "            822 |  # node_MatMul_4288\n",
              "                   %\"val_4360\"<FLOAT,[1,s0*s53,256]> ⬅️ ::MatMul(%\"gelu_11\", %\"val_4359\"{...})\n",
              "            823 |  # node_linear_47\n",
              "                   %\"linear_47\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"val_4360\", %\"layers.2.blocks.3.mlp.2.bias\"{...})\n",
              "            824 |  # node_add_3464\n",
              "                   %\"add_3464\"<FLOAT,[1,s0*s53,256]> ⬅️ ::Add(%\"add_3448\", %\"linear_47\")\n",
              "            825 |  # node_view_161\n",
              "                   %\"view_161\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Reshape(%\"add_3464\", %\"val_1468\") {allowzero=1}\n",
              "            826 |  # node_permute_65\n",
              "                   %\"permute_65\"<FLOAT,[1,256,s53,s0]> ⬅️ ::Transpose(%\"view_161\") {perm=(0, 3, 1, 2)}\n",
              "            827 |  # node_conv2d_3\n",
              "                   %\"conv2d_3\"<FLOAT,[1,256,s53,s0]> ⬅️ ::Conv(%\"permute_65\", %\"layers.2.conv.weight\"{...}, %\"layers.2.conv.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
              "            828 |  # node_permute_66\n",
              "                   %\"permute_66\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Transpose(%\"conv2d_3\") {perm=(0, 2, 3, 1)}\n",
              "            829 |  # node_add_3491\n",
              "                   %\"add_3491\"<FLOAT,[1,s53,s0,256]> ⬅️ ::Add(%\"add_2334\", %\"permute_66\")\n",
              "            830 |  # node_layer_norm_24\n",
              "                   %\"layer_norm_24\"<FLOAT,[1,s53,s0,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_3491\", %\"norm.weight\"{...}, %\"norm.bias\"{...}) {stash_type=1, epsilon=1e-05, axis=-1}\n",
              "            831 |  # node_permute_67\n",
              "                   %\"permute_67\"<FLOAT,[1,256,s53,s0]> ⬅️ ::Transpose(%\"layer_norm_24\") {perm=(0, 3, 1, 2)}\n",
              "            832 |  # node_conv2d_4\n",
              "                   %\"conv2d_4\"<FLOAT,[1,256,s53,s0]> ⬅️ ::Conv(%\"permute_67\", %\"conv_after_body.weight\"{...}, %\"conv_after_body.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
              "            833 |  # node_conv2d_5\n",
              "                   %\"conv2d_5\"<FLOAT,[1,1024,s53,s0]> ⬅️ ::Conv(%\"conv2d_4\", %\"upsample.0.weight\"{...}, %\"upsample.0.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
              "            834 |  # node_pixel_shuffle\n",
              "                   %\"pixel_shuffle\"<FLOAT,[1,256,2*s53,2*s0]> ⬅️ ::DepthToSpace(%\"conv2d_5\") {mode='CRD', blocksize=2}\n",
              "            835 |  # node_conv2d_6\n",
              "                   %\"conv2d_6\"<FLOAT,[1,1024,2*s53,2*s0]> ⬅️ ::Conv(%\"pixel_shuffle\", %\"upsample.2.weight\"{...}, %\"upsample.2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
              "            836 |  # node_pixel_shuffle_1\n",
              "                   %\"pixel_shuffle_1\"<FLOAT,[1,256,4*s53,4*s0]> ⬅️ ::DepthToSpace(%\"conv2d_6\") {mode='CRD', blocksize=2}\n",
              "            837 |  # node_conv2d_7\n",
              "                   %\"output\"<FLOAT,[1,3,4*s53,4*s0]> ⬅️ ::Conv(%\"pixel_shuffle_1\", %\"upsample.4.weight\"{...}, %\"upsample.4.bias\"{[-0.04729572683572769, -0.008816397748887539, 0.03684764355421066]}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
              "            return %\"output\"<FLOAT,[1,3,4*s53,4*s0]>\n",
              "        }\n",
              "\n",
              "\n",
              "    ,\n",
              "    exported_program=\n",
              "        ExportedProgram:\n",
              "            class GraphModule(torch.nn.Module):\n",
              "                def forward(self, p_conv_first_weight: \"f32[256, 3, 3, 3]\", p_conv_first_bias: \"f32[256]\", p_layers_0_blocks_0_norm1_weight: \"f32[256]\", p_layers_0_blocks_0_norm1_bias: \"f32[256]\", p_layers_0_blocks_0_attn_qkv_weight: \"f32[768, 256]\", p_layers_0_blocks_0_attn_qkv_bias: \"f32[768]\", p_layers_0_blocks_0_attn_proj_weight: \"f32[256, 256]\", p_layers_0_blocks_0_attn_proj_bias: \"f32[256]\", p_layers_0_blocks_0_norm2_weight: \"f32[256]\", p_layers_0_blocks_0_norm2_bias: \"f32[256]\", p_layers_0_blocks_0_mlp_0_weight: \"f32[1024, 256]\", p_layers_0_blocks_0_mlp_0_bias: \"f32[1024]\", p_layers_0_blocks_0_mlp_2_weight: \"f32[256, 1024]\", p_layers_0_blocks_0_mlp_2_bias: \"f32[256]\", p_layers_0_blocks_1_norm1_weight: \"f32[256]\", p_layers_0_blocks_1_norm1_bias: \"f32[256]\", p_layers_0_blocks_1_attn_qkv_weight: \"f32[768, 256]\", p_layers_0_blocks_1_attn_qkv_bias: \"f32[768]\", p_layers_0_blocks_1_attn_proj_weight: \"f32[256, 256]\", p_layers_0_blocks_1_attn_proj_bias: \"f32[256]\", p_layers_0_blocks_1_norm2_weight: \"f32[256]\", p_layers_0_blocks_1_norm2_bias: \"f32[256]\", p_layers_0_blocks_1_mlp_0_weight: \"f32[1024, 256]\", p_layers_0_blocks_1_mlp_0_bias: \"f32[1024]\", p_layers_0_blocks_1_mlp_2_weight: \"f32[256, 1024]\", p_layers_0_blocks_1_mlp_2_bias: \"f32[256]\", p_layers_0_blocks_2_norm1_weight: \"f32[256]\", p_layers_0_blocks_2_norm1_bias: \"f32[256]\", p_layers_0_blocks_2_attn_qkv_weight: \"f32[768, 256]\", p_layers_0_blocks_2_attn_qkv_bias: \"f32[768]\", p_layers_0_blocks_2_attn_proj_weight: \"f32[256, 256]\", p_layers_0_blocks_2_attn_proj_bias: \"f32[256]\", p_layers_0_blocks_2_norm2_weight: \"f32[256]\", p_layers_0_blocks_2_norm2_bias: \"f32[256]\", p_layers_0_blocks_2_mlp_0_weight: \"f32[1024, 256]\", p_layers_0_blocks_2_mlp_0_bias: \"f32[1024]\", p_layers_0_blocks_2_mlp_2_weight: \"f32[256, 1024]\", p_layers_0_blocks_2_mlp_2_bias: \"f32[256]\", p_layers_0_blocks_3_norm1_weight: \"f32[256]\", p_layers_0_blocks_3_norm1_bias: \"f32[256]\", p_layers_0_blocks_3_attn_qkv_weight: \"f32[768, 256]\", p_layers_0_blocks_3_attn_qkv_bias: \"f32[768]\", p_layers_0_blocks_3_attn_proj_weight: \"f32[256, 256]\", p_layers_0_blocks_3_attn_proj_bias: \"f32[256]\", p_layers_0_blocks_3_norm2_weight: \"f32[256]\", p_layers_0_blocks_3_norm2_bias: \"f32[256]\", p_layers_0_blocks_3_mlp_0_weight: \"f32[1024, 256]\", p_layers_0_blocks_3_mlp_0_bias: \"f32[1024]\", p_layers_0_blocks_3_mlp_2_weight: \"f32[256, 1024]\", p_layers_0_blocks_3_mlp_2_bias: \"f32[256]\", p_layers_0_conv_weight: \"f32[256, 256, 3, 3]\", p_layers_0_conv_bias: \"f32[256]\", p_layers_1_blocks_0_norm1_weight: \"f32[256]\", p_layers_1_blocks_0_norm1_bias: \"f32[256]\", p_layers_1_blocks_0_attn_qkv_weight: \"f32[768, 256]\", p_layers_1_blocks_0_attn_qkv_bias: \"f32[768]\", p_layers_1_blocks_0_attn_proj_weight: \"f32[256, 256]\", p_layers_1_blocks_0_attn_proj_bias: \"f32[256]\", p_layers_1_blocks_0_norm2_weight: \"f32[256]\", p_layers_1_blocks_0_norm2_bias: \"f32[256]\", p_layers_1_blocks_0_mlp_0_weight: \"f32[1024, 256]\", p_layers_1_blocks_0_mlp_0_bias: \"f32[1024]\", p_layers_1_blocks_0_mlp_2_weight: \"f32[256, 1024]\", p_layers_1_blocks_0_mlp_2_bias: \"f32[256]\", p_layers_1_blocks_1_norm1_weight: \"f32[256]\", p_layers_1_blocks_1_norm1_bias: \"f32[256]\", p_layers_1_blocks_1_attn_qkv_weight: \"f32[768, 256]\", p_layers_1_blocks_1_attn_qkv_bias: \"f32[768]\", p_layers_1_blocks_1_attn_proj_weight: \"f32[256, 256]\", p_layers_1_blocks_1_attn_proj_bias: \"f32[256]\", p_layers_1_blocks_1_norm2_weight: \"f32[256]\", p_layers_1_blocks_1_norm2_bias: \"f32[256]\", p_layers_1_blocks_1_mlp_0_weight: \"f32[1024, 256]\", p_layers_1_blocks_1_mlp_0_bias: \"f32[1024]\", p_layers_1_blocks_1_mlp_2_weight: \"f32[256, 1024]\", p_layers_1_blocks_1_mlp_2_bias: \"f32[256]\", p_layers_1_blocks_2_norm1_weight: \"f32[256]\", p_layers_1_blocks_2_norm1_bias: \"f32[256]\", p_layers_1_blocks_2_attn_qkv_weight: \"f32[768, 256]\", p_layers_1_blocks_2_attn_qkv_bias: \"f32[768]\", p_layers_1_blocks_2_attn_proj_weight: \"f32[256, 256]\", p_layers_1_blocks_2_attn_proj_bias: \"f32[256]\", p_layers_1_blocks_2_norm2_weight: \"f32[256]\", p_layers_1_blocks_2_norm2_bias: \"f32[256]\", p_layers_1_blocks_2_mlp_0_weight: \"f32[1024, 256]\", p_layers_1_blocks_2_mlp_0_bias: \"f32[1024]\", p_layers_1_blocks_2_mlp_2_weight: \"f32[256, 1024]\", p_layers_1_blocks_2_mlp_2_bias: \"f32[256]\", p_layers_1_blocks_3_norm1_weight: \"f32[256]\", p_layers_1_blocks_3_norm1_bias: \"f32[256]\", p_layers_1_blocks_3_attn_qkv_weight: \"f32[768, 256]\", p_layers_1_blocks_3_attn_qkv_bias: \"f32[768]\", p_layers_1_blocks_3_attn_proj_weight: \"f32[256, 256]\", p_layers_1_blocks_3_attn_proj_bias: \"f32[256]\", p_layers_1_blocks_3_norm2_weight: \"f32[256]\", p_layers_1_blocks_3_norm2_bias: \"f32[256]\", p_layers_1_blocks_3_mlp_0_weight: \"f32[1024, 256]\", p_layers_1_blocks_3_mlp_0_bias: \"f32[1024]\", p_layers_1_blocks_3_mlp_2_weight: \"f32[256, 1024]\", p_layers_1_blocks_3_mlp_2_bias: \"f32[256]\", p_layers_1_conv_weight: \"f32[256, 256, 3, 3]\", p_layers_1_conv_bias: \"f32[256]\", p_layers_2_blocks_0_norm1_weight: \"f32[256]\", p_layers_2_blocks_0_norm1_bias: \"f32[256]\", p_layers_2_blocks_0_attn_qkv_weight: \"f32[768, 256]\", p_layers_2_blocks_0_attn_qkv_bias: \"f32[768]\", p_layers_2_blocks_0_attn_proj_weight: \"f32[256, 256]\", p_layers_2_blocks_0_attn_proj_bias: \"f32[256]\", p_layers_2_blocks_0_norm2_weight: \"f32[256]\", p_layers_2_blocks_0_norm2_bias: \"f32[256]\", p_layers_2_blocks_0_mlp_0_weight: \"f32[1024, 256]\", p_layers_2_blocks_0_mlp_0_bias: \"f32[1024]\", p_layers_2_blocks_0_mlp_2_weight: \"f32[256, 1024]\", p_layers_2_blocks_0_mlp_2_bias: \"f32[256]\", p_layers_2_blocks_1_norm1_weight: \"f32[256]\", p_layers_2_blocks_1_norm1_bias: \"f32[256]\", p_layers_2_blocks_1_attn_qkv_weight: \"f32[768, 256]\", p_layers_2_blocks_1_attn_qkv_bias: \"f32[768]\", p_layers_2_blocks_1_attn_proj_weight: \"f32[256, 256]\", p_layers_2_blocks_1_attn_proj_bias: \"f32[256]\", p_layers_2_blocks_1_norm2_weight: \"f32[256]\", p_layers_2_blocks_1_norm2_bias: \"f32[256]\", p_layers_2_blocks_1_mlp_0_weight: \"f32[1024, 256]\", p_layers_2_blocks_1_mlp_0_bias: \"f32[1024]\", p_layers_2_blocks_1_mlp_2_weight: \"f32[256, 1024]\", p_layers_2_blocks_1_mlp_2_bias: \"f32[256]\", p_layers_2_blocks_2_norm1_weight: \"f32[256]\", p_layers_2_blocks_2_norm1_bias: \"f32[256]\", p_layers_2_blocks_2_attn_qkv_weight: \"f32[768, 256]\", p_layers_2_blocks_2_attn_qkv_bias: \"f32[768]\", p_layers_2_blocks_2_attn_proj_weight: \"f32[256, 256]\", p_layers_2_blocks_2_attn_proj_bias: \"f32[256]\", p_layers_2_blocks_2_norm2_weight: \"f32[256]\", p_layers_2_blocks_2_norm2_bias: \"f32[256]\", p_layers_2_blocks_2_mlp_0_weight: \"f32[1024, 256]\", p_layers_2_blocks_2_mlp_0_bias: \"f32[1024]\", p_layers_2_blocks_2_mlp_2_weight: \"f32[256, 1024]\", p_layers_2_blocks_2_mlp_2_bias: \"f32[256]\", p_layers_2_blocks_3_norm1_weight: \"f32[256]\", p_layers_2_blocks_3_norm1_bias: \"f32[256]\", p_layers_2_blocks_3_attn_qkv_weight: \"f32[768, 256]\", p_layers_2_blocks_3_attn_qkv_bias: \"f32[768]\", p_layers_2_blocks_3_attn_proj_weight: \"f32[256, 256]\", p_layers_2_blocks_3_attn_proj_bias: \"f32[256]\", p_layers_2_blocks_3_norm2_weight: \"f32[256]\", p_layers_2_blocks_3_norm2_bias: \"f32[256]\", p_layers_2_blocks_3_mlp_0_weight: \"f32[1024, 256]\", p_layers_2_blocks_3_mlp_0_bias: \"f32[1024]\", p_layers_2_blocks_3_mlp_2_weight: \"f32[256, 1024]\", p_layers_2_blocks_3_mlp_2_bias: \"f32[256]\", p_layers_2_conv_weight: \"f32[256, 256, 3, 3]\", p_layers_2_conv_bias: \"f32[256]\", p_norm_weight: \"f32[256]\", p_norm_bias: \"f32[256]\", p_conv_after_body_weight: \"f32[256, 256, 3, 3]\", p_conv_after_body_bias: \"f32[256]\", p_upsample_0_weight: \"f32[1024, 256, 3, 3]\", p_upsample_0_bias: \"f32[1024]\", p_upsample_2_weight: \"f32[1024, 256, 3, 3]\", p_upsample_2_bias: \"f32[1024]\", p_upsample_4_weight: \"f32[3, 256, 3, 3]\", p_upsample_4_bias: \"f32[3]\", c_layers_0_blocks_1_lifted_tensor_0: \"f32[]\", c_layers_0_blocks_1_lifted_tensor_1: \"f32[]\", c_layers_0_blocks_1_lifted_tensor_2: \"f32[]\", c_layers_0_blocks_1_lifted_tensor_3: \"f32[]\", c_layers_0_blocks_1_lifted_tensor_4: \"f32[]\", c_layers_0_blocks_1_lifted_tensor_5: \"f32[]\", c_layers_0_blocks_1_lifted_tensor_6: \"f32[]\", c_layers_0_blocks_1_lifted_tensor_7: \"f32[]\", c_layers_0_blocks_1_lifted_tensor_8: \"f32[]\", c_layers_0_blocks_3_lifted_tensor_9: \"f32[]\", c_layers_0_blocks_3_lifted_tensor_10: \"f32[]\", c_layers_0_blocks_3_lifted_tensor_11: \"f32[]\", c_layers_0_blocks_3_lifted_tensor_12: \"f32[]\", c_layers_0_blocks_3_lifted_tensor_13: \"f32[]\", c_layers_0_blocks_3_lifted_tensor_14: \"f32[]\", c_layers_0_blocks_3_lifted_tensor_15: \"f32[]\", c_layers_0_blocks_3_lifted_tensor_16: \"f32[]\", c_layers_0_blocks_3_lifted_tensor_17: \"f32[]\", c_layers_1_blocks_1_lifted_tensor_18: \"f32[]\", c_layers_1_blocks_1_lifted_tensor_19: \"f32[]\", c_layers_1_blocks_1_lifted_tensor_20: \"f32[]\", c_layers_1_blocks_1_lifted_tensor_21: \"f32[]\", c_layers_1_blocks_1_lifted_tensor_22: \"f32[]\", c_layers_1_blocks_1_lifted_tensor_23: \"f32[]\", c_layers_1_blocks_1_lifted_tensor_24: \"f32[]\", c_layers_1_blocks_1_lifted_tensor_25: \"f32[]\", c_layers_1_blocks_1_lifted_tensor_26: \"f32[]\", c_layers_1_blocks_3_lifted_tensor_27: \"f32[]\", c_layers_1_blocks_3_lifted_tensor_28: \"f32[]\", c_layers_1_blocks_3_lifted_tensor_29: \"f32[]\", c_layers_1_blocks_3_lifted_tensor_30: \"f32[]\", c_layers_1_blocks_3_lifted_tensor_31: \"f32[]\", c_layers_1_blocks_3_lifted_tensor_32: \"f32[]\", c_layers_1_blocks_3_lifted_tensor_33: \"f32[]\", c_layers_1_blocks_3_lifted_tensor_34: \"f32[]\", c_layers_1_blocks_3_lifted_tensor_35: \"f32[]\", c_layers_2_blocks_1_lifted_tensor_36: \"f32[]\", c_layers_2_blocks_1_lifted_tensor_37: \"f32[]\", c_layers_2_blocks_1_lifted_tensor_38: \"f32[]\", c_layers_2_blocks_1_lifted_tensor_39: \"f32[]\", c_layers_2_blocks_1_lifted_tensor_40: \"f32[]\", c_layers_2_blocks_1_lifted_tensor_41: \"f32[]\", c_layers_2_blocks_1_lifted_tensor_42: \"f32[]\", c_layers_2_blocks_1_lifted_tensor_43: \"f32[]\", c_layers_2_blocks_1_lifted_tensor_44: \"f32[]\", c_layers_2_blocks_3_lifted_tensor_45: \"f32[]\", c_layers_2_blocks_3_lifted_tensor_46: \"f32[]\", c_layers_2_blocks_3_lifted_tensor_47: \"f32[]\", c_layers_2_blocks_3_lifted_tensor_48: \"f32[]\", c_layers_2_blocks_3_lifted_tensor_49: \"f32[]\", c_layers_2_blocks_3_lifted_tensor_50: \"f32[]\", c_layers_2_blocks_3_lifted_tensor_51: \"f32[]\", c_layers_2_blocks_3_lifted_tensor_52: \"f32[]\", c_layers_2_blocks_3_lifted_tensor_53: \"f32[]\", x: \"f32[s77, 3, s53, s0]\"):\n",
              "                     # \n",
              "                    sym_size_int_317: \"Sym(s77)\" = torch.ops.aten.sym_size.int(x, 0)\n",
              "                    sym_size_int_318: \"Sym(s53)\" = torch.ops.aten.sym_size.int(x, 2)\n",
              "                    sym_size_int_319: \"Sym(s0)\" = torch.ops.aten.sym_size.int(x, 3)\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
              "                    conv2d: \"f32[s77, 256, s53, s0]\" = torch.ops.aten.conv2d.default(x, p_conv_first_weight, p_conv_first_bias, [1, 1], [1, 1]);  x = p_conv_first_weight = p_conv_first_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:164 in forward, code: x=x.permute(0,2,3,1).contiguous()\n",
              "                    permute: \"f32[s77, s53, s0, 256]\" = torch.ops.aten.permute.default(conv2d, [0, 2, 3, 1]);  conv2d = None\n",
              "                    clone: \"f32[s77, s53, s0, 256]\" = torch.ops.aten.clone.default(permute, memory_format = torch.contiguous_format);  permute = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:132 in forward, code: x=x.view(B,H*W,C)\n",
              "                    mul_471: \"Sym(s0*s53)\" = sym_size_int_318 * sym_size_int_319\n",
              "                    view: \"f32[s77, s0*s53, 256]\" = torch.ops.aten.view.default(clone, [sym_size_int_317, mul_471, 256])\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm: \"f32[s77, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(view, [256], p_layers_0_blocks_0_norm1_weight, p_layers_0_blocks_0_norm1_bias);  p_layers_0_blocks_0_norm1_weight = p_layers_0_blocks_0_norm1_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:92 in forward, code: x=self.norm1(x).view(x.shape[0], H, W, -1)\n",
              "                    view_1: \"f32[s77, s53, s0, 256]\" = torch.ops.aten.view.default(layer_norm, [sym_size_int_317, sym_size_int_318, sym_size_int_319, -1]);  layer_norm = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:97 in forward, code: x_windows=window_partition(x, self.window_size)\n",
              "                    floordiv_388: \"Sym((s53//8))\" = sym_size_int_318 // 8\n",
              "                    floordiv_389: \"Sym((s0//8))\" = sym_size_int_319 // 8\n",
              "                    view_2: \"f32[s77, (s53//8), (s53//((s53//8))), (s0//8), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(view_1, [sym_size_int_317, floordiv_388, 8, floordiv_389, 8, -1]);  view_1 = None\n",
              "                    permute_1: \"f32[s77, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.permute.default(view_2, [0, 1, 3, 2, 4, 5]);  view_2 = None\n",
              "                    clone_1: \"f32[s77, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
              "                    view_3: \"f32[((s0//8))*((s53//8)), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(clone_1, [-1, 8, 8, 256]);  clone_1 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:98 in forward, code: x_windows=x_windows.view(-1, self.window_size*self.window_size, C)    #(B_, N, C)\n",
              "                    view_4: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(view_3, [-1, 64, 256]);  view_3 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 768]\" = torch.ops.aten.linear.default(view_4, p_layers_0_blocks_0_attn_qkv_weight, p_layers_0_blocks_0_attn_qkv_bias);  view_4 = p_layers_0_blocks_0_attn_qkv_weight = p_layers_0_blocks_0_attn_qkv_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:42 in forward, code: qkv=self.qkv(x).view(B_, N, 3, self.n_heads,C//self.n_heads)\n",
              "                    mul_523: \"Sym(4*s0)\" = 4 * sym_size_int_319\n",
              "                    mul_524: \"Sym(4*s0*s53)\" = mul_523 * sym_size_int_318;  mul_523 = None\n",
              "                    mul_525: \"Sym(((s0//8))*((s53//8)))\" = floordiv_389 * floordiv_388\n",
              "                    floordiv_390: \"Sym(((4*s0*s53)//(((s0//8))*((s53//8)))))\" = mul_524 // mul_525;  mul_524 = None\n",
              "                    mul_526: \"Sym(s77*((s0//8)))\" = sym_size_int_317 * floordiv_389\n",
              "                    mul_527: \"Sym(s77*((s0//8))*((s53//8)))\" = mul_526 * floordiv_388;  mul_526 = None\n",
              "                    mul_528: \"Sym(s77*((s0//8))*((s53//8))*(((4*s0*s53)//(((s0//8))*((s53//8))))))\" = mul_527 * floordiv_390;  mul_527 = floordiv_390 = None\n",
              "                    floordiv_391: \"Sym(((s77*((s0//8))*((s53//8))*(((4*s0*s53)//(((s0//8))*((s53//8))))))//256))\" = mul_528 // 256;  mul_528 = None\n",
              "                    view_5: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 3, 8, 32]\" = torch.ops.aten.view.default(linear, [floordiv_391, 64, 3, 8, 32]);  linear = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:43 in forward, code: q, k, v=qkv.permute(2,0,3,1,4).contiguous()\n",
              "                    permute_2: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.permute.default(view_5, [2, 0, 3, 1, 4]);  view_5 = None\n",
              "                    clone_2: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None\n",
              "                    unbind = torch.ops.aten.unbind.int(clone_2);  clone_2 = None\n",
              "                    getitem: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind[0]\n",
              "                    getitem_1: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind[1]\n",
              "                    getitem_2: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind[2];  unbind = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:44 in forward, code: attn=(q@k.transpose(-2, -1))*self.scale\n",
              "                    transpose: \"f32[((s0//8))*((s53//8)), 8, 32, ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.transpose.int(getitem_1, -2, -1);  getitem_1 = None\n",
              "                    matmul: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.matmul.default(getitem, transpose);  getitem = transpose = None\n",
              "                    mul_568: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.mul.Tensor(matmul, 0.1767766952966369);  matmul = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:51 in forward, code: attn=attn.softmax(dim=-1)\n",
              "                    softmax: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.softmax.int(mul_568, -1);  mul_568 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:52 in forward, code: attn=(attn@v).permute(0,2,1,3).contiguous().view(B_, N, C)\n",
              "                    matmul_1: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.matmul.default(softmax, getitem_2);  softmax = getitem_2 = None\n",
              "                    permute_3: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.permute.default(matmul_1, [0, 2, 1, 3]);  matmul_1 = None\n",
              "                    clone_3: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None\n",
              "                    view_6: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(clone_3, [floordiv_391, 64, 256]);  clone_3 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_1: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.linear.default(view_6, p_layers_0_blocks_0_attn_proj_weight, p_layers_0_blocks_0_attn_proj_bias);  view_6 = p_layers_0_blocks_0_attn_proj_weight = p_layers_0_blocks_0_attn_proj_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:103 in forward, code: x_attn=x_attn.view(-1,self.window_size, self.window_size, C)\n",
              "                    view_7: \"f32[((s0//8))*((s53//8)), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(linear_1, [-1, 8, 8, 256]);  linear_1 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:104 in forward, code: x=window_reverse(x_attn, self.window_size, H, W)   # (B, H, W ,C)\n",
              "                    view_8: \"f32[1, (s53//8), (s0//8), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(view_7, [1, floordiv_388, floordiv_389, 8, 8, -1]);  view_7 = None\n",
              "                    permute_4: \"f32[1, (s53//8), 8, (s0//8), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.permute.default(view_8, [0, 1, 3, 2, 4, 5]);  view_8 = None\n",
              "                    clone_4: \"f32[1, (s53//8), 8, (s0//8), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None\n",
              "                    view_9: \"f32[1, 8*((s53//8)), ((s0//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(clone_4, [1, sym_size_int_318, sym_size_int_319, -1]);  clone_4 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:109 in forward, code: x=x.view(-1, H*W, C)\n",
              "                    view_10: \"f32[1, 8*((s0//8))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(view_9, [-1, mul_471, 256]);  view_9 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:110 in forward, code: x=shortcut+self.drop_path1(x)\n",
              "                    add_171: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(view, view_10);  view = view_10 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_1: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_171, [256], p_layers_0_blocks_0_norm2_weight, p_layers_0_blocks_0_norm2_bias);  p_layers_0_blocks_0_norm2_weight = p_layers_0_blocks_0_norm2_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_2: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.linear.default(layer_norm_1, p_layers_0_blocks_0_mlp_0_weight, p_layers_0_blocks_0_mlp_0_bias);  layer_norm_1 = p_layers_0_blocks_0_mlp_0_weight = p_layers_0_blocks_0_mlp_0_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
              "                    gelu: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.gelu.default(linear_2);  linear_2 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_3: \"f32[1, s0*s53, 256]\" = torch.ops.aten.linear.default(gelu, p_layers_0_blocks_0_mlp_2_weight, p_layers_0_blocks_0_mlp_2_bias);  gelu = p_layers_0_blocks_0_mlp_2_weight = p_layers_0_blocks_0_mlp_2_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:111 in forward, code: x=x+self.drop_path2(self.mlp(self.norm2(x)))\n",
              "                    add_187: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_171, linear_3);  add_171 = linear_3 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_2: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_187, [256], p_layers_0_blocks_1_norm1_weight, p_layers_0_blocks_1_norm1_bias);  p_layers_0_blocks_1_norm1_weight = p_layers_0_blocks_1_norm1_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:92 in forward, code: x=self.norm1(x).view(x.shape[0], H, W, -1)\n",
              "                    view_11: \"f32[1, s53, s0, 256]\" = torch.ops.aten.view.default(layer_norm_2, [sym_size_int_317, sym_size_int_318, sym_size_int_319, -1]);  layer_norm_2 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:95 in forward, code: x=torch.roll(x, shifts=(-self.shift_size, -self.shift_size),dims=(1,2))\n",
              "                    roll: \"f32[1, s53, s0, 256]\" = torch.ops.aten.roll.default(view_11, [-4, -4], [1, 2]);  view_11 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:97 in forward, code: x_windows=window_partition(x, self.window_size)\n",
              "                    view_12: \"f32[1, (s53//8), (s53//((s53//8))), (s0//8), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(roll, [sym_size_int_317, floordiv_388, 8, floordiv_389, 8, -1]);  roll = None\n",
              "                    permute_5: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.permute.default(view_12, [0, 1, 3, 2, 4, 5]);  view_12 = None\n",
              "                    clone_5: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None\n",
              "                    view_13: \"f32[((s0//8))*((s53//8)), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(clone_5, [-1, 8, 8, 256]);  clone_5 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:98 in forward, code: x_windows=x_windows.view(-1, self.window_size*self.window_size, C)    #(B_, N, C)\n",
              "                    view_14: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(view_13, [-1, 64, 256]);  view_13 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:100 in forward, code: attn_mask=get_attn_mask(self.window_size, self.shift_size, H, W, x.device) if self.shift_size>0 else None\n",
              "                    zeros: \"f32[1, s53, s0, 1]\" = torch.ops.aten.zeros.default([1, sym_size_int_318, sym_size_int_319, 1], device = device(type='cpu'), pin_memory = False)\n",
              "                    clone_6: \"f32[]\" = torch.ops.aten.clone.default(c_layers_0_blocks_1_lifted_tensor_0);  c_layers_0_blocks_1_lifted_tensor_0 = None\n",
              "                    slice_1: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(zeros, 1, 0, -8)\n",
              "                    slice_2: \"f32[1, s53 - 8, s0 - 8, 1]\" = torch.ops.aten.slice.Tensor(slice_1, 2, 0, -8);  slice_1 = None\n",
              "                    fill: \"f32[1, s53 - 8, s0 - 8, 1]\" = torch.ops.aten.fill.Tensor(slice_2, clone_6);  slice_2 = clone_6 = None\n",
              "                    slice_3: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(zeros, 1, 0, -8)\n",
              "                    slice_scatter: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_3, fill, 2, 0, -8);  slice_3 = fill = None\n",
              "                    slice_scatter_1: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(zeros, slice_scatter, 1, 0, -8);  zeros = slice_scatter = None\n",
              "                    clone_7: \"f32[]\" = torch.ops.aten.clone.default(c_layers_0_blocks_1_lifted_tensor_1);  c_layers_0_blocks_1_lifted_tensor_1 = None\n",
              "                    slice_8: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_1, 1, 0, -8)\n",
              "                    slice_9: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_8, 2, -8, -4);  slice_8 = None\n",
              "                    fill_1: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_9, clone_7);  slice_9 = clone_7 = None\n",
              "                    slice_10: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_1, 1, 0, -8)\n",
              "                    slice_scatter_2: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_10, fill_1, 2, -8, -4);  slice_10 = fill_1 = None\n",
              "                    slice_scatter_3: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_1, slice_scatter_2, 1, 0, -8);  slice_scatter_1 = slice_scatter_2 = None\n",
              "                    clone_8: \"f32[]\" = torch.ops.aten.clone.default(c_layers_0_blocks_1_lifted_tensor_2);  c_layers_0_blocks_1_lifted_tensor_2 = None\n",
              "                    slice_15: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_3, 1, 0, -8)\n",
              "                    slice_16: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_15, 2, -4, 9223372036854775807);  slice_15 = None\n",
              "                    fill_2: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_16, clone_8);  slice_16 = clone_8 = None\n",
              "                    slice_17: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_3, 1, 0, -8)\n",
              "                    slice_scatter_4: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_17, fill_2, 2, -4, 9223372036854775807);  slice_17 = fill_2 = None\n",
              "                    slice_scatter_5: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_3, slice_scatter_4, 1, 0, -8);  slice_scatter_3 = slice_scatter_4 = None\n",
              "                    clone_9: \"f32[]\" = torch.ops.aten.clone.default(c_layers_0_blocks_1_lifted_tensor_3);  c_layers_0_blocks_1_lifted_tensor_3 = None\n",
              "                    slice_22: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_5, 1, -8, -4)\n",
              "                    slice_23: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.slice.Tensor(slice_22, 2, 0, -8);  slice_22 = None\n",
              "                    fill_3: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.fill.Tensor(slice_23, clone_9);  slice_23 = clone_9 = None\n",
              "                    slice_24: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_5, 1, -8, -4)\n",
              "                    slice_scatter_6: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_24, fill_3, 2, 0, -8);  slice_24 = fill_3 = None\n",
              "                    slice_scatter_7: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_5, slice_scatter_6, 1, -8, -4);  slice_scatter_5 = slice_scatter_6 = None\n",
              "                    clone_10: \"f32[]\" = torch.ops.aten.clone.default(c_layers_0_blocks_1_lifted_tensor_4);  c_layers_0_blocks_1_lifted_tensor_4 = None\n",
              "                    slice_29: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_7, 1, -8, -4)\n",
              "                    slice_30: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_29, 2, -8, -4);  slice_29 = None\n",
              "                    fill_4: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_30, clone_10);  slice_30 = clone_10 = None\n",
              "                    slice_31: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_7, 1, -8, -4)\n",
              "                    slice_scatter_8: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_31, fill_4, 2, -8, -4);  slice_31 = fill_4 = None\n",
              "                    slice_scatter_9: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_7, slice_scatter_8, 1, -8, -4);  slice_scatter_7 = slice_scatter_8 = None\n",
              "                    clone_11: \"f32[]\" = torch.ops.aten.clone.default(c_layers_0_blocks_1_lifted_tensor_5);  c_layers_0_blocks_1_lifted_tensor_5 = None\n",
              "                    slice_36: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_9, 1, -8, -4)\n",
              "                    slice_37: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_36, 2, -4, 9223372036854775807);  slice_36 = None\n",
              "                    fill_5: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_37, clone_11);  slice_37 = clone_11 = None\n",
              "                    slice_38: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_9, 1, -8, -4)\n",
              "                    slice_scatter_10: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_38, fill_5, 2, -4, 9223372036854775807);  slice_38 = fill_5 = None\n",
              "                    slice_scatter_11: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_9, slice_scatter_10, 1, -8, -4);  slice_scatter_9 = slice_scatter_10 = None\n",
              "                    clone_12: \"f32[]\" = torch.ops.aten.clone.default(c_layers_0_blocks_1_lifted_tensor_6);  c_layers_0_blocks_1_lifted_tensor_6 = None\n",
              "                    slice_43: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_11, 1, -4, 9223372036854775807)\n",
              "                    slice_44: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.slice.Tensor(slice_43, 2, 0, -8);  slice_43 = None\n",
              "                    fill_6: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.fill.Tensor(slice_44, clone_12);  slice_44 = clone_12 = None\n",
              "                    slice_45: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_11, 1, -4, 9223372036854775807)\n",
              "                    slice_scatter_12: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_45, fill_6, 2, 0, -8);  slice_45 = fill_6 = None\n",
              "                    slice_scatter_13: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_11, slice_scatter_12, 1, -4, 9223372036854775807);  slice_scatter_11 = slice_scatter_12 = None\n",
              "                    clone_13: \"f32[]\" = torch.ops.aten.clone.default(c_layers_0_blocks_1_lifted_tensor_7);  c_layers_0_blocks_1_lifted_tensor_7 = None\n",
              "                    slice_50: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_13, 1, -4, 9223372036854775807)\n",
              "                    slice_51: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_50, 2, -8, -4);  slice_50 = None\n",
              "                    fill_7: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_51, clone_13);  slice_51 = clone_13 = None\n",
              "                    slice_52: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_13, 1, -4, 9223372036854775807)\n",
              "                    slice_scatter_14: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_52, fill_7, 2, -8, -4);  slice_52 = fill_7 = None\n",
              "                    slice_scatter_15: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_13, slice_scatter_14, 1, -4, 9223372036854775807);  slice_scatter_13 = slice_scatter_14 = None\n",
              "                    clone_14: \"f32[]\" = torch.ops.aten.clone.default(c_layers_0_blocks_1_lifted_tensor_8);  c_layers_0_blocks_1_lifted_tensor_8 = None\n",
              "                    slice_57: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_15, 1, -4, 9223372036854775807)\n",
              "                    slice_58: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_57, 2, -4, 9223372036854775807);  slice_57 = None\n",
              "                    fill_8: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_58, clone_14);  slice_58 = clone_14 = None\n",
              "                    slice_59: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_15, 1, -4, 9223372036854775807)\n",
              "                    slice_scatter_16: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_59, fill_8, 2, -4, 9223372036854775807);  slice_59 = fill_8 = None\n",
              "                    slice_scatter_17: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_15, slice_scatter_16, 1, -4, 9223372036854775807);  slice_scatter_15 = slice_scatter_16 = None\n",
              "                    view_16: \"f32[1, (s53//8), 8, (s0//8), 8, ((s0*s53)//(64*((s0//8))*((s53//8))))]\" = torch.ops.aten.view.default(slice_scatter_17, [1, floordiv_388, 8, floordiv_389, 8, -1]);  slice_scatter_17 = None\n",
              "                    permute_7: \"f32[1, (s53//8), (s0//8), 8, 8, ((s0*s53)//(64*((s0//8))*((s53//8))))]\" = torch.ops.aten.permute.default(view_16, [0, 1, 3, 2, 4, 5]);  view_16 = None\n",
              "                    clone_15: \"f32[1, (s53//8), (s0//8), 8, 8, ((s0*s53)//(64*((s0//8))*((s53//8))))]\" = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None\n",
              "                    view_17: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 8, 8, 1]\" = torch.ops.aten.view.default(clone_15, [-1, 8, 8, 1]);  clone_15 = None\n",
              "                    mul_843: \"Sym(64*((s0//8)))\" = 64 * floordiv_389\n",
              "                    mul_844: \"Sym(64*((s0//8))*((s53//8)))\" = mul_843 * floordiv_388;  mul_843 = None\n",
              "                    floordiv_392: \"Sym(((s0*s53)//(64*((s0//8))*((s53//8)))))\" = mul_471 // mul_844;  mul_844 = None\n",
              "                    mul_846: \"Sym(((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))))\" = mul_525 * floordiv_392;  mul_525 = floordiv_392 = None\n",
              "                    view_18: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64]\" = torch.ops.aten.view.default(view_17, [mul_846, -1]);  view_17 = None\n",
              "                    unsqueeze: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 1, 64]\" = torch.ops.aten.unsqueeze.default(view_18, 1)\n",
              "                    unsqueeze_1: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 1]\" = torch.ops.aten.unsqueeze.default(view_18, 2);  view_18 = None\n",
              "                    sub_158: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.sub.Tensor(unsqueeze, unsqueeze_1);  unsqueeze = unsqueeze_1 = None\n",
              "                    scalar_tensor_default: \"f32[]\" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32)\n",
              "                    ne_36: \"b8[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.ne.Tensor(sub_158, scalar_tensor_default);  scalar_tensor_default = None\n",
              "                    masked_fill: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.masked_fill.Scalar(sub_158, ne_36, -100.0);  ne_36 = None\n",
              "                    scalar_tensor_default_1: \"f32[]\" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32)\n",
              "                    eq_265: \"b8[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.eq.Tensor(sub_158, scalar_tensor_default_1);  sub_158 = scalar_tensor_default_1 = None\n",
              "                    masked_fill_1: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.masked_fill.Scalar(masked_fill, eq_265, 0.0);  masked_fill = eq_265 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_4: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 768]\" = torch.ops.aten.linear.default(view_14, p_layers_0_blocks_1_attn_qkv_weight, p_layers_0_blocks_1_attn_qkv_bias);  view_14 = p_layers_0_blocks_1_attn_qkv_weight = p_layers_0_blocks_1_attn_qkv_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:42 in forward, code: qkv=self.qkv(x).view(B_, N, 3, self.n_heads,C//self.n_heads)\n",
              "                    view_19: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 3, 8, 32]\" = torch.ops.aten.view.default(linear_4, [floordiv_391, 64, 3, 8, 32]);  linear_4 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:43 in forward, code: q, k, v=qkv.permute(2,0,3,1,4).contiguous()\n",
              "                    permute_8: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.permute.default(view_19, [2, 0, 3, 1, 4]);  view_19 = None\n",
              "                    clone_16: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None\n",
              "                    unbind_1 = torch.ops.aten.unbind.int(clone_16);  clone_16 = None\n",
              "                    getitem_3: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_1[0]\n",
              "                    getitem_4: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_1[1]\n",
              "                    getitem_5: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_1[2];  unbind_1 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:44 in forward, code: attn=(q@k.transpose(-2, -1))*self.scale\n",
              "                    transpose_1: \"f32[((s0//8))*((s53//8)), 8, 32, ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.transpose.int(getitem_4, -2, -1);  getitem_4 = None\n",
              "                    matmul_2: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.matmul.default(getitem_3, transpose_1);  getitem_3 = transpose_1 = None\n",
              "                    mul_903: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.mul.Tensor(matmul_2, 0.1767766952966369);  matmul_2 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:48 in forward, code: attn=attn.view(B_//nW, nW, self.n_heads, N, N)\n",
              "                    floordiv_393: \"Sym(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))\" = floordiv_391 // mul_846\n",
              "                    view_20: \"f32[((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))), ((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.view.default(mul_903, [floordiv_393, mul_846, 8, 64, 64]);  mul_903 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:49 in forward, code: attn=attn+mask.unsqueeze(1).unsqueeze(0)\n",
              "                    unsqueeze_2: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 1, 64, 64]\" = torch.ops.aten.unsqueeze.default(masked_fill_1, 1);  masked_fill_1 = None\n",
              "                    unsqueeze_3: \"f32[1, ((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 1, 64, 64]\" = torch.ops.aten.unsqueeze.default(unsqueeze_2, 0);  unsqueeze_2 = None\n",
              "                    add_498: \"f32[1, ((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.add.Tensor(view_20, unsqueeze_3);  view_20 = unsqueeze_3 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:50 in forward, code: attn=attn.view(B_,self.n_heads,N,N)\n",
              "                    view_21: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.view.default(add_498, [floordiv_391, 8, 64, 64]);  add_498 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:51 in forward, code: attn=attn.softmax(dim=-1)\n",
              "                    softmax_1: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.softmax.int(view_21, -1);  view_21 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:52 in forward, code: attn=(attn@v).permute(0,2,1,3).contiguous().view(B_, N, C)\n",
              "                    matmul_3: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.matmul.default(softmax_1, getitem_5);  softmax_1 = getitem_5 = None\n",
              "                    permute_9: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.permute.default(matmul_3, [0, 2, 1, 3]);  matmul_3 = None\n",
              "                    clone_17: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None\n",
              "                    view_22: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(clone_17, [floordiv_391, 64, 256]);  clone_17 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_5: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.linear.default(view_22, p_layers_0_blocks_1_attn_proj_weight, p_layers_0_blocks_1_attn_proj_bias);  view_22 = p_layers_0_blocks_1_attn_proj_weight = p_layers_0_blocks_1_attn_proj_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:103 in forward, code: x_attn=x_attn.view(-1,self.window_size, self.window_size, C)\n",
              "                    view_23: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(linear_5, [-1, 8, 8, 256]);  linear_5 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:104 in forward, code: x=window_reverse(x_attn, self.window_size, H, W)   # (B, H, W ,C)\n",
              "                    view_24: \"f32[1, (s53//8), (s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(view_23, [1, floordiv_388, floordiv_389, 8, 8, -1]);  view_23 = None\n",
              "                    permute_10: \"f32[1, (s53//8), 8, (s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.permute.default(view_24, [0, 1, 3, 2, 4, 5]);  view_24 = None\n",
              "                    clone_18: \"f32[1, (s53//8), 8, (s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.clone.default(permute_10, memory_format = torch.contiguous_format);  permute_10 = None\n",
              "                    view_25: \"f32[1, 8*((s53//8)), ((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(clone_18, [1, sym_size_int_318, sym_size_int_319, -1]);  clone_18 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:107 in forward, code: x=torch.roll(x, shifts=(self.shift_size, self.shift_size),dims=(1,2))\n",
              "                    roll_1: \"f32[1, 8*((s53//8)), ((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.roll.default(view_25, [4, 4], [1, 2]);  view_25 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:109 in forward, code: x=x.view(-1, H*W, C)\n",
              "                    view_26: \"f32[1, 8*((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(roll_1, [-1, mul_471, 256]);  roll_1 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:110 in forward, code: x=shortcut+self.drop_path1(x)\n",
              "                    add_571: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_187, view_26);  add_187 = view_26 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_3: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_571, [256], p_layers_0_blocks_1_norm2_weight, p_layers_0_blocks_1_norm2_bias);  p_layers_0_blocks_1_norm2_weight = p_layers_0_blocks_1_norm2_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_6: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.linear.default(layer_norm_3, p_layers_0_blocks_1_mlp_0_weight, p_layers_0_blocks_1_mlp_0_bias);  layer_norm_3 = p_layers_0_blocks_1_mlp_0_weight = p_layers_0_blocks_1_mlp_0_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
              "                    gelu_1: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.gelu.default(linear_6);  linear_6 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_7: \"f32[1, s0*s53, 256]\" = torch.ops.aten.linear.default(gelu_1, p_layers_0_blocks_1_mlp_2_weight, p_layers_0_blocks_1_mlp_2_bias);  gelu_1 = p_layers_0_blocks_1_mlp_2_weight = p_layers_0_blocks_1_mlp_2_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:111 in forward, code: x=x+self.drop_path2(self.mlp(self.norm2(x)))\n",
              "                    add_587: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_571, linear_7);  add_571 = linear_7 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_4: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_587, [256], p_layers_0_blocks_2_norm1_weight, p_layers_0_blocks_2_norm1_bias);  p_layers_0_blocks_2_norm1_weight = p_layers_0_blocks_2_norm1_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:92 in forward, code: x=self.norm1(x).view(x.shape[0], H, W, -1)\n",
              "                    view_27: \"f32[1, s53, s0, 256]\" = torch.ops.aten.view.default(layer_norm_4, [sym_size_int_317, sym_size_int_318, sym_size_int_319, -1]);  layer_norm_4 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:97 in forward, code: x_windows=window_partition(x, self.window_size)\n",
              "                    view_28: \"f32[1, (s53//8), (s53//((s53//8))), (s0//8), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(view_27, [sym_size_int_317, floordiv_388, 8, floordiv_389, 8, -1]);  view_27 = None\n",
              "                    permute_11: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.permute.default(view_28, [0, 1, 3, 2, 4, 5]);  view_28 = None\n",
              "                    clone_19: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.clone.default(permute_11, memory_format = torch.contiguous_format);  permute_11 = None\n",
              "                    view_29: \"f32[((s0//8))*((s53//8)), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(clone_19, [-1, 8, 8, 256]);  clone_19 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:98 in forward, code: x_windows=x_windows.view(-1, self.window_size*self.window_size, C)    #(B_, N, C)\n",
              "                    view_30: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(view_29, [-1, 64, 256]);  view_29 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_8: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 768]\" = torch.ops.aten.linear.default(view_30, p_layers_0_blocks_2_attn_qkv_weight, p_layers_0_blocks_2_attn_qkv_bias);  view_30 = p_layers_0_blocks_2_attn_qkv_weight = p_layers_0_blocks_2_attn_qkv_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:42 in forward, code: qkv=self.qkv(x).view(B_, N, 3, self.n_heads,C//self.n_heads)\n",
              "                    view_31: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 3, 8, 32]\" = torch.ops.aten.view.default(linear_8, [floordiv_391, 64, 3, 8, 32]);  linear_8 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:43 in forward, code: q, k, v=qkv.permute(2,0,3,1,4).contiguous()\n",
              "                    permute_12: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.permute.default(view_31, [2, 0, 3, 1, 4]);  view_31 = None\n",
              "                    clone_20: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.clone.default(permute_12, memory_format = torch.contiguous_format);  permute_12 = None\n",
              "                    unbind_2 = torch.ops.aten.unbind.int(clone_20);  clone_20 = None\n",
              "                    getitem_6: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_2[0]\n",
              "                    getitem_7: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_2[1]\n",
              "                    getitem_8: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_2[2];  unbind_2 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:44 in forward, code: attn=(q@k.transpose(-2, -1))*self.scale\n",
              "                    transpose_2: \"f32[((s0//8))*((s53//8)), 8, 32, ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.transpose.int(getitem_7, -2, -1);  getitem_7 = None\n",
              "                    matmul_4: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.matmul.default(getitem_6, transpose_2);  getitem_6 = transpose_2 = None\n",
              "                    mul_1100: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.mul.Tensor(matmul_4, 0.1767766952966369);  matmul_4 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:51 in forward, code: attn=attn.softmax(dim=-1)\n",
              "                    softmax_2: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.softmax.int(mul_1100, -1);  mul_1100 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:52 in forward, code: attn=(attn@v).permute(0,2,1,3).contiguous().view(B_, N, C)\n",
              "                    matmul_5: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.matmul.default(softmax_2, getitem_8);  softmax_2 = getitem_8 = None\n",
              "                    permute_13: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.permute.default(matmul_5, [0, 2, 1, 3]);  matmul_5 = None\n",
              "                    clone_21: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.clone.default(permute_13, memory_format = torch.contiguous_format);  permute_13 = None\n",
              "                    view_32: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(clone_21, [floordiv_391, 64, 256]);  clone_21 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_9: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.linear.default(view_32, p_layers_0_blocks_2_attn_proj_weight, p_layers_0_blocks_2_attn_proj_bias);  view_32 = p_layers_0_blocks_2_attn_proj_weight = p_layers_0_blocks_2_attn_proj_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:103 in forward, code: x_attn=x_attn.view(-1,self.window_size, self.window_size, C)\n",
              "                    view_33: \"f32[((s0//8))*((s53//8)), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(linear_9, [-1, 8, 8, 256]);  linear_9 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:104 in forward, code: x=window_reverse(x_attn, self.window_size, H, W)   # (B, H, W ,C)\n",
              "                    view_34: \"f32[1, (s53//8), (s0//8), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(view_33, [1, floordiv_388, floordiv_389, 8, 8, -1]);  view_33 = None\n",
              "                    permute_14: \"f32[1, (s53//8), 8, (s0//8), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.permute.default(view_34, [0, 1, 3, 2, 4, 5]);  view_34 = None\n",
              "                    clone_22: \"f32[1, (s53//8), 8, (s0//8), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.clone.default(permute_14, memory_format = torch.contiguous_format);  permute_14 = None\n",
              "                    view_35: \"f32[1, 8*((s53//8)), ((s0//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(clone_22, [1, sym_size_int_318, sym_size_int_319, -1]);  clone_22 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:109 in forward, code: x=x.view(-1, H*W, C)\n",
              "                    view_36: \"f32[1, 8*((s0//8))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(view_35, [-1, mul_471, 256]);  view_35 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:110 in forward, code: x=shortcut+self.drop_path1(x)\n",
              "                    add_734: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_587, view_36);  add_587 = view_36 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_5: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_734, [256], p_layers_0_blocks_2_norm2_weight, p_layers_0_blocks_2_norm2_bias);  p_layers_0_blocks_2_norm2_weight = p_layers_0_blocks_2_norm2_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_10: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.linear.default(layer_norm_5, p_layers_0_blocks_2_mlp_0_weight, p_layers_0_blocks_2_mlp_0_bias);  layer_norm_5 = p_layers_0_blocks_2_mlp_0_weight = p_layers_0_blocks_2_mlp_0_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
              "                    gelu_2: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.gelu.default(linear_10);  linear_10 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_11: \"f32[1, s0*s53, 256]\" = torch.ops.aten.linear.default(gelu_2, p_layers_0_blocks_2_mlp_2_weight, p_layers_0_blocks_2_mlp_2_bias);  gelu_2 = p_layers_0_blocks_2_mlp_2_weight = p_layers_0_blocks_2_mlp_2_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:111 in forward, code: x=x+self.drop_path2(self.mlp(self.norm2(x)))\n",
              "                    add_750: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_734, linear_11);  add_734 = linear_11 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_6: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_750, [256], p_layers_0_blocks_3_norm1_weight, p_layers_0_blocks_3_norm1_bias);  p_layers_0_blocks_3_norm1_weight = p_layers_0_blocks_3_norm1_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:92 in forward, code: x=self.norm1(x).view(x.shape[0], H, W, -1)\n",
              "                    view_37: \"f32[1, s53, s0, 256]\" = torch.ops.aten.view.default(layer_norm_6, [sym_size_int_317, sym_size_int_318, sym_size_int_319, -1]);  layer_norm_6 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:95 in forward, code: x=torch.roll(x, shifts=(-self.shift_size, -self.shift_size),dims=(1,2))\n",
              "                    roll_2: \"f32[1, s53, s0, 256]\" = torch.ops.aten.roll.default(view_37, [-4, -4], [1, 2]);  view_37 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:97 in forward, code: x_windows=window_partition(x, self.window_size)\n",
              "                    view_38: \"f32[1, (s53//8), (s53//((s53//8))), (s0//8), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(roll_2, [sym_size_int_317, floordiv_388, 8, floordiv_389, 8, -1]);  roll_2 = None\n",
              "                    permute_15: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.permute.default(view_38, [0, 1, 3, 2, 4, 5]);  view_38 = None\n",
              "                    clone_23: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.clone.default(permute_15, memory_format = torch.contiguous_format);  permute_15 = None\n",
              "                    view_39: \"f32[((s0//8))*((s53//8)), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(clone_23, [-1, 8, 8, 256]);  clone_23 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:98 in forward, code: x_windows=x_windows.view(-1, self.window_size*self.window_size, C)    #(B_, N, C)\n",
              "                    view_40: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(view_39, [-1, 64, 256]);  view_39 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:100 in forward, code: attn_mask=get_attn_mask(self.window_size, self.shift_size, H, W, x.device) if self.shift_size>0 else None\n",
              "                    zeros_1: \"f32[1, s53, s0, 1]\" = torch.ops.aten.zeros.default([1, sym_size_int_318, sym_size_int_319, 1], device = device(type='cpu'), pin_memory = False)\n",
              "                    clone_24: \"f32[]\" = torch.ops.aten.clone.default(c_layers_0_blocks_3_lifted_tensor_9);  c_layers_0_blocks_3_lifted_tensor_9 = None\n",
              "                    slice_62: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(zeros_1, 1, 0, -8)\n",
              "                    slice_63: \"f32[1, s53 - 8, s0 - 8, 1]\" = torch.ops.aten.slice.Tensor(slice_62, 2, 0, -8);  slice_62 = None\n",
              "                    fill_9: \"f32[1, s53 - 8, s0 - 8, 1]\" = torch.ops.aten.fill.Tensor(slice_63, clone_24);  slice_63 = clone_24 = None\n",
              "                    slice_64: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(zeros_1, 1, 0, -8)\n",
              "                    slice_scatter_18: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_64, fill_9, 2, 0, -8);  slice_64 = fill_9 = None\n",
              "                    slice_scatter_19: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(zeros_1, slice_scatter_18, 1, 0, -8);  zeros_1 = slice_scatter_18 = None\n",
              "                    clone_25: \"f32[]\" = torch.ops.aten.clone.default(c_layers_0_blocks_3_lifted_tensor_10);  c_layers_0_blocks_3_lifted_tensor_10 = None\n",
              "                    slice_69: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_19, 1, 0, -8)\n",
              "                    slice_70: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_69, 2, -8, -4);  slice_69 = None\n",
              "                    fill_10: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_70, clone_25);  slice_70 = clone_25 = None\n",
              "                    slice_71: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_19, 1, 0, -8)\n",
              "                    slice_scatter_20: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_71, fill_10, 2, -8, -4);  slice_71 = fill_10 = None\n",
              "                    slice_scatter_21: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_19, slice_scatter_20, 1, 0, -8);  slice_scatter_19 = slice_scatter_20 = None\n",
              "                    clone_26: \"f32[]\" = torch.ops.aten.clone.default(c_layers_0_blocks_3_lifted_tensor_11);  c_layers_0_blocks_3_lifted_tensor_11 = None\n",
              "                    slice_76: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_21, 1, 0, -8)\n",
              "                    slice_77: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_76, 2, -4, 9223372036854775807);  slice_76 = None\n",
              "                    fill_11: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_77, clone_26);  slice_77 = clone_26 = None\n",
              "                    slice_78: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_21, 1, 0, -8)\n",
              "                    slice_scatter_22: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_78, fill_11, 2, -4, 9223372036854775807);  slice_78 = fill_11 = None\n",
              "                    slice_scatter_23: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_21, slice_scatter_22, 1, 0, -8);  slice_scatter_21 = slice_scatter_22 = None\n",
              "                    clone_27: \"f32[]\" = torch.ops.aten.clone.default(c_layers_0_blocks_3_lifted_tensor_12);  c_layers_0_blocks_3_lifted_tensor_12 = None\n",
              "                    slice_83: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_23, 1, -8, -4)\n",
              "                    slice_84: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.slice.Tensor(slice_83, 2, 0, -8);  slice_83 = None\n",
              "                    fill_12: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.fill.Tensor(slice_84, clone_27);  slice_84 = clone_27 = None\n",
              "                    slice_85: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_23, 1, -8, -4)\n",
              "                    slice_scatter_24: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_85, fill_12, 2, 0, -8);  slice_85 = fill_12 = None\n",
              "                    slice_scatter_25: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_23, slice_scatter_24, 1, -8, -4);  slice_scatter_23 = slice_scatter_24 = None\n",
              "                    clone_28: \"f32[]\" = torch.ops.aten.clone.default(c_layers_0_blocks_3_lifted_tensor_13);  c_layers_0_blocks_3_lifted_tensor_13 = None\n",
              "                    slice_90: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_25, 1, -8, -4)\n",
              "                    slice_91: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_90, 2, -8, -4);  slice_90 = None\n",
              "                    fill_13: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_91, clone_28);  slice_91 = clone_28 = None\n",
              "                    slice_92: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_25, 1, -8, -4)\n",
              "                    slice_scatter_26: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_92, fill_13, 2, -8, -4);  slice_92 = fill_13 = None\n",
              "                    slice_scatter_27: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_25, slice_scatter_26, 1, -8, -4);  slice_scatter_25 = slice_scatter_26 = None\n",
              "                    clone_29: \"f32[]\" = torch.ops.aten.clone.default(c_layers_0_blocks_3_lifted_tensor_14);  c_layers_0_blocks_3_lifted_tensor_14 = None\n",
              "                    slice_97: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_27, 1, -8, -4)\n",
              "                    slice_98: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_97, 2, -4, 9223372036854775807);  slice_97 = None\n",
              "                    fill_14: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_98, clone_29);  slice_98 = clone_29 = None\n",
              "                    slice_99: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_27, 1, -8, -4)\n",
              "                    slice_scatter_28: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_99, fill_14, 2, -4, 9223372036854775807);  slice_99 = fill_14 = None\n",
              "                    slice_scatter_29: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_27, slice_scatter_28, 1, -8, -4);  slice_scatter_27 = slice_scatter_28 = None\n",
              "                    clone_30: \"f32[]\" = torch.ops.aten.clone.default(c_layers_0_blocks_3_lifted_tensor_15);  c_layers_0_blocks_3_lifted_tensor_15 = None\n",
              "                    slice_104: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_29, 1, -4, 9223372036854775807)\n",
              "                    slice_105: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.slice.Tensor(slice_104, 2, 0, -8);  slice_104 = None\n",
              "                    fill_15: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.fill.Tensor(slice_105, clone_30);  slice_105 = clone_30 = None\n",
              "                    slice_106: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_29, 1, -4, 9223372036854775807)\n",
              "                    slice_scatter_30: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_106, fill_15, 2, 0, -8);  slice_106 = fill_15 = None\n",
              "                    slice_scatter_31: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_29, slice_scatter_30, 1, -4, 9223372036854775807);  slice_scatter_29 = slice_scatter_30 = None\n",
              "                    clone_31: \"f32[]\" = torch.ops.aten.clone.default(c_layers_0_blocks_3_lifted_tensor_16);  c_layers_0_blocks_3_lifted_tensor_16 = None\n",
              "                    slice_111: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_31, 1, -4, 9223372036854775807)\n",
              "                    slice_112: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_111, 2, -8, -4);  slice_111 = None\n",
              "                    fill_16: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_112, clone_31);  slice_112 = clone_31 = None\n",
              "                    slice_113: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_31, 1, -4, 9223372036854775807)\n",
              "                    slice_scatter_32: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_113, fill_16, 2, -8, -4);  slice_113 = fill_16 = None\n",
              "                    slice_scatter_33: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_31, slice_scatter_32, 1, -4, 9223372036854775807);  slice_scatter_31 = slice_scatter_32 = None\n",
              "                    clone_32: \"f32[]\" = torch.ops.aten.clone.default(c_layers_0_blocks_3_lifted_tensor_17);  c_layers_0_blocks_3_lifted_tensor_17 = None\n",
              "                    slice_118: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_33, 1, -4, 9223372036854775807)\n",
              "                    slice_119: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_118, 2, -4, 9223372036854775807);  slice_118 = None\n",
              "                    fill_17: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_119, clone_32);  slice_119 = clone_32 = None\n",
              "                    slice_120: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_33, 1, -4, 9223372036854775807)\n",
              "                    slice_scatter_34: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_120, fill_17, 2, -4, 9223372036854775807);  slice_120 = fill_17 = None\n",
              "                    slice_scatter_35: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_33, slice_scatter_34, 1, -4, 9223372036854775807);  slice_scatter_33 = slice_scatter_34 = None\n",
              "                    view_42: \"f32[1, (s53//8), 8, (s0//8), 8, ((s0*s53)//(64*((s0//8))*((s53//8))))]\" = torch.ops.aten.view.default(slice_scatter_35, [1, floordiv_388, 8, floordiv_389, 8, -1]);  slice_scatter_35 = None\n",
              "                    permute_17: \"f32[1, (s53//8), (s0//8), 8, 8, ((s0*s53)//(64*((s0//8))*((s53//8))))]\" = torch.ops.aten.permute.default(view_42, [0, 1, 3, 2, 4, 5]);  view_42 = None\n",
              "                    clone_33: \"f32[1, (s53//8), (s0//8), 8, 8, ((s0*s53)//(64*((s0//8))*((s53//8))))]\" = torch.ops.aten.clone.default(permute_17, memory_format = torch.contiguous_format);  permute_17 = None\n",
              "                    view_43: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 8, 8, 1]\" = torch.ops.aten.view.default(clone_33, [-1, 8, 8, 1]);  clone_33 = None\n",
              "                    view_44: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64]\" = torch.ops.aten.view.default(view_43, [mul_846, -1]);  view_43 = None\n",
              "                    unsqueeze_4: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 1, 64]\" = torch.ops.aten.unsqueeze.default(view_44, 1)\n",
              "                    unsqueeze_5: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 1]\" = torch.ops.aten.unsqueeze.default(view_44, 2);  view_44 = None\n",
              "                    sub_375: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.sub.Tensor(unsqueeze_4, unsqueeze_5);  unsqueeze_4 = unsqueeze_5 = None\n",
              "                    scalar_tensor_default_2: \"f32[]\" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32)\n",
              "                    ne_60: \"b8[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.ne.Tensor(sub_375, scalar_tensor_default_2);  scalar_tensor_default_2 = None\n",
              "                    masked_fill_2: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.masked_fill.Scalar(sub_375, ne_60, -100.0);  ne_60 = None\n",
              "                    scalar_tensor_default_3: \"f32[]\" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32)\n",
              "                    eq_587: \"b8[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.eq.Tensor(sub_375, scalar_tensor_default_3);  sub_375 = scalar_tensor_default_3 = None\n",
              "                    masked_fill_3: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.masked_fill.Scalar(masked_fill_2, eq_587, 0.0);  masked_fill_2 = eq_587 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_12: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 768]\" = torch.ops.aten.linear.default(view_40, p_layers_0_blocks_3_attn_qkv_weight, p_layers_0_blocks_3_attn_qkv_bias);  view_40 = p_layers_0_blocks_3_attn_qkv_weight = p_layers_0_blocks_3_attn_qkv_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:42 in forward, code: qkv=self.qkv(x).view(B_, N, 3, self.n_heads,C//self.n_heads)\n",
              "                    view_45: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 3, 8, 32]\" = torch.ops.aten.view.default(linear_12, [floordiv_391, 64, 3, 8, 32]);  linear_12 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:43 in forward, code: q, k, v=qkv.permute(2,0,3,1,4).contiguous()\n",
              "                    permute_18: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.permute.default(view_45, [2, 0, 3, 1, 4]);  view_45 = None\n",
              "                    clone_34: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.clone.default(permute_18, memory_format = torch.contiguous_format);  permute_18 = None\n",
              "                    unbind_3 = torch.ops.aten.unbind.int(clone_34);  clone_34 = None\n",
              "                    getitem_9: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_3[0]\n",
              "                    getitem_10: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_3[1]\n",
              "                    getitem_11: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_3[2];  unbind_3 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:44 in forward, code: attn=(q@k.transpose(-2, -1))*self.scale\n",
              "                    transpose_3: \"f32[((s0//8))*((s53//8)), 8, 32, ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.transpose.int(getitem_10, -2, -1);  getitem_10 = None\n",
              "                    matmul_6: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.matmul.default(getitem_9, transpose_3);  getitem_9 = transpose_3 = None\n",
              "                    mul_1431: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.mul.Tensor(matmul_6, 0.1767766952966369);  matmul_6 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:48 in forward, code: attn=attn.view(B_//nW, nW, self.n_heads, N, N)\n",
              "                    view_46: \"f32[((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))), ((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.view.default(mul_1431, [floordiv_393, mul_846, 8, 64, 64]);  mul_1431 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:49 in forward, code: attn=attn+mask.unsqueeze(1).unsqueeze(0)\n",
              "                    unsqueeze_6: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 1, 64, 64]\" = torch.ops.aten.unsqueeze.default(masked_fill_3, 1);  masked_fill_3 = None\n",
              "                    unsqueeze_7: \"f32[1, ((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 1, 64, 64]\" = torch.ops.aten.unsqueeze.default(unsqueeze_6, 0);  unsqueeze_6 = None\n",
              "                    add_1061: \"f32[1, ((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.add.Tensor(view_46, unsqueeze_7);  view_46 = unsqueeze_7 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:50 in forward, code: attn=attn.view(B_,self.n_heads,N,N)\n",
              "                    view_47: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.view.default(add_1061, [floordiv_391, 8, 64, 64]);  add_1061 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:51 in forward, code: attn=attn.softmax(dim=-1)\n",
              "                    softmax_3: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.softmax.int(view_47, -1);  view_47 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:52 in forward, code: attn=(attn@v).permute(0,2,1,3).contiguous().view(B_, N, C)\n",
              "                    matmul_7: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.matmul.default(softmax_3, getitem_11);  softmax_3 = getitem_11 = None\n",
              "                    permute_19: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.permute.default(matmul_7, [0, 2, 1, 3]);  matmul_7 = None\n",
              "                    clone_35: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.clone.default(permute_19, memory_format = torch.contiguous_format);  permute_19 = None\n",
              "                    view_48: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(clone_35, [floordiv_391, 64, 256]);  clone_35 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_13: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.linear.default(view_48, p_layers_0_blocks_3_attn_proj_weight, p_layers_0_blocks_3_attn_proj_bias);  view_48 = p_layers_0_blocks_3_attn_proj_weight = p_layers_0_blocks_3_attn_proj_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:103 in forward, code: x_attn=x_attn.view(-1,self.window_size, self.window_size, C)\n",
              "                    view_49: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(linear_13, [-1, 8, 8, 256]);  linear_13 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:104 in forward, code: x=window_reverse(x_attn, self.window_size, H, W)   # (B, H, W ,C)\n",
              "                    view_50: \"f32[1, (s53//8), (s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(view_49, [1, floordiv_388, floordiv_389, 8, 8, -1]);  view_49 = None\n",
              "                    permute_20: \"f32[1, (s53//8), 8, (s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.permute.default(view_50, [0, 1, 3, 2, 4, 5]);  view_50 = None\n",
              "                    clone_36: \"f32[1, (s53//8), 8, (s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.clone.default(permute_20, memory_format = torch.contiguous_format);  permute_20 = None\n",
              "                    view_51: \"f32[1, 8*((s53//8)), ((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(clone_36, [1, sym_size_int_318, sym_size_int_319, -1]);  clone_36 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:107 in forward, code: x=torch.roll(x, shifts=(self.shift_size, self.shift_size),dims=(1,2))\n",
              "                    roll_3: \"f32[1, 8*((s53//8)), ((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.roll.default(view_51, [4, 4], [1, 2]);  view_51 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:109 in forward, code: x=x.view(-1, H*W, C)\n",
              "                    view_52: \"f32[1, 8*((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(roll_3, [-1, mul_471, 256]);  roll_3 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:110 in forward, code: x=shortcut+self.drop_path1(x)\n",
              "                    add_1134: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_750, view_52);  add_750 = view_52 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_7: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_1134, [256], p_layers_0_blocks_3_norm2_weight, p_layers_0_blocks_3_norm2_bias);  p_layers_0_blocks_3_norm2_weight = p_layers_0_blocks_3_norm2_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_14: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.linear.default(layer_norm_7, p_layers_0_blocks_3_mlp_0_weight, p_layers_0_blocks_3_mlp_0_bias);  layer_norm_7 = p_layers_0_blocks_3_mlp_0_weight = p_layers_0_blocks_3_mlp_0_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
              "                    gelu_3: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.gelu.default(linear_14);  linear_14 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_15: \"f32[1, s0*s53, 256]\" = torch.ops.aten.linear.default(gelu_3, p_layers_0_blocks_3_mlp_2_weight, p_layers_0_blocks_3_mlp_2_bias);  gelu_3 = p_layers_0_blocks_3_mlp_2_weight = p_layers_0_blocks_3_mlp_2_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:111 in forward, code: x=x+self.drop_path2(self.mlp(self.norm2(x)))\n",
              "                    add_1150: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_1134, linear_15);  add_1134 = linear_15 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:135 in forward, code: x=x.view(B, H, W, C).permute(0,3,1,2).contiguous()\n",
              "                    view_53: \"f32[1, s53, s0, 256]\" = torch.ops.aten.view.default(add_1150, [sym_size_int_317, sym_size_int_318, sym_size_int_319, 256]);  add_1150 = None\n",
              "                    permute_21: \"f32[1, 256, s53, s0]\" = torch.ops.aten.permute.default(view_53, [0, 3, 1, 2]);  view_53 = None\n",
              "                    clone_37: \"f32[1, 256, s53, s0]\" = torch.ops.aten.clone.default(permute_21, memory_format = torch.contiguous_format);  permute_21 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
              "                    conv2d_1: \"f32[1, 256, s53, s0]\" = torch.ops.aten.conv2d.default(clone_37, p_layers_0_conv_weight, p_layers_0_conv_bias, [1, 1], [1, 1]);  clone_37 = p_layers_0_conv_weight = p_layers_0_conv_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:137 in forward, code: x=x.permute(0,2,3,1).contiguous()\n",
              "                    permute_22: \"f32[1, s53, s0, 256]\" = torch.ops.aten.permute.default(conv2d_1, [0, 2, 3, 1]);  conv2d_1 = None\n",
              "                    clone_38: \"f32[1, s53, s0, 256]\" = torch.ops.aten.clone.default(permute_22, memory_format = torch.contiguous_format);  permute_22 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:138 in forward, code: x=shortcut+x\n",
              "                    add_1177: \"f32[1, s53, s0, 256]\" = torch.ops.aten.add.Tensor(clone, clone_38);  clone = clone_38 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:132 in forward, code: x=x.view(B,H*W,C)\n",
              "                    view_54: \"f32[1, s0*s53, 256]\" = torch.ops.aten.view.default(add_1177, [sym_size_int_317, mul_471, 256])\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_8: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(view_54, [256], p_layers_1_blocks_0_norm1_weight, p_layers_1_blocks_0_norm1_bias);  p_layers_1_blocks_0_norm1_weight = p_layers_1_blocks_0_norm1_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:92 in forward, code: x=self.norm1(x).view(x.shape[0], H, W, -1)\n",
              "                    view_55: \"f32[1, s53, s0, 256]\" = torch.ops.aten.view.default(layer_norm_8, [sym_size_int_317, sym_size_int_318, sym_size_int_319, -1]);  layer_norm_8 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:97 in forward, code: x_windows=window_partition(x, self.window_size)\n",
              "                    view_56: \"f32[1, (s53//8), (s53//((s53//8))), (s0//8), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(view_55, [sym_size_int_317, floordiv_388, 8, floordiv_389, 8, -1]);  view_55 = None\n",
              "                    permute_23: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.permute.default(view_56, [0, 1, 3, 2, 4, 5]);  view_56 = None\n",
              "                    clone_39: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.clone.default(permute_23, memory_format = torch.contiguous_format);  permute_23 = None\n",
              "                    view_57: \"f32[((s0//8))*((s53//8)), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(clone_39, [-1, 8, 8, 256]);  clone_39 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:98 in forward, code: x_windows=x_windows.view(-1, self.window_size*self.window_size, C)    #(B_, N, C)\n",
              "                    view_58: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(view_57, [-1, 64, 256]);  view_57 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_16: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 768]\" = torch.ops.aten.linear.default(view_58, p_layers_1_blocks_0_attn_qkv_weight, p_layers_1_blocks_0_attn_qkv_bias);  view_58 = p_layers_1_blocks_0_attn_qkv_weight = p_layers_1_blocks_0_attn_qkv_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:42 in forward, code: qkv=self.qkv(x).view(B_, N, 3, self.n_heads,C//self.n_heads)\n",
              "                    view_59: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 3, 8, 32]\" = torch.ops.aten.view.default(linear_16, [floordiv_391, 64, 3, 8, 32]);  linear_16 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:43 in forward, code: q, k, v=qkv.permute(2,0,3,1,4).contiguous()\n",
              "                    permute_24: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.permute.default(view_59, [2, 0, 3, 1, 4]);  view_59 = None\n",
              "                    clone_40: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.clone.default(permute_24, memory_format = torch.contiguous_format);  permute_24 = None\n",
              "                    unbind_4 = torch.ops.aten.unbind.int(clone_40);  clone_40 = None\n",
              "                    getitem_12: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_4[0]\n",
              "                    getitem_13: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_4[1]\n",
              "                    getitem_14: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_4[2];  unbind_4 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:44 in forward, code: attn=(q@k.transpose(-2, -1))*self.scale\n",
              "                    transpose_4: \"f32[((s0//8))*((s53//8)), 8, 32, ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.transpose.int(getitem_13, -2, -1);  getitem_13 = None\n",
              "                    matmul_8: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.matmul.default(getitem_12, transpose_4);  getitem_12 = transpose_4 = None\n",
              "                    mul_1667: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.mul.Tensor(matmul_8, 0.1767766952966369);  matmul_8 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:51 in forward, code: attn=attn.softmax(dim=-1)\n",
              "                    softmax_4: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.softmax.int(mul_1667, -1);  mul_1667 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:52 in forward, code: attn=(attn@v).permute(0,2,1,3).contiguous().view(B_, N, C)\n",
              "                    matmul_9: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.matmul.default(softmax_4, getitem_14);  softmax_4 = getitem_14 = None\n",
              "                    permute_25: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.permute.default(matmul_9, [0, 2, 1, 3]);  matmul_9 = None\n",
              "                    clone_41: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.clone.default(permute_25, memory_format = torch.contiguous_format);  permute_25 = None\n",
              "                    view_60: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(clone_41, [floordiv_391, 64, 256]);  clone_41 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_17: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.linear.default(view_60, p_layers_1_blocks_0_attn_proj_weight, p_layers_1_blocks_0_attn_proj_bias);  view_60 = p_layers_1_blocks_0_attn_proj_weight = p_layers_1_blocks_0_attn_proj_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:103 in forward, code: x_attn=x_attn.view(-1,self.window_size, self.window_size, C)\n",
              "                    view_61: \"f32[((s0//8))*((s53//8)), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(linear_17, [-1, 8, 8, 256]);  linear_17 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:104 in forward, code: x=window_reverse(x_attn, self.window_size, H, W)   # (B, H, W ,C)\n",
              "                    view_62: \"f32[1, (s53//8), (s0//8), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(view_61, [1, floordiv_388, floordiv_389, 8, 8, -1]);  view_61 = None\n",
              "                    permute_26: \"f32[1, (s53//8), 8, (s0//8), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.permute.default(view_62, [0, 1, 3, 2, 4, 5]);  view_62 = None\n",
              "                    clone_42: \"f32[1, (s53//8), 8, (s0//8), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.clone.default(permute_26, memory_format = torch.contiguous_format);  permute_26 = None\n",
              "                    view_63: \"f32[1, 8*((s53//8)), ((s0//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(clone_42, [1, sym_size_int_318, sym_size_int_319, -1]);  clone_42 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:109 in forward, code: x=x.view(-1, H*W, C)\n",
              "                    view_64: \"f32[1, 8*((s0//8))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(view_63, [-1, mul_471, 256]);  view_63 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:110 in forward, code: x=shortcut+self.drop_path1(x)\n",
              "                    add_1328: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(view_54, view_64);  view_54 = view_64 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_9: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_1328, [256], p_layers_1_blocks_0_norm2_weight, p_layers_1_blocks_0_norm2_bias);  p_layers_1_blocks_0_norm2_weight = p_layers_1_blocks_0_norm2_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_18: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.linear.default(layer_norm_9, p_layers_1_blocks_0_mlp_0_weight, p_layers_1_blocks_0_mlp_0_bias);  layer_norm_9 = p_layers_1_blocks_0_mlp_0_weight = p_layers_1_blocks_0_mlp_0_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
              "                    gelu_4: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.gelu.default(linear_18);  linear_18 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_19: \"f32[1, s0*s53, 256]\" = torch.ops.aten.linear.default(gelu_4, p_layers_1_blocks_0_mlp_2_weight, p_layers_1_blocks_0_mlp_2_bias);  gelu_4 = p_layers_1_blocks_0_mlp_2_weight = p_layers_1_blocks_0_mlp_2_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:111 in forward, code: x=x+self.drop_path2(self.mlp(self.norm2(x)))\n",
              "                    add_1344: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_1328, linear_19);  add_1328 = linear_19 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_10: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_1344, [256], p_layers_1_blocks_1_norm1_weight, p_layers_1_blocks_1_norm1_bias);  p_layers_1_blocks_1_norm1_weight = p_layers_1_blocks_1_norm1_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:92 in forward, code: x=self.norm1(x).view(x.shape[0], H, W, -1)\n",
              "                    view_65: \"f32[1, s53, s0, 256]\" = torch.ops.aten.view.default(layer_norm_10, [sym_size_int_317, sym_size_int_318, sym_size_int_319, -1]);  layer_norm_10 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:95 in forward, code: x=torch.roll(x, shifts=(-self.shift_size, -self.shift_size),dims=(1,2))\n",
              "                    roll_4: \"f32[1, s53, s0, 256]\" = torch.ops.aten.roll.default(view_65, [-4, -4], [1, 2]);  view_65 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:97 in forward, code: x_windows=window_partition(x, self.window_size)\n",
              "                    view_66: \"f32[1, (s53//8), (s53//((s53//8))), (s0//8), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(roll_4, [sym_size_int_317, floordiv_388, 8, floordiv_389, 8, -1]);  roll_4 = None\n",
              "                    permute_27: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.permute.default(view_66, [0, 1, 3, 2, 4, 5]);  view_66 = None\n",
              "                    clone_43: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.clone.default(permute_27, memory_format = torch.contiguous_format);  permute_27 = None\n",
              "                    view_67: \"f32[((s0//8))*((s53//8)), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(clone_43, [-1, 8, 8, 256]);  clone_43 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:98 in forward, code: x_windows=x_windows.view(-1, self.window_size*self.window_size, C)    #(B_, N, C)\n",
              "                    view_68: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(view_67, [-1, 64, 256]);  view_67 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:100 in forward, code: attn_mask=get_attn_mask(self.window_size, self.shift_size, H, W, x.device) if self.shift_size>0 else None\n",
              "                    zeros_2: \"f32[1, s53, s0, 1]\" = torch.ops.aten.zeros.default([1, sym_size_int_318, sym_size_int_319, 1], device = device(type='cpu'), pin_memory = False)\n",
              "                    clone_44: \"f32[]\" = torch.ops.aten.clone.default(c_layers_1_blocks_1_lifted_tensor_18);  c_layers_1_blocks_1_lifted_tensor_18 = None\n",
              "                    slice_123: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(zeros_2, 1, 0, -8)\n",
              "                    slice_124: \"f32[1, s53 - 8, s0 - 8, 1]\" = torch.ops.aten.slice.Tensor(slice_123, 2, 0, -8);  slice_123 = None\n",
              "                    fill_18: \"f32[1, s53 - 8, s0 - 8, 1]\" = torch.ops.aten.fill.Tensor(slice_124, clone_44);  slice_124 = clone_44 = None\n",
              "                    slice_125: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(zeros_2, 1, 0, -8)\n",
              "                    slice_scatter_36: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_125, fill_18, 2, 0, -8);  slice_125 = fill_18 = None\n",
              "                    slice_scatter_37: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(zeros_2, slice_scatter_36, 1, 0, -8);  zeros_2 = slice_scatter_36 = None\n",
              "                    clone_45: \"f32[]\" = torch.ops.aten.clone.default(c_layers_1_blocks_1_lifted_tensor_19);  c_layers_1_blocks_1_lifted_tensor_19 = None\n",
              "                    slice_130: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_37, 1, 0, -8)\n",
              "                    slice_131: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_130, 2, -8, -4);  slice_130 = None\n",
              "                    fill_19: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_131, clone_45);  slice_131 = clone_45 = None\n",
              "                    slice_132: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_37, 1, 0, -8)\n",
              "                    slice_scatter_38: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_132, fill_19, 2, -8, -4);  slice_132 = fill_19 = None\n",
              "                    slice_scatter_39: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_37, slice_scatter_38, 1, 0, -8);  slice_scatter_37 = slice_scatter_38 = None\n",
              "                    clone_46: \"f32[]\" = torch.ops.aten.clone.default(c_layers_1_blocks_1_lifted_tensor_20);  c_layers_1_blocks_1_lifted_tensor_20 = None\n",
              "                    slice_137: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_39, 1, 0, -8)\n",
              "                    slice_138: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_137, 2, -4, 9223372036854775807);  slice_137 = None\n",
              "                    fill_20: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_138, clone_46);  slice_138 = clone_46 = None\n",
              "                    slice_139: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_39, 1, 0, -8)\n",
              "                    slice_scatter_40: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_139, fill_20, 2, -4, 9223372036854775807);  slice_139 = fill_20 = None\n",
              "                    slice_scatter_41: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_39, slice_scatter_40, 1, 0, -8);  slice_scatter_39 = slice_scatter_40 = None\n",
              "                    clone_47: \"f32[]\" = torch.ops.aten.clone.default(c_layers_1_blocks_1_lifted_tensor_21);  c_layers_1_blocks_1_lifted_tensor_21 = None\n",
              "                    slice_144: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_41, 1, -8, -4)\n",
              "                    slice_145: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.slice.Tensor(slice_144, 2, 0, -8);  slice_144 = None\n",
              "                    fill_21: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.fill.Tensor(slice_145, clone_47);  slice_145 = clone_47 = None\n",
              "                    slice_146: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_41, 1, -8, -4)\n",
              "                    slice_scatter_42: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_146, fill_21, 2, 0, -8);  slice_146 = fill_21 = None\n",
              "                    slice_scatter_43: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_41, slice_scatter_42, 1, -8, -4);  slice_scatter_41 = slice_scatter_42 = None\n",
              "                    clone_48: \"f32[]\" = torch.ops.aten.clone.default(c_layers_1_blocks_1_lifted_tensor_22);  c_layers_1_blocks_1_lifted_tensor_22 = None\n",
              "                    slice_151: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_43, 1, -8, -4)\n",
              "                    slice_152: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_151, 2, -8, -4);  slice_151 = None\n",
              "                    fill_22: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_152, clone_48);  slice_152 = clone_48 = None\n",
              "                    slice_153: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_43, 1, -8, -4)\n",
              "                    slice_scatter_44: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_153, fill_22, 2, -8, -4);  slice_153 = fill_22 = None\n",
              "                    slice_scatter_45: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_43, slice_scatter_44, 1, -8, -4);  slice_scatter_43 = slice_scatter_44 = None\n",
              "                    clone_49: \"f32[]\" = torch.ops.aten.clone.default(c_layers_1_blocks_1_lifted_tensor_23);  c_layers_1_blocks_1_lifted_tensor_23 = None\n",
              "                    slice_158: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_45, 1, -8, -4)\n",
              "                    slice_159: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_158, 2, -4, 9223372036854775807);  slice_158 = None\n",
              "                    fill_23: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_159, clone_49);  slice_159 = clone_49 = None\n",
              "                    slice_160: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_45, 1, -8, -4)\n",
              "                    slice_scatter_46: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_160, fill_23, 2, -4, 9223372036854775807);  slice_160 = fill_23 = None\n",
              "                    slice_scatter_47: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_45, slice_scatter_46, 1, -8, -4);  slice_scatter_45 = slice_scatter_46 = None\n",
              "                    clone_50: \"f32[]\" = torch.ops.aten.clone.default(c_layers_1_blocks_1_lifted_tensor_24);  c_layers_1_blocks_1_lifted_tensor_24 = None\n",
              "                    slice_165: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_47, 1, -4, 9223372036854775807)\n",
              "                    slice_166: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.slice.Tensor(slice_165, 2, 0, -8);  slice_165 = None\n",
              "                    fill_24: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.fill.Tensor(slice_166, clone_50);  slice_166 = clone_50 = None\n",
              "                    slice_167: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_47, 1, -4, 9223372036854775807)\n",
              "                    slice_scatter_48: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_167, fill_24, 2, 0, -8);  slice_167 = fill_24 = None\n",
              "                    slice_scatter_49: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_47, slice_scatter_48, 1, -4, 9223372036854775807);  slice_scatter_47 = slice_scatter_48 = None\n",
              "                    clone_51: \"f32[]\" = torch.ops.aten.clone.default(c_layers_1_blocks_1_lifted_tensor_25);  c_layers_1_blocks_1_lifted_tensor_25 = None\n",
              "                    slice_172: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_49, 1, -4, 9223372036854775807)\n",
              "                    slice_173: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_172, 2, -8, -4);  slice_172 = None\n",
              "                    fill_25: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_173, clone_51);  slice_173 = clone_51 = None\n",
              "                    slice_174: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_49, 1, -4, 9223372036854775807)\n",
              "                    slice_scatter_50: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_174, fill_25, 2, -8, -4);  slice_174 = fill_25 = None\n",
              "                    slice_scatter_51: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_49, slice_scatter_50, 1, -4, 9223372036854775807);  slice_scatter_49 = slice_scatter_50 = None\n",
              "                    clone_52: \"f32[]\" = torch.ops.aten.clone.default(c_layers_1_blocks_1_lifted_tensor_26);  c_layers_1_blocks_1_lifted_tensor_26 = None\n",
              "                    slice_179: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_51, 1, -4, 9223372036854775807)\n",
              "                    slice_180: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_179, 2, -4, 9223372036854775807);  slice_179 = None\n",
              "                    fill_26: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_180, clone_52);  slice_180 = clone_52 = None\n",
              "                    slice_181: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_51, 1, -4, 9223372036854775807)\n",
              "                    slice_scatter_52: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_181, fill_26, 2, -4, 9223372036854775807);  slice_181 = fill_26 = None\n",
              "                    slice_scatter_53: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_51, slice_scatter_52, 1, -4, 9223372036854775807);  slice_scatter_51 = slice_scatter_52 = None\n",
              "                    view_70: \"f32[1, (s53//8), 8, (s0//8), 8, ((s0*s53)//(64*((s0//8))*((s53//8))))]\" = torch.ops.aten.view.default(slice_scatter_53, [1, floordiv_388, 8, floordiv_389, 8, -1]);  slice_scatter_53 = None\n",
              "                    permute_29: \"f32[1, (s53//8), (s0//8), 8, 8, ((s0*s53)//(64*((s0//8))*((s53//8))))]\" = torch.ops.aten.permute.default(view_70, [0, 1, 3, 2, 4, 5]);  view_70 = None\n",
              "                    clone_53: \"f32[1, (s53//8), (s0//8), 8, 8, ((s0*s53)//(64*((s0//8))*((s53//8))))]\" = torch.ops.aten.clone.default(permute_29, memory_format = torch.contiguous_format);  permute_29 = None\n",
              "                    view_71: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 8, 8, 1]\" = torch.ops.aten.view.default(clone_53, [-1, 8, 8, 1]);  clone_53 = None\n",
              "                    view_72: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64]\" = torch.ops.aten.view.default(view_71, [mul_846, -1]);  view_71 = None\n",
              "                    unsqueeze_8: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 1, 64]\" = torch.ops.aten.unsqueeze.default(view_72, 1)\n",
              "                    unsqueeze_9: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 1]\" = torch.ops.aten.unsqueeze.default(view_72, 2);  view_72 = None\n",
              "                    sub_607: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.sub.Tensor(unsqueeze_8, unsqueeze_9);  unsqueeze_8 = unsqueeze_9 = None\n",
              "                    scalar_tensor_default_4: \"f32[]\" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32)\n",
              "                    ne_87: \"b8[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.ne.Tensor(sub_607, scalar_tensor_default_4);  scalar_tensor_default_4 = None\n",
              "                    masked_fill_4: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.masked_fill.Scalar(sub_607, ne_87, -100.0);  ne_87 = None\n",
              "                    scalar_tensor_default_5: \"f32[]\" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32)\n",
              "                    eq_942: \"b8[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.eq.Tensor(sub_607, scalar_tensor_default_5);  sub_607 = scalar_tensor_default_5 = None\n",
              "                    masked_fill_5: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.masked_fill.Scalar(masked_fill_4, eq_942, 0.0);  masked_fill_4 = eq_942 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_20: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 768]\" = torch.ops.aten.linear.default(view_68, p_layers_1_blocks_1_attn_qkv_weight, p_layers_1_blocks_1_attn_qkv_bias);  view_68 = p_layers_1_blocks_1_attn_qkv_weight = p_layers_1_blocks_1_attn_qkv_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:42 in forward, code: qkv=self.qkv(x).view(B_, N, 3, self.n_heads,C//self.n_heads)\n",
              "                    view_73: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 3, 8, 32]\" = torch.ops.aten.view.default(linear_20, [floordiv_391, 64, 3, 8, 32]);  linear_20 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:43 in forward, code: q, k, v=qkv.permute(2,0,3,1,4).contiguous()\n",
              "                    permute_30: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.permute.default(view_73, [2, 0, 3, 1, 4]);  view_73 = None\n",
              "                    clone_54: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.clone.default(permute_30, memory_format = torch.contiguous_format);  permute_30 = None\n",
              "                    unbind_5 = torch.ops.aten.unbind.int(clone_54);  clone_54 = None\n",
              "                    getitem_15: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_5[0]\n",
              "                    getitem_16: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_5[1]\n",
              "                    getitem_17: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_5[2];  unbind_5 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:44 in forward, code: attn=(q@k.transpose(-2, -1))*self.scale\n",
              "                    transpose_5: \"f32[((s0//8))*((s53//8)), 8, 32, ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.transpose.int(getitem_16, -2, -1);  getitem_16 = None\n",
              "                    matmul_10: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.matmul.default(getitem_15, transpose_5);  getitem_15 = transpose_5 = None\n",
              "                    mul_1998: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.mul.Tensor(matmul_10, 0.1767766952966369);  matmul_10 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:48 in forward, code: attn=attn.view(B_//nW, nW, self.n_heads, N, N)\n",
              "                    view_74: \"f32[((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))), ((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.view.default(mul_1998, [floordiv_393, mul_846, 8, 64, 64]);  mul_1998 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:49 in forward, code: attn=attn+mask.unsqueeze(1).unsqueeze(0)\n",
              "                    unsqueeze_10: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 1, 64, 64]\" = torch.ops.aten.unsqueeze.default(masked_fill_5, 1);  masked_fill_5 = None\n",
              "                    unsqueeze_11: \"f32[1, ((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 1, 64, 64]\" = torch.ops.aten.unsqueeze.default(unsqueeze_10, 0);  unsqueeze_10 = None\n",
              "                    add_1655: \"f32[1, ((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.add.Tensor(view_74, unsqueeze_11);  view_74 = unsqueeze_11 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:50 in forward, code: attn=attn.view(B_,self.n_heads,N,N)\n",
              "                    view_75: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.view.default(add_1655, [floordiv_391, 8, 64, 64]);  add_1655 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:51 in forward, code: attn=attn.softmax(dim=-1)\n",
              "                    softmax_5: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.softmax.int(view_75, -1);  view_75 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:52 in forward, code: attn=(attn@v).permute(0,2,1,3).contiguous().view(B_, N, C)\n",
              "                    matmul_11: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.matmul.default(softmax_5, getitem_17);  softmax_5 = getitem_17 = None\n",
              "                    permute_31: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.permute.default(matmul_11, [0, 2, 1, 3]);  matmul_11 = None\n",
              "                    clone_55: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.clone.default(permute_31, memory_format = torch.contiguous_format);  permute_31 = None\n",
              "                    view_76: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(clone_55, [floordiv_391, 64, 256]);  clone_55 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_21: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.linear.default(view_76, p_layers_1_blocks_1_attn_proj_weight, p_layers_1_blocks_1_attn_proj_bias);  view_76 = p_layers_1_blocks_1_attn_proj_weight = p_layers_1_blocks_1_attn_proj_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:103 in forward, code: x_attn=x_attn.view(-1,self.window_size, self.window_size, C)\n",
              "                    view_77: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(linear_21, [-1, 8, 8, 256]);  linear_21 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:104 in forward, code: x=window_reverse(x_attn, self.window_size, H, W)   # (B, H, W ,C)\n",
              "                    view_78: \"f32[1, (s53//8), (s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(view_77, [1, floordiv_388, floordiv_389, 8, 8, -1]);  view_77 = None\n",
              "                    permute_32: \"f32[1, (s53//8), 8, (s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.permute.default(view_78, [0, 1, 3, 2, 4, 5]);  view_78 = None\n",
              "                    clone_56: \"f32[1, (s53//8), 8, (s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.clone.default(permute_32, memory_format = torch.contiguous_format);  permute_32 = None\n",
              "                    view_79: \"f32[1, 8*((s53//8)), ((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(clone_56, [1, sym_size_int_318, sym_size_int_319, -1]);  clone_56 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:107 in forward, code: x=torch.roll(x, shifts=(self.shift_size, self.shift_size),dims=(1,2))\n",
              "                    roll_5: \"f32[1, 8*((s53//8)), ((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.roll.default(view_79, [4, 4], [1, 2]);  view_79 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:109 in forward, code: x=x.view(-1, H*W, C)\n",
              "                    view_80: \"f32[1, 8*((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(roll_5, [-1, mul_471, 256]);  roll_5 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:110 in forward, code: x=shortcut+self.drop_path1(x)\n",
              "                    add_1728: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_1344, view_80);  add_1344 = view_80 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_11: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_1728, [256], p_layers_1_blocks_1_norm2_weight, p_layers_1_blocks_1_norm2_bias);  p_layers_1_blocks_1_norm2_weight = p_layers_1_blocks_1_norm2_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_22: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.linear.default(layer_norm_11, p_layers_1_blocks_1_mlp_0_weight, p_layers_1_blocks_1_mlp_0_bias);  layer_norm_11 = p_layers_1_blocks_1_mlp_0_weight = p_layers_1_blocks_1_mlp_0_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
              "                    gelu_5: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.gelu.default(linear_22);  linear_22 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_23: \"f32[1, s0*s53, 256]\" = torch.ops.aten.linear.default(gelu_5, p_layers_1_blocks_1_mlp_2_weight, p_layers_1_blocks_1_mlp_2_bias);  gelu_5 = p_layers_1_blocks_1_mlp_2_weight = p_layers_1_blocks_1_mlp_2_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:111 in forward, code: x=x+self.drop_path2(self.mlp(self.norm2(x)))\n",
              "                    add_1744: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_1728, linear_23);  add_1728 = linear_23 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_12: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_1744, [256], p_layers_1_blocks_2_norm1_weight, p_layers_1_blocks_2_norm1_bias);  p_layers_1_blocks_2_norm1_weight = p_layers_1_blocks_2_norm1_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:92 in forward, code: x=self.norm1(x).view(x.shape[0], H, W, -1)\n",
              "                    view_81: \"f32[1, s53, s0, 256]\" = torch.ops.aten.view.default(layer_norm_12, [sym_size_int_317, sym_size_int_318, sym_size_int_319, -1]);  layer_norm_12 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:97 in forward, code: x_windows=window_partition(x, self.window_size)\n",
              "                    view_82: \"f32[1, (s53//8), (s53//((s53//8))), (s0//8), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(view_81, [sym_size_int_317, floordiv_388, 8, floordiv_389, 8, -1]);  view_81 = None\n",
              "                    permute_33: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.permute.default(view_82, [0, 1, 3, 2, 4, 5]);  view_82 = None\n",
              "                    clone_57: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.clone.default(permute_33, memory_format = torch.contiguous_format);  permute_33 = None\n",
              "                    view_83: \"f32[((s0//8))*((s53//8)), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(clone_57, [-1, 8, 8, 256]);  clone_57 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:98 in forward, code: x_windows=x_windows.view(-1, self.window_size*self.window_size, C)    #(B_, N, C)\n",
              "                    view_84: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(view_83, [-1, 64, 256]);  view_83 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_24: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 768]\" = torch.ops.aten.linear.default(view_84, p_layers_1_blocks_2_attn_qkv_weight, p_layers_1_blocks_2_attn_qkv_bias);  view_84 = p_layers_1_blocks_2_attn_qkv_weight = p_layers_1_blocks_2_attn_qkv_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:42 in forward, code: qkv=self.qkv(x).view(B_, N, 3, self.n_heads,C//self.n_heads)\n",
              "                    view_85: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 3, 8, 32]\" = torch.ops.aten.view.default(linear_24, [floordiv_391, 64, 3, 8, 32]);  linear_24 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:43 in forward, code: q, k, v=qkv.permute(2,0,3,1,4).contiguous()\n",
              "                    permute_34: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.permute.default(view_85, [2, 0, 3, 1, 4]);  view_85 = None\n",
              "                    clone_58: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.clone.default(permute_34, memory_format = torch.contiguous_format);  permute_34 = None\n",
              "                    unbind_6 = torch.ops.aten.unbind.int(clone_58);  clone_58 = None\n",
              "                    getitem_18: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_6[0]\n",
              "                    getitem_19: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_6[1]\n",
              "                    getitem_20: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_6[2];  unbind_6 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:44 in forward, code: attn=(q@k.transpose(-2, -1))*self.scale\n",
              "                    transpose_6: \"f32[((s0//8))*((s53//8)), 8, 32, ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.transpose.int(getitem_19, -2, -1);  getitem_19 = None\n",
              "                    matmul_12: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.matmul.default(getitem_18, transpose_6);  getitem_18 = transpose_6 = None\n",
              "                    mul_2195: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.mul.Tensor(matmul_12, 0.1767766952966369);  matmul_12 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:51 in forward, code: attn=attn.softmax(dim=-1)\n",
              "                    softmax_6: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.softmax.int(mul_2195, -1);  mul_2195 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:52 in forward, code: attn=(attn@v).permute(0,2,1,3).contiguous().view(B_, N, C)\n",
              "                    matmul_13: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.matmul.default(softmax_6, getitem_20);  softmax_6 = getitem_20 = None\n",
              "                    permute_35: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.permute.default(matmul_13, [0, 2, 1, 3]);  matmul_13 = None\n",
              "                    clone_59: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.clone.default(permute_35, memory_format = torch.contiguous_format);  permute_35 = None\n",
              "                    view_86: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(clone_59, [floordiv_391, 64, 256]);  clone_59 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_25: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.linear.default(view_86, p_layers_1_blocks_2_attn_proj_weight, p_layers_1_blocks_2_attn_proj_bias);  view_86 = p_layers_1_blocks_2_attn_proj_weight = p_layers_1_blocks_2_attn_proj_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:103 in forward, code: x_attn=x_attn.view(-1,self.window_size, self.window_size, C)\n",
              "                    view_87: \"f32[((s0//8))*((s53//8)), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(linear_25, [-1, 8, 8, 256]);  linear_25 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:104 in forward, code: x=window_reverse(x_attn, self.window_size, H, W)   # (B, H, W ,C)\n",
              "                    view_88: \"f32[1, (s53//8), (s0//8), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(view_87, [1, floordiv_388, floordiv_389, 8, 8, -1]);  view_87 = None\n",
              "                    permute_36: \"f32[1, (s53//8), 8, (s0//8), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.permute.default(view_88, [0, 1, 3, 2, 4, 5]);  view_88 = None\n",
              "                    clone_60: \"f32[1, (s53//8), 8, (s0//8), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.clone.default(permute_36, memory_format = torch.contiguous_format);  permute_36 = None\n",
              "                    view_89: \"f32[1, 8*((s53//8)), ((s0//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(clone_60, [1, sym_size_int_318, sym_size_int_319, -1]);  clone_60 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:109 in forward, code: x=x.view(-1, H*W, C)\n",
              "                    view_90: \"f32[1, 8*((s0//8))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(view_89, [-1, mul_471, 256]);  view_89 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:110 in forward, code: x=shortcut+self.drop_path1(x)\n",
              "                    add_1891: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_1744, view_90);  add_1744 = view_90 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_13: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_1891, [256], p_layers_1_blocks_2_norm2_weight, p_layers_1_blocks_2_norm2_bias);  p_layers_1_blocks_2_norm2_weight = p_layers_1_blocks_2_norm2_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_26: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.linear.default(layer_norm_13, p_layers_1_blocks_2_mlp_0_weight, p_layers_1_blocks_2_mlp_0_bias);  layer_norm_13 = p_layers_1_blocks_2_mlp_0_weight = p_layers_1_blocks_2_mlp_0_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
              "                    gelu_6: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.gelu.default(linear_26);  linear_26 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_27: \"f32[1, s0*s53, 256]\" = torch.ops.aten.linear.default(gelu_6, p_layers_1_blocks_2_mlp_2_weight, p_layers_1_blocks_2_mlp_2_bias);  gelu_6 = p_layers_1_blocks_2_mlp_2_weight = p_layers_1_blocks_2_mlp_2_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:111 in forward, code: x=x+self.drop_path2(self.mlp(self.norm2(x)))\n",
              "                    add_1907: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_1891, linear_27);  add_1891 = linear_27 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_14: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_1907, [256], p_layers_1_blocks_3_norm1_weight, p_layers_1_blocks_3_norm1_bias);  p_layers_1_blocks_3_norm1_weight = p_layers_1_blocks_3_norm1_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:92 in forward, code: x=self.norm1(x).view(x.shape[0], H, W, -1)\n",
              "                    view_91: \"f32[1, s53, s0, 256]\" = torch.ops.aten.view.default(layer_norm_14, [sym_size_int_317, sym_size_int_318, sym_size_int_319, -1]);  layer_norm_14 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:95 in forward, code: x=torch.roll(x, shifts=(-self.shift_size, -self.shift_size),dims=(1,2))\n",
              "                    roll_6: \"f32[1, s53, s0, 256]\" = torch.ops.aten.roll.default(view_91, [-4, -4], [1, 2]);  view_91 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:97 in forward, code: x_windows=window_partition(x, self.window_size)\n",
              "                    view_92: \"f32[1, (s53//8), (s53//((s53//8))), (s0//8), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(roll_6, [sym_size_int_317, floordiv_388, 8, floordiv_389, 8, -1]);  roll_6 = None\n",
              "                    permute_37: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.permute.default(view_92, [0, 1, 3, 2, 4, 5]);  view_92 = None\n",
              "                    clone_61: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.clone.default(permute_37, memory_format = torch.contiguous_format);  permute_37 = None\n",
              "                    view_93: \"f32[((s0//8))*((s53//8)), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(clone_61, [-1, 8, 8, 256]);  clone_61 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:98 in forward, code: x_windows=x_windows.view(-1, self.window_size*self.window_size, C)    #(B_, N, C)\n",
              "                    view_94: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(view_93, [-1, 64, 256]);  view_93 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:100 in forward, code: attn_mask=get_attn_mask(self.window_size, self.shift_size, H, W, x.device) if self.shift_size>0 else None\n",
              "                    zeros_3: \"f32[1, s53, s0, 1]\" = torch.ops.aten.zeros.default([1, sym_size_int_318, sym_size_int_319, 1], device = device(type='cpu'), pin_memory = False)\n",
              "                    clone_62: \"f32[]\" = torch.ops.aten.clone.default(c_layers_1_blocks_3_lifted_tensor_27);  c_layers_1_blocks_3_lifted_tensor_27 = None\n",
              "                    slice_184: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(zeros_3, 1, 0, -8)\n",
              "                    slice_185: \"f32[1, s53 - 8, s0 - 8, 1]\" = torch.ops.aten.slice.Tensor(slice_184, 2, 0, -8);  slice_184 = None\n",
              "                    fill_27: \"f32[1, s53 - 8, s0 - 8, 1]\" = torch.ops.aten.fill.Tensor(slice_185, clone_62);  slice_185 = clone_62 = None\n",
              "                    slice_186: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(zeros_3, 1, 0, -8)\n",
              "                    slice_scatter_54: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_186, fill_27, 2, 0, -8);  slice_186 = fill_27 = None\n",
              "                    slice_scatter_55: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(zeros_3, slice_scatter_54, 1, 0, -8);  zeros_3 = slice_scatter_54 = None\n",
              "                    clone_63: \"f32[]\" = torch.ops.aten.clone.default(c_layers_1_blocks_3_lifted_tensor_28);  c_layers_1_blocks_3_lifted_tensor_28 = None\n",
              "                    slice_191: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_55, 1, 0, -8)\n",
              "                    slice_192: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_191, 2, -8, -4);  slice_191 = None\n",
              "                    fill_28: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_192, clone_63);  slice_192 = clone_63 = None\n",
              "                    slice_193: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_55, 1, 0, -8)\n",
              "                    slice_scatter_56: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_193, fill_28, 2, -8, -4);  slice_193 = fill_28 = None\n",
              "                    slice_scatter_57: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_55, slice_scatter_56, 1, 0, -8);  slice_scatter_55 = slice_scatter_56 = None\n",
              "                    clone_64: \"f32[]\" = torch.ops.aten.clone.default(c_layers_1_blocks_3_lifted_tensor_29);  c_layers_1_blocks_3_lifted_tensor_29 = None\n",
              "                    slice_198: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_57, 1, 0, -8)\n",
              "                    slice_199: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_198, 2, -4, 9223372036854775807);  slice_198 = None\n",
              "                    fill_29: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_199, clone_64);  slice_199 = clone_64 = None\n",
              "                    slice_200: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_57, 1, 0, -8)\n",
              "                    slice_scatter_58: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_200, fill_29, 2, -4, 9223372036854775807);  slice_200 = fill_29 = None\n",
              "                    slice_scatter_59: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_57, slice_scatter_58, 1, 0, -8);  slice_scatter_57 = slice_scatter_58 = None\n",
              "                    clone_65: \"f32[]\" = torch.ops.aten.clone.default(c_layers_1_blocks_3_lifted_tensor_30);  c_layers_1_blocks_3_lifted_tensor_30 = None\n",
              "                    slice_205: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_59, 1, -8, -4)\n",
              "                    slice_206: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.slice.Tensor(slice_205, 2, 0, -8);  slice_205 = None\n",
              "                    fill_30: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.fill.Tensor(slice_206, clone_65);  slice_206 = clone_65 = None\n",
              "                    slice_207: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_59, 1, -8, -4)\n",
              "                    slice_scatter_60: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_207, fill_30, 2, 0, -8);  slice_207 = fill_30 = None\n",
              "                    slice_scatter_61: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_59, slice_scatter_60, 1, -8, -4);  slice_scatter_59 = slice_scatter_60 = None\n",
              "                    clone_66: \"f32[]\" = torch.ops.aten.clone.default(c_layers_1_blocks_3_lifted_tensor_31);  c_layers_1_blocks_3_lifted_tensor_31 = None\n",
              "                    slice_212: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_61, 1, -8, -4)\n",
              "                    slice_213: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_212, 2, -8, -4);  slice_212 = None\n",
              "                    fill_31: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_213, clone_66);  slice_213 = clone_66 = None\n",
              "                    slice_214: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_61, 1, -8, -4)\n",
              "                    slice_scatter_62: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_214, fill_31, 2, -8, -4);  slice_214 = fill_31 = None\n",
              "                    slice_scatter_63: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_61, slice_scatter_62, 1, -8, -4);  slice_scatter_61 = slice_scatter_62 = None\n",
              "                    clone_67: \"f32[]\" = torch.ops.aten.clone.default(c_layers_1_blocks_3_lifted_tensor_32);  c_layers_1_blocks_3_lifted_tensor_32 = None\n",
              "                    slice_219: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_63, 1, -8, -4)\n",
              "                    slice_220: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_219, 2, -4, 9223372036854775807);  slice_219 = None\n",
              "                    fill_32: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_220, clone_67);  slice_220 = clone_67 = None\n",
              "                    slice_221: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_63, 1, -8, -4)\n",
              "                    slice_scatter_64: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_221, fill_32, 2, -4, 9223372036854775807);  slice_221 = fill_32 = None\n",
              "                    slice_scatter_65: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_63, slice_scatter_64, 1, -8, -4);  slice_scatter_63 = slice_scatter_64 = None\n",
              "                    clone_68: \"f32[]\" = torch.ops.aten.clone.default(c_layers_1_blocks_3_lifted_tensor_33);  c_layers_1_blocks_3_lifted_tensor_33 = None\n",
              "                    slice_226: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_65, 1, -4, 9223372036854775807)\n",
              "                    slice_227: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.slice.Tensor(slice_226, 2, 0, -8);  slice_226 = None\n",
              "                    fill_33: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.fill.Tensor(slice_227, clone_68);  slice_227 = clone_68 = None\n",
              "                    slice_228: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_65, 1, -4, 9223372036854775807)\n",
              "                    slice_scatter_66: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_228, fill_33, 2, 0, -8);  slice_228 = fill_33 = None\n",
              "                    slice_scatter_67: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_65, slice_scatter_66, 1, -4, 9223372036854775807);  slice_scatter_65 = slice_scatter_66 = None\n",
              "                    clone_69: \"f32[]\" = torch.ops.aten.clone.default(c_layers_1_blocks_3_lifted_tensor_34);  c_layers_1_blocks_3_lifted_tensor_34 = None\n",
              "                    slice_233: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_67, 1, -4, 9223372036854775807)\n",
              "                    slice_234: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_233, 2, -8, -4);  slice_233 = None\n",
              "                    fill_34: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_234, clone_69);  slice_234 = clone_69 = None\n",
              "                    slice_235: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_67, 1, -4, 9223372036854775807)\n",
              "                    slice_scatter_68: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_235, fill_34, 2, -8, -4);  slice_235 = fill_34 = None\n",
              "                    slice_scatter_69: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_67, slice_scatter_68, 1, -4, 9223372036854775807);  slice_scatter_67 = slice_scatter_68 = None\n",
              "                    clone_70: \"f32[]\" = torch.ops.aten.clone.default(c_layers_1_blocks_3_lifted_tensor_35);  c_layers_1_blocks_3_lifted_tensor_35 = None\n",
              "                    slice_240: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_69, 1, -4, 9223372036854775807)\n",
              "                    slice_241: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_240, 2, -4, 9223372036854775807);  slice_240 = None\n",
              "                    fill_35: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_241, clone_70);  slice_241 = clone_70 = None\n",
              "                    slice_242: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_69, 1, -4, 9223372036854775807)\n",
              "                    slice_scatter_70: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_242, fill_35, 2, -4, 9223372036854775807);  slice_242 = fill_35 = None\n",
              "                    slice_scatter_71: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_69, slice_scatter_70, 1, -4, 9223372036854775807);  slice_scatter_69 = slice_scatter_70 = None\n",
              "                    view_96: \"f32[1, (s53//8), 8, (s0//8), 8, ((s0*s53)//(64*((s0//8))*((s53//8))))]\" = torch.ops.aten.view.default(slice_scatter_71, [1, floordiv_388, 8, floordiv_389, 8, -1]);  slice_scatter_71 = None\n",
              "                    permute_39: \"f32[1, (s53//8), (s0//8), 8, 8, ((s0*s53)//(64*((s0//8))*((s53//8))))]\" = torch.ops.aten.permute.default(view_96, [0, 1, 3, 2, 4, 5]);  view_96 = None\n",
              "                    clone_71: \"f32[1, (s53//8), (s0//8), 8, 8, ((s0*s53)//(64*((s0//8))*((s53//8))))]\" = torch.ops.aten.clone.default(permute_39, memory_format = torch.contiguous_format);  permute_39 = None\n",
              "                    view_97: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 8, 8, 1]\" = torch.ops.aten.view.default(clone_71, [-1, 8, 8, 1]);  clone_71 = None\n",
              "                    view_98: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64]\" = torch.ops.aten.view.default(view_97, [mul_846, -1]);  view_97 = None\n",
              "                    unsqueeze_12: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 1, 64]\" = torch.ops.aten.unsqueeze.default(view_98, 1)\n",
              "                    unsqueeze_13: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 1]\" = torch.ops.aten.unsqueeze.default(view_98, 2);  view_98 = None\n",
              "                    sub_824: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.sub.Tensor(unsqueeze_12, unsqueeze_13);  unsqueeze_12 = unsqueeze_13 = None\n",
              "                    scalar_tensor_default_6: \"f32[]\" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32)\n",
              "                    ne_111: \"b8[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.ne.Tensor(sub_824, scalar_tensor_default_6);  scalar_tensor_default_6 = None\n",
              "                    masked_fill_6: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.masked_fill.Scalar(sub_824, ne_111, -100.0);  ne_111 = None\n",
              "                    scalar_tensor_default_7: \"f32[]\" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32)\n",
              "                    eq_1264: \"b8[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.eq.Tensor(sub_824, scalar_tensor_default_7);  sub_824 = scalar_tensor_default_7 = None\n",
              "                    masked_fill_7: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.masked_fill.Scalar(masked_fill_6, eq_1264, 0.0);  masked_fill_6 = eq_1264 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_28: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 768]\" = torch.ops.aten.linear.default(view_94, p_layers_1_blocks_3_attn_qkv_weight, p_layers_1_blocks_3_attn_qkv_bias);  view_94 = p_layers_1_blocks_3_attn_qkv_weight = p_layers_1_blocks_3_attn_qkv_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:42 in forward, code: qkv=self.qkv(x).view(B_, N, 3, self.n_heads,C//self.n_heads)\n",
              "                    view_99: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 3, 8, 32]\" = torch.ops.aten.view.default(linear_28, [floordiv_391, 64, 3, 8, 32]);  linear_28 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:43 in forward, code: q, k, v=qkv.permute(2,0,3,1,4).contiguous()\n",
              "                    permute_40: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.permute.default(view_99, [2, 0, 3, 1, 4]);  view_99 = None\n",
              "                    clone_72: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.clone.default(permute_40, memory_format = torch.contiguous_format);  permute_40 = None\n",
              "                    unbind_7 = torch.ops.aten.unbind.int(clone_72);  clone_72 = None\n",
              "                    getitem_21: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_7[0]\n",
              "                    getitem_22: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_7[1]\n",
              "                    getitem_23: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_7[2];  unbind_7 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:44 in forward, code: attn=(q@k.transpose(-2, -1))*self.scale\n",
              "                    transpose_7: \"f32[((s0//8))*((s53//8)), 8, 32, ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.transpose.int(getitem_22, -2, -1);  getitem_22 = None\n",
              "                    matmul_14: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.matmul.default(getitem_21, transpose_7);  getitem_21 = transpose_7 = None\n",
              "                    mul_2526: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.mul.Tensor(matmul_14, 0.1767766952966369);  matmul_14 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:48 in forward, code: attn=attn.view(B_//nW, nW, self.n_heads, N, N)\n",
              "                    view_100: \"f32[((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))), ((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.view.default(mul_2526, [floordiv_393, mul_846, 8, 64, 64]);  mul_2526 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:49 in forward, code: attn=attn+mask.unsqueeze(1).unsqueeze(0)\n",
              "                    unsqueeze_14: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 1, 64, 64]\" = torch.ops.aten.unsqueeze.default(masked_fill_7, 1);  masked_fill_7 = None\n",
              "                    unsqueeze_15: \"f32[1, ((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 1, 64, 64]\" = torch.ops.aten.unsqueeze.default(unsqueeze_14, 0);  unsqueeze_14 = None\n",
              "                    add_2218: \"f32[1, ((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.add.Tensor(view_100, unsqueeze_15);  view_100 = unsqueeze_15 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:50 in forward, code: attn=attn.view(B_,self.n_heads,N,N)\n",
              "                    view_101: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.view.default(add_2218, [floordiv_391, 8, 64, 64]);  add_2218 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:51 in forward, code: attn=attn.softmax(dim=-1)\n",
              "                    softmax_7: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.softmax.int(view_101, -1);  view_101 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:52 in forward, code: attn=(attn@v).permute(0,2,1,3).contiguous().view(B_, N, C)\n",
              "                    matmul_15: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.matmul.default(softmax_7, getitem_23);  softmax_7 = getitem_23 = None\n",
              "                    permute_41: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.permute.default(matmul_15, [0, 2, 1, 3]);  matmul_15 = None\n",
              "                    clone_73: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.clone.default(permute_41, memory_format = torch.contiguous_format);  permute_41 = None\n",
              "                    view_102: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(clone_73, [floordiv_391, 64, 256]);  clone_73 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_29: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.linear.default(view_102, p_layers_1_blocks_3_attn_proj_weight, p_layers_1_blocks_3_attn_proj_bias);  view_102 = p_layers_1_blocks_3_attn_proj_weight = p_layers_1_blocks_3_attn_proj_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:103 in forward, code: x_attn=x_attn.view(-1,self.window_size, self.window_size, C)\n",
              "                    view_103: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(linear_29, [-1, 8, 8, 256]);  linear_29 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:104 in forward, code: x=window_reverse(x_attn, self.window_size, H, W)   # (B, H, W ,C)\n",
              "                    view_104: \"f32[1, (s53//8), (s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(view_103, [1, floordiv_388, floordiv_389, 8, 8, -1]);  view_103 = None\n",
              "                    permute_42: \"f32[1, (s53//8), 8, (s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.permute.default(view_104, [0, 1, 3, 2, 4, 5]);  view_104 = None\n",
              "                    clone_74: \"f32[1, (s53//8), 8, (s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.clone.default(permute_42, memory_format = torch.contiguous_format);  permute_42 = None\n",
              "                    view_105: \"f32[1, 8*((s53//8)), ((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(clone_74, [1, sym_size_int_318, sym_size_int_319, -1]);  clone_74 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:107 in forward, code: x=torch.roll(x, shifts=(self.shift_size, self.shift_size),dims=(1,2))\n",
              "                    roll_7: \"f32[1, 8*((s53//8)), ((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.roll.default(view_105, [4, 4], [1, 2]);  view_105 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:109 in forward, code: x=x.view(-1, H*W, C)\n",
              "                    view_106: \"f32[1, 8*((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(roll_7, [-1, mul_471, 256]);  roll_7 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:110 in forward, code: x=shortcut+self.drop_path1(x)\n",
              "                    add_2291: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_1907, view_106);  add_1907 = view_106 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_15: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_2291, [256], p_layers_1_blocks_3_norm2_weight, p_layers_1_blocks_3_norm2_bias);  p_layers_1_blocks_3_norm2_weight = p_layers_1_blocks_3_norm2_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_30: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.linear.default(layer_norm_15, p_layers_1_blocks_3_mlp_0_weight, p_layers_1_blocks_3_mlp_0_bias);  layer_norm_15 = p_layers_1_blocks_3_mlp_0_weight = p_layers_1_blocks_3_mlp_0_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
              "                    gelu_7: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.gelu.default(linear_30);  linear_30 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_31: \"f32[1, s0*s53, 256]\" = torch.ops.aten.linear.default(gelu_7, p_layers_1_blocks_3_mlp_2_weight, p_layers_1_blocks_3_mlp_2_bias);  gelu_7 = p_layers_1_blocks_3_mlp_2_weight = p_layers_1_blocks_3_mlp_2_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:111 in forward, code: x=x+self.drop_path2(self.mlp(self.norm2(x)))\n",
              "                    add_2307: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_2291, linear_31);  add_2291 = linear_31 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:135 in forward, code: x=x.view(B, H, W, C).permute(0,3,1,2).contiguous()\n",
              "                    view_107: \"f32[1, s53, s0, 256]\" = torch.ops.aten.view.default(add_2307, [sym_size_int_317, sym_size_int_318, sym_size_int_319, 256]);  add_2307 = None\n",
              "                    permute_43: \"f32[1, 256, s53, s0]\" = torch.ops.aten.permute.default(view_107, [0, 3, 1, 2]);  view_107 = None\n",
              "                    clone_75: \"f32[1, 256, s53, s0]\" = torch.ops.aten.clone.default(permute_43, memory_format = torch.contiguous_format);  permute_43 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
              "                    conv2d_2: \"f32[1, 256, s53, s0]\" = torch.ops.aten.conv2d.default(clone_75, p_layers_1_conv_weight, p_layers_1_conv_bias, [1, 1], [1, 1]);  clone_75 = p_layers_1_conv_weight = p_layers_1_conv_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:137 in forward, code: x=x.permute(0,2,3,1).contiguous()\n",
              "                    permute_44: \"f32[1, s53, s0, 256]\" = torch.ops.aten.permute.default(conv2d_2, [0, 2, 3, 1]);  conv2d_2 = None\n",
              "                    clone_76: \"f32[1, s53, s0, 256]\" = torch.ops.aten.clone.default(permute_44, memory_format = torch.contiguous_format);  permute_44 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:138 in forward, code: x=shortcut+x\n",
              "                    add_2334: \"f32[1, s53, s0, 256]\" = torch.ops.aten.add.Tensor(add_1177, clone_76);  add_1177 = clone_76 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:132 in forward, code: x=x.view(B,H*W,C)\n",
              "                    view_108: \"f32[1, s0*s53, 256]\" = torch.ops.aten.view.default(add_2334, [sym_size_int_317, mul_471, 256])\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_16: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(view_108, [256], p_layers_2_blocks_0_norm1_weight, p_layers_2_blocks_0_norm1_bias);  p_layers_2_blocks_0_norm1_weight = p_layers_2_blocks_0_norm1_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:92 in forward, code: x=self.norm1(x).view(x.shape[0], H, W, -1)\n",
              "                    view_109: \"f32[1, s53, s0, 256]\" = torch.ops.aten.view.default(layer_norm_16, [sym_size_int_317, sym_size_int_318, sym_size_int_319, -1]);  layer_norm_16 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:97 in forward, code: x_windows=window_partition(x, self.window_size)\n",
              "                    view_110: \"f32[1, (s53//8), (s53//((s53//8))), (s0//8), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(view_109, [sym_size_int_317, floordiv_388, 8, floordiv_389, 8, -1]);  view_109 = None\n",
              "                    permute_45: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.permute.default(view_110, [0, 1, 3, 2, 4, 5]);  view_110 = None\n",
              "                    clone_77: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.clone.default(permute_45, memory_format = torch.contiguous_format);  permute_45 = None\n",
              "                    view_111: \"f32[((s0//8))*((s53//8)), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(clone_77, [-1, 8, 8, 256]);  clone_77 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:98 in forward, code: x_windows=x_windows.view(-1, self.window_size*self.window_size, C)    #(B_, N, C)\n",
              "                    view_112: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(view_111, [-1, 64, 256]);  view_111 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_32: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 768]\" = torch.ops.aten.linear.default(view_112, p_layers_2_blocks_0_attn_qkv_weight, p_layers_2_blocks_0_attn_qkv_bias);  view_112 = p_layers_2_blocks_0_attn_qkv_weight = p_layers_2_blocks_0_attn_qkv_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:42 in forward, code: qkv=self.qkv(x).view(B_, N, 3, self.n_heads,C//self.n_heads)\n",
              "                    view_113: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 3, 8, 32]\" = torch.ops.aten.view.default(linear_32, [floordiv_391, 64, 3, 8, 32]);  linear_32 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:43 in forward, code: q, k, v=qkv.permute(2,0,3,1,4).contiguous()\n",
              "                    permute_46: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.permute.default(view_113, [2, 0, 3, 1, 4]);  view_113 = None\n",
              "                    clone_78: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.clone.default(permute_46, memory_format = torch.contiguous_format);  permute_46 = None\n",
              "                    unbind_8 = torch.ops.aten.unbind.int(clone_78);  clone_78 = None\n",
              "                    getitem_24: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_8[0]\n",
              "                    getitem_25: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_8[1]\n",
              "                    getitem_26: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_8[2];  unbind_8 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:44 in forward, code: attn=(q@k.transpose(-2, -1))*self.scale\n",
              "                    transpose_8: \"f32[((s0//8))*((s53//8)), 8, 32, ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.transpose.int(getitem_25, -2, -1);  getitem_25 = None\n",
              "                    matmul_16: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.matmul.default(getitem_24, transpose_8);  getitem_24 = transpose_8 = None\n",
              "                    mul_2762: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.mul.Tensor(matmul_16, 0.1767766952966369);  matmul_16 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:51 in forward, code: attn=attn.softmax(dim=-1)\n",
              "                    softmax_8: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.softmax.int(mul_2762, -1);  mul_2762 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:52 in forward, code: attn=(attn@v).permute(0,2,1,3).contiguous().view(B_, N, C)\n",
              "                    matmul_17: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.matmul.default(softmax_8, getitem_26);  softmax_8 = getitem_26 = None\n",
              "                    permute_47: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.permute.default(matmul_17, [0, 2, 1, 3]);  matmul_17 = None\n",
              "                    clone_79: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.clone.default(permute_47, memory_format = torch.contiguous_format);  permute_47 = None\n",
              "                    view_114: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(clone_79, [floordiv_391, 64, 256]);  clone_79 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_33: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.linear.default(view_114, p_layers_2_blocks_0_attn_proj_weight, p_layers_2_blocks_0_attn_proj_bias);  view_114 = p_layers_2_blocks_0_attn_proj_weight = p_layers_2_blocks_0_attn_proj_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:103 in forward, code: x_attn=x_attn.view(-1,self.window_size, self.window_size, C)\n",
              "                    view_115: \"f32[((s0//8))*((s53//8)), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(linear_33, [-1, 8, 8, 256]);  linear_33 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:104 in forward, code: x=window_reverse(x_attn, self.window_size, H, W)   # (B, H, W ,C)\n",
              "                    view_116: \"f32[1, (s53//8), (s0//8), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(view_115, [1, floordiv_388, floordiv_389, 8, 8, -1]);  view_115 = None\n",
              "                    permute_48: \"f32[1, (s53//8), 8, (s0//8), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.permute.default(view_116, [0, 1, 3, 2, 4, 5]);  view_116 = None\n",
              "                    clone_80: \"f32[1, (s53//8), 8, (s0//8), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.clone.default(permute_48, memory_format = torch.contiguous_format);  permute_48 = None\n",
              "                    view_117: \"f32[1, 8*((s53//8)), ((s0//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(clone_80, [1, sym_size_int_318, sym_size_int_319, -1]);  clone_80 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:109 in forward, code: x=x.view(-1, H*W, C)\n",
              "                    view_118: \"f32[1, 8*((s0//8))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(view_117, [-1, mul_471, 256]);  view_117 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:110 in forward, code: x=shortcut+self.drop_path1(x)\n",
              "                    add_2485: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(view_108, view_118);  view_108 = view_118 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_17: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_2485, [256], p_layers_2_blocks_0_norm2_weight, p_layers_2_blocks_0_norm2_bias);  p_layers_2_blocks_0_norm2_weight = p_layers_2_blocks_0_norm2_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_34: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.linear.default(layer_norm_17, p_layers_2_blocks_0_mlp_0_weight, p_layers_2_blocks_0_mlp_0_bias);  layer_norm_17 = p_layers_2_blocks_0_mlp_0_weight = p_layers_2_blocks_0_mlp_0_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
              "                    gelu_8: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.gelu.default(linear_34);  linear_34 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_35: \"f32[1, s0*s53, 256]\" = torch.ops.aten.linear.default(gelu_8, p_layers_2_blocks_0_mlp_2_weight, p_layers_2_blocks_0_mlp_2_bias);  gelu_8 = p_layers_2_blocks_0_mlp_2_weight = p_layers_2_blocks_0_mlp_2_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:111 in forward, code: x=x+self.drop_path2(self.mlp(self.norm2(x)))\n",
              "                    add_2501: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_2485, linear_35);  add_2485 = linear_35 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_18: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_2501, [256], p_layers_2_blocks_1_norm1_weight, p_layers_2_blocks_1_norm1_bias);  p_layers_2_blocks_1_norm1_weight = p_layers_2_blocks_1_norm1_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:92 in forward, code: x=self.norm1(x).view(x.shape[0], H, W, -1)\n",
              "                    view_119: \"f32[1, s53, s0, 256]\" = torch.ops.aten.view.default(layer_norm_18, [sym_size_int_317, sym_size_int_318, sym_size_int_319, -1]);  layer_norm_18 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:95 in forward, code: x=torch.roll(x, shifts=(-self.shift_size, -self.shift_size),dims=(1,2))\n",
              "                    roll_8: \"f32[1, s53, s0, 256]\" = torch.ops.aten.roll.default(view_119, [-4, -4], [1, 2]);  view_119 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:97 in forward, code: x_windows=window_partition(x, self.window_size)\n",
              "                    view_120: \"f32[1, (s53//8), (s53//((s53//8))), (s0//8), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(roll_8, [sym_size_int_317, floordiv_388, 8, floordiv_389, 8, -1]);  roll_8 = None\n",
              "                    permute_49: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.permute.default(view_120, [0, 1, 3, 2, 4, 5]);  view_120 = None\n",
              "                    clone_81: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.clone.default(permute_49, memory_format = torch.contiguous_format);  permute_49 = None\n",
              "                    view_121: \"f32[((s0//8))*((s53//8)), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(clone_81, [-1, 8, 8, 256]);  clone_81 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:98 in forward, code: x_windows=x_windows.view(-1, self.window_size*self.window_size, C)    #(B_, N, C)\n",
              "                    view_122: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(view_121, [-1, 64, 256]);  view_121 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:100 in forward, code: attn_mask=get_attn_mask(self.window_size, self.shift_size, H, W, x.device) if self.shift_size>0 else None\n",
              "                    zeros_4: \"f32[1, s53, s0, 1]\" = torch.ops.aten.zeros.default([1, sym_size_int_318, sym_size_int_319, 1], device = device(type='cpu'), pin_memory = False)\n",
              "                    clone_82: \"f32[]\" = torch.ops.aten.clone.default(c_layers_2_blocks_1_lifted_tensor_36);  c_layers_2_blocks_1_lifted_tensor_36 = None\n",
              "                    slice_245: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(zeros_4, 1, 0, -8)\n",
              "                    slice_246: \"f32[1, s53 - 8, s0 - 8, 1]\" = torch.ops.aten.slice.Tensor(slice_245, 2, 0, -8);  slice_245 = None\n",
              "                    fill_36: \"f32[1, s53 - 8, s0 - 8, 1]\" = torch.ops.aten.fill.Tensor(slice_246, clone_82);  slice_246 = clone_82 = None\n",
              "                    slice_247: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(zeros_4, 1, 0, -8)\n",
              "                    slice_scatter_72: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_247, fill_36, 2, 0, -8);  slice_247 = fill_36 = None\n",
              "                    slice_scatter_73: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(zeros_4, slice_scatter_72, 1, 0, -8);  zeros_4 = slice_scatter_72 = None\n",
              "                    clone_83: \"f32[]\" = torch.ops.aten.clone.default(c_layers_2_blocks_1_lifted_tensor_37);  c_layers_2_blocks_1_lifted_tensor_37 = None\n",
              "                    slice_252: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_73, 1, 0, -8)\n",
              "                    slice_253: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_252, 2, -8, -4);  slice_252 = None\n",
              "                    fill_37: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_253, clone_83);  slice_253 = clone_83 = None\n",
              "                    slice_254: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_73, 1, 0, -8)\n",
              "                    slice_scatter_74: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_254, fill_37, 2, -8, -4);  slice_254 = fill_37 = None\n",
              "                    slice_scatter_75: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_73, slice_scatter_74, 1, 0, -8);  slice_scatter_73 = slice_scatter_74 = None\n",
              "                    clone_84: \"f32[]\" = torch.ops.aten.clone.default(c_layers_2_blocks_1_lifted_tensor_38);  c_layers_2_blocks_1_lifted_tensor_38 = None\n",
              "                    slice_259: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_75, 1, 0, -8)\n",
              "                    slice_260: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_259, 2, -4, 9223372036854775807);  slice_259 = None\n",
              "                    fill_38: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_260, clone_84);  slice_260 = clone_84 = None\n",
              "                    slice_261: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_75, 1, 0, -8)\n",
              "                    slice_scatter_76: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_261, fill_38, 2, -4, 9223372036854775807);  slice_261 = fill_38 = None\n",
              "                    slice_scatter_77: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_75, slice_scatter_76, 1, 0, -8);  slice_scatter_75 = slice_scatter_76 = None\n",
              "                    clone_85: \"f32[]\" = torch.ops.aten.clone.default(c_layers_2_blocks_1_lifted_tensor_39);  c_layers_2_blocks_1_lifted_tensor_39 = None\n",
              "                    slice_266: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_77, 1, -8, -4)\n",
              "                    slice_267: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.slice.Tensor(slice_266, 2, 0, -8);  slice_266 = None\n",
              "                    fill_39: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.fill.Tensor(slice_267, clone_85);  slice_267 = clone_85 = None\n",
              "                    slice_268: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_77, 1, -8, -4)\n",
              "                    slice_scatter_78: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_268, fill_39, 2, 0, -8);  slice_268 = fill_39 = None\n",
              "                    slice_scatter_79: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_77, slice_scatter_78, 1, -8, -4);  slice_scatter_77 = slice_scatter_78 = None\n",
              "                    clone_86: \"f32[]\" = torch.ops.aten.clone.default(c_layers_2_blocks_1_lifted_tensor_40);  c_layers_2_blocks_1_lifted_tensor_40 = None\n",
              "                    slice_273: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_79, 1, -8, -4)\n",
              "                    slice_274: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_273, 2, -8, -4);  slice_273 = None\n",
              "                    fill_40: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_274, clone_86);  slice_274 = clone_86 = None\n",
              "                    slice_275: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_79, 1, -8, -4)\n",
              "                    slice_scatter_80: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_275, fill_40, 2, -8, -4);  slice_275 = fill_40 = None\n",
              "                    slice_scatter_81: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_79, slice_scatter_80, 1, -8, -4);  slice_scatter_79 = slice_scatter_80 = None\n",
              "                    clone_87: \"f32[]\" = torch.ops.aten.clone.default(c_layers_2_blocks_1_lifted_tensor_41);  c_layers_2_blocks_1_lifted_tensor_41 = None\n",
              "                    slice_280: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_81, 1, -8, -4)\n",
              "                    slice_281: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_280, 2, -4, 9223372036854775807);  slice_280 = None\n",
              "                    fill_41: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_281, clone_87);  slice_281 = clone_87 = None\n",
              "                    slice_282: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_81, 1, -8, -4)\n",
              "                    slice_scatter_82: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_282, fill_41, 2, -4, 9223372036854775807);  slice_282 = fill_41 = None\n",
              "                    slice_scatter_83: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_81, slice_scatter_82, 1, -8, -4);  slice_scatter_81 = slice_scatter_82 = None\n",
              "                    clone_88: \"f32[]\" = torch.ops.aten.clone.default(c_layers_2_blocks_1_lifted_tensor_42);  c_layers_2_blocks_1_lifted_tensor_42 = None\n",
              "                    slice_287: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_83, 1, -4, 9223372036854775807)\n",
              "                    slice_288: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.slice.Tensor(slice_287, 2, 0, -8);  slice_287 = None\n",
              "                    fill_42: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.fill.Tensor(slice_288, clone_88);  slice_288 = clone_88 = None\n",
              "                    slice_289: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_83, 1, -4, 9223372036854775807)\n",
              "                    slice_scatter_84: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_289, fill_42, 2, 0, -8);  slice_289 = fill_42 = None\n",
              "                    slice_scatter_85: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_83, slice_scatter_84, 1, -4, 9223372036854775807);  slice_scatter_83 = slice_scatter_84 = None\n",
              "                    clone_89: \"f32[]\" = torch.ops.aten.clone.default(c_layers_2_blocks_1_lifted_tensor_43);  c_layers_2_blocks_1_lifted_tensor_43 = None\n",
              "                    slice_294: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_85, 1, -4, 9223372036854775807)\n",
              "                    slice_295: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_294, 2, -8, -4);  slice_294 = None\n",
              "                    fill_43: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_295, clone_89);  slice_295 = clone_89 = None\n",
              "                    slice_296: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_85, 1, -4, 9223372036854775807)\n",
              "                    slice_scatter_86: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_296, fill_43, 2, -8, -4);  slice_296 = fill_43 = None\n",
              "                    slice_scatter_87: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_85, slice_scatter_86, 1, -4, 9223372036854775807);  slice_scatter_85 = slice_scatter_86 = None\n",
              "                    clone_90: \"f32[]\" = torch.ops.aten.clone.default(c_layers_2_blocks_1_lifted_tensor_44);  c_layers_2_blocks_1_lifted_tensor_44 = None\n",
              "                    slice_301: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_87, 1, -4, 9223372036854775807)\n",
              "                    slice_302: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_301, 2, -4, 9223372036854775807);  slice_301 = None\n",
              "                    fill_44: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_302, clone_90);  slice_302 = clone_90 = None\n",
              "                    slice_303: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_87, 1, -4, 9223372036854775807)\n",
              "                    slice_scatter_88: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_303, fill_44, 2, -4, 9223372036854775807);  slice_303 = fill_44 = None\n",
              "                    slice_scatter_89: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_87, slice_scatter_88, 1, -4, 9223372036854775807);  slice_scatter_87 = slice_scatter_88 = None\n",
              "                    view_124: \"f32[1, (s53//8), 8, (s0//8), 8, ((s0*s53)//(64*((s0//8))*((s53//8))))]\" = torch.ops.aten.view.default(slice_scatter_89, [1, floordiv_388, 8, floordiv_389, 8, -1]);  slice_scatter_89 = None\n",
              "                    permute_51: \"f32[1, (s53//8), (s0//8), 8, 8, ((s0*s53)//(64*((s0//8))*((s53//8))))]\" = torch.ops.aten.permute.default(view_124, [0, 1, 3, 2, 4, 5]);  view_124 = None\n",
              "                    clone_91: \"f32[1, (s53//8), (s0//8), 8, 8, ((s0*s53)//(64*((s0//8))*((s53//8))))]\" = torch.ops.aten.clone.default(permute_51, memory_format = torch.contiguous_format);  permute_51 = None\n",
              "                    view_125: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 8, 8, 1]\" = torch.ops.aten.view.default(clone_91, [-1, 8, 8, 1]);  clone_91 = None\n",
              "                    view_126: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64]\" = torch.ops.aten.view.default(view_125, [mul_846, -1]);  view_125 = None\n",
              "                    unsqueeze_16: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 1, 64]\" = torch.ops.aten.unsqueeze.default(view_126, 1)\n",
              "                    unsqueeze_17: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 1]\" = torch.ops.aten.unsqueeze.default(view_126, 2);  view_126 = None\n",
              "                    sub_1056: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.sub.Tensor(unsqueeze_16, unsqueeze_17);  unsqueeze_16 = unsqueeze_17 = None\n",
              "                    scalar_tensor_default_8: \"f32[]\" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32)\n",
              "                    ne_138: \"b8[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.ne.Tensor(sub_1056, scalar_tensor_default_8);  scalar_tensor_default_8 = None\n",
              "                    masked_fill_8: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.masked_fill.Scalar(sub_1056, ne_138, -100.0);  ne_138 = None\n",
              "                    scalar_tensor_default_9: \"f32[]\" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32)\n",
              "                    eq_1619: \"b8[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.eq.Tensor(sub_1056, scalar_tensor_default_9);  sub_1056 = scalar_tensor_default_9 = None\n",
              "                    masked_fill_9: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.masked_fill.Scalar(masked_fill_8, eq_1619, 0.0);  masked_fill_8 = eq_1619 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_36: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 768]\" = torch.ops.aten.linear.default(view_122, p_layers_2_blocks_1_attn_qkv_weight, p_layers_2_blocks_1_attn_qkv_bias);  view_122 = p_layers_2_blocks_1_attn_qkv_weight = p_layers_2_blocks_1_attn_qkv_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:42 in forward, code: qkv=self.qkv(x).view(B_, N, 3, self.n_heads,C//self.n_heads)\n",
              "                    view_127: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 3, 8, 32]\" = torch.ops.aten.view.default(linear_36, [floordiv_391, 64, 3, 8, 32]);  linear_36 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:43 in forward, code: q, k, v=qkv.permute(2,0,3,1,4).contiguous()\n",
              "                    permute_52: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.permute.default(view_127, [2, 0, 3, 1, 4]);  view_127 = None\n",
              "                    clone_92: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.clone.default(permute_52, memory_format = torch.contiguous_format);  permute_52 = None\n",
              "                    unbind_9 = torch.ops.aten.unbind.int(clone_92);  clone_92 = None\n",
              "                    getitem_27: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_9[0]\n",
              "                    getitem_28: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_9[1]\n",
              "                    getitem_29: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_9[2];  unbind_9 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:44 in forward, code: attn=(q@k.transpose(-2, -1))*self.scale\n",
              "                    transpose_9: \"f32[((s0//8))*((s53//8)), 8, 32, ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.transpose.int(getitem_28, -2, -1);  getitem_28 = None\n",
              "                    matmul_18: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.matmul.default(getitem_27, transpose_9);  getitem_27 = transpose_9 = None\n",
              "                    mul_3093: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.mul.Tensor(matmul_18, 0.1767766952966369);  matmul_18 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:48 in forward, code: attn=attn.view(B_//nW, nW, self.n_heads, N, N)\n",
              "                    view_128: \"f32[((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))), ((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.view.default(mul_3093, [floordiv_393, mul_846, 8, 64, 64]);  mul_3093 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:49 in forward, code: attn=attn+mask.unsqueeze(1).unsqueeze(0)\n",
              "                    unsqueeze_18: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 1, 64, 64]\" = torch.ops.aten.unsqueeze.default(masked_fill_9, 1);  masked_fill_9 = None\n",
              "                    unsqueeze_19: \"f32[1, ((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 1, 64, 64]\" = torch.ops.aten.unsqueeze.default(unsqueeze_18, 0);  unsqueeze_18 = None\n",
              "                    add_2812: \"f32[1, ((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.add.Tensor(view_128, unsqueeze_19);  view_128 = unsqueeze_19 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:50 in forward, code: attn=attn.view(B_,self.n_heads,N,N)\n",
              "                    view_129: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.view.default(add_2812, [floordiv_391, 8, 64, 64]);  add_2812 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:51 in forward, code: attn=attn.softmax(dim=-1)\n",
              "                    softmax_9: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.softmax.int(view_129, -1);  view_129 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:52 in forward, code: attn=(attn@v).permute(0,2,1,3).contiguous().view(B_, N, C)\n",
              "                    matmul_19: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.matmul.default(softmax_9, getitem_29);  softmax_9 = getitem_29 = None\n",
              "                    permute_53: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.permute.default(matmul_19, [0, 2, 1, 3]);  matmul_19 = None\n",
              "                    clone_93: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.clone.default(permute_53, memory_format = torch.contiguous_format);  permute_53 = None\n",
              "                    view_130: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(clone_93, [floordiv_391, 64, 256]);  clone_93 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_37: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.linear.default(view_130, p_layers_2_blocks_1_attn_proj_weight, p_layers_2_blocks_1_attn_proj_bias);  view_130 = p_layers_2_blocks_1_attn_proj_weight = p_layers_2_blocks_1_attn_proj_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:103 in forward, code: x_attn=x_attn.view(-1,self.window_size, self.window_size, C)\n",
              "                    view_131: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(linear_37, [-1, 8, 8, 256]);  linear_37 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:104 in forward, code: x=window_reverse(x_attn, self.window_size, H, W)   # (B, H, W ,C)\n",
              "                    view_132: \"f32[1, (s53//8), (s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(view_131, [1, floordiv_388, floordiv_389, 8, 8, -1]);  view_131 = None\n",
              "                    permute_54: \"f32[1, (s53//8), 8, (s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.permute.default(view_132, [0, 1, 3, 2, 4, 5]);  view_132 = None\n",
              "                    clone_94: \"f32[1, (s53//8), 8, (s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.clone.default(permute_54, memory_format = torch.contiguous_format);  permute_54 = None\n",
              "                    view_133: \"f32[1, 8*((s53//8)), ((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(clone_94, [1, sym_size_int_318, sym_size_int_319, -1]);  clone_94 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:107 in forward, code: x=torch.roll(x, shifts=(self.shift_size, self.shift_size),dims=(1,2))\n",
              "                    roll_9: \"f32[1, 8*((s53//8)), ((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.roll.default(view_133, [4, 4], [1, 2]);  view_133 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:109 in forward, code: x=x.view(-1, H*W, C)\n",
              "                    view_134: \"f32[1, 8*((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(roll_9, [-1, mul_471, 256]);  roll_9 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:110 in forward, code: x=shortcut+self.drop_path1(x)\n",
              "                    add_2885: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_2501, view_134);  add_2501 = view_134 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_19: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_2885, [256], p_layers_2_blocks_1_norm2_weight, p_layers_2_blocks_1_norm2_bias);  p_layers_2_blocks_1_norm2_weight = p_layers_2_blocks_1_norm2_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_38: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.linear.default(layer_norm_19, p_layers_2_blocks_1_mlp_0_weight, p_layers_2_blocks_1_mlp_0_bias);  layer_norm_19 = p_layers_2_blocks_1_mlp_0_weight = p_layers_2_blocks_1_mlp_0_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
              "                    gelu_9: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.gelu.default(linear_38);  linear_38 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_39: \"f32[1, s0*s53, 256]\" = torch.ops.aten.linear.default(gelu_9, p_layers_2_blocks_1_mlp_2_weight, p_layers_2_blocks_1_mlp_2_bias);  gelu_9 = p_layers_2_blocks_1_mlp_2_weight = p_layers_2_blocks_1_mlp_2_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:111 in forward, code: x=x+self.drop_path2(self.mlp(self.norm2(x)))\n",
              "                    add_2901: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_2885, linear_39);  add_2885 = linear_39 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_20: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_2901, [256], p_layers_2_blocks_2_norm1_weight, p_layers_2_blocks_2_norm1_bias);  p_layers_2_blocks_2_norm1_weight = p_layers_2_blocks_2_norm1_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:92 in forward, code: x=self.norm1(x).view(x.shape[0], H, W, -1)\n",
              "                    view_135: \"f32[1, s53, s0, 256]\" = torch.ops.aten.view.default(layer_norm_20, [sym_size_int_317, sym_size_int_318, sym_size_int_319, -1]);  layer_norm_20 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:97 in forward, code: x_windows=window_partition(x, self.window_size)\n",
              "                    view_136: \"f32[1, (s53//8), (s53//((s53//8))), (s0//8), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(view_135, [sym_size_int_317, floordiv_388, 8, floordiv_389, 8, -1]);  view_135 = None\n",
              "                    permute_55: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.permute.default(view_136, [0, 1, 3, 2, 4, 5]);  view_136 = None\n",
              "                    clone_95: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.clone.default(permute_55, memory_format = torch.contiguous_format);  permute_55 = None\n",
              "                    view_137: \"f32[((s0//8))*((s53//8)), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(clone_95, [-1, 8, 8, 256]);  clone_95 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:98 in forward, code: x_windows=x_windows.view(-1, self.window_size*self.window_size, C)    #(B_, N, C)\n",
              "                    view_138: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(view_137, [-1, 64, 256]);  view_137 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_40: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 768]\" = torch.ops.aten.linear.default(view_138, p_layers_2_blocks_2_attn_qkv_weight, p_layers_2_blocks_2_attn_qkv_bias);  view_138 = p_layers_2_blocks_2_attn_qkv_weight = p_layers_2_blocks_2_attn_qkv_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:42 in forward, code: qkv=self.qkv(x).view(B_, N, 3, self.n_heads,C//self.n_heads)\n",
              "                    view_139: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 3, 8, 32]\" = torch.ops.aten.view.default(linear_40, [floordiv_391, 64, 3, 8, 32]);  linear_40 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:43 in forward, code: q, k, v=qkv.permute(2,0,3,1,4).contiguous()\n",
              "                    permute_56: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.permute.default(view_139, [2, 0, 3, 1, 4]);  view_139 = None\n",
              "                    clone_96: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.clone.default(permute_56, memory_format = torch.contiguous_format);  permute_56 = None\n",
              "                    unbind_10 = torch.ops.aten.unbind.int(clone_96);  clone_96 = None\n",
              "                    getitem_30: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_10[0]\n",
              "                    getitem_31: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_10[1]\n",
              "                    getitem_32: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_10[2];  unbind_10 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:44 in forward, code: attn=(q@k.transpose(-2, -1))*self.scale\n",
              "                    transpose_10: \"f32[((s0//8))*((s53//8)), 8, 32, ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.transpose.int(getitem_31, -2, -1);  getitem_31 = None\n",
              "                    matmul_20: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.matmul.default(getitem_30, transpose_10);  getitem_30 = transpose_10 = None\n",
              "                    mul_3290: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.mul.Tensor(matmul_20, 0.1767766952966369);  matmul_20 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:51 in forward, code: attn=attn.softmax(dim=-1)\n",
              "                    softmax_10: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.softmax.int(mul_3290, -1);  mul_3290 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:52 in forward, code: attn=(attn@v).permute(0,2,1,3).contiguous().view(B_, N, C)\n",
              "                    matmul_21: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.matmul.default(softmax_10, getitem_32);  softmax_10 = getitem_32 = None\n",
              "                    permute_57: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.permute.default(matmul_21, [0, 2, 1, 3]);  matmul_21 = None\n",
              "                    clone_97: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.clone.default(permute_57, memory_format = torch.contiguous_format);  permute_57 = None\n",
              "                    view_140: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(clone_97, [floordiv_391, 64, 256]);  clone_97 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_41: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.linear.default(view_140, p_layers_2_blocks_2_attn_proj_weight, p_layers_2_blocks_2_attn_proj_bias);  view_140 = p_layers_2_blocks_2_attn_proj_weight = p_layers_2_blocks_2_attn_proj_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:103 in forward, code: x_attn=x_attn.view(-1,self.window_size, self.window_size, C)\n",
              "                    view_141: \"f32[((s0//8))*((s53//8)), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(linear_41, [-1, 8, 8, 256]);  linear_41 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:104 in forward, code: x=window_reverse(x_attn, self.window_size, H, W)   # (B, H, W ,C)\n",
              "                    view_142: \"f32[1, (s53//8), (s0//8), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(view_141, [1, floordiv_388, floordiv_389, 8, 8, -1]);  view_141 = None\n",
              "                    permute_58: \"f32[1, (s53//8), 8, (s0//8), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.permute.default(view_142, [0, 1, 3, 2, 4, 5]);  view_142 = None\n",
              "                    clone_98: \"f32[1, (s53//8), 8, (s0//8), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.clone.default(permute_58, memory_format = torch.contiguous_format);  permute_58 = None\n",
              "                    view_143: \"f32[1, 8*((s53//8)), ((s0//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(clone_98, [1, sym_size_int_318, sym_size_int_319, -1]);  clone_98 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:109 in forward, code: x=x.view(-1, H*W, C)\n",
              "                    view_144: \"f32[1, 8*((s0//8))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(view_143, [-1, mul_471, 256]);  view_143 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:110 in forward, code: x=shortcut+self.drop_path1(x)\n",
              "                    add_3048: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_2901, view_144);  add_2901 = view_144 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_21: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_3048, [256], p_layers_2_blocks_2_norm2_weight, p_layers_2_blocks_2_norm2_bias);  p_layers_2_blocks_2_norm2_weight = p_layers_2_blocks_2_norm2_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_42: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.linear.default(layer_norm_21, p_layers_2_blocks_2_mlp_0_weight, p_layers_2_blocks_2_mlp_0_bias);  layer_norm_21 = p_layers_2_blocks_2_mlp_0_weight = p_layers_2_blocks_2_mlp_0_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
              "                    gelu_10: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.gelu.default(linear_42);  linear_42 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_43: \"f32[1, s0*s53, 256]\" = torch.ops.aten.linear.default(gelu_10, p_layers_2_blocks_2_mlp_2_weight, p_layers_2_blocks_2_mlp_2_bias);  gelu_10 = p_layers_2_blocks_2_mlp_2_weight = p_layers_2_blocks_2_mlp_2_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:111 in forward, code: x=x+self.drop_path2(self.mlp(self.norm2(x)))\n",
              "                    add_3064: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_3048, linear_43);  add_3048 = linear_43 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_22: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_3064, [256], p_layers_2_blocks_3_norm1_weight, p_layers_2_blocks_3_norm1_bias);  p_layers_2_blocks_3_norm1_weight = p_layers_2_blocks_3_norm1_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:92 in forward, code: x=self.norm1(x).view(x.shape[0], H, W, -1)\n",
              "                    view_145: \"f32[1, s53, s0, 256]\" = torch.ops.aten.view.default(layer_norm_22, [sym_size_int_317, sym_size_int_318, sym_size_int_319, -1]);  layer_norm_22 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:95 in forward, code: x=torch.roll(x, shifts=(-self.shift_size, -self.shift_size),dims=(1,2))\n",
              "                    roll_10: \"f32[1, s53, s0, 256]\" = torch.ops.aten.roll.default(view_145, [-4, -4], [1, 2]);  view_145 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:97 in forward, code: x_windows=window_partition(x, self.window_size)\n",
              "                    view_146: \"f32[1, (s53//8), (s53//((s53//8))), (s0//8), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(roll_10, [sym_size_int_317, floordiv_388, 8, floordiv_389, 8, -1]);  roll_10 = None\n",
              "                    permute_59: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.permute.default(view_146, [0, 1, 3, 2, 4, 5]);  view_146 = None\n",
              "                    clone_99: \"f32[1, (s53//8), (s0//8), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.clone.default(permute_59, memory_format = torch.contiguous_format);  permute_59 = None\n",
              "                    view_147: \"f32[((s0//8))*((s53//8)), (s53//((s53//8))), (s0//((s0//8))), 256]\" = torch.ops.aten.view.default(clone_99, [-1, 8, 8, 256]);  clone_99 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:98 in forward, code: x_windows=x_windows.view(-1, self.window_size*self.window_size, C)    #(B_, N, C)\n",
              "                    view_148: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(view_147, [-1, 64, 256]);  view_147 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:100 in forward, code: attn_mask=get_attn_mask(self.window_size, self.shift_size, H, W, x.device) if self.shift_size>0 else None\n",
              "                    zeros_5: \"f32[1, s53, s0, 1]\" = torch.ops.aten.zeros.default([1, sym_size_int_318, sym_size_int_319, 1], device = device(type='cpu'), pin_memory = False)\n",
              "                    clone_100: \"f32[]\" = torch.ops.aten.clone.default(c_layers_2_blocks_3_lifted_tensor_45);  c_layers_2_blocks_3_lifted_tensor_45 = None\n",
              "                    slice_306: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(zeros_5, 1, 0, -8)\n",
              "                    slice_307: \"f32[1, s53 - 8, s0 - 8, 1]\" = torch.ops.aten.slice.Tensor(slice_306, 2, 0, -8);  slice_306 = None\n",
              "                    fill_45: \"f32[1, s53 - 8, s0 - 8, 1]\" = torch.ops.aten.fill.Tensor(slice_307, clone_100);  slice_307 = clone_100 = None\n",
              "                    slice_308: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(zeros_5, 1, 0, -8)\n",
              "                    slice_scatter_90: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_308, fill_45, 2, 0, -8);  slice_308 = fill_45 = None\n",
              "                    slice_scatter_91: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(zeros_5, slice_scatter_90, 1, 0, -8);  zeros_5 = slice_scatter_90 = None\n",
              "                    clone_101: \"f32[]\" = torch.ops.aten.clone.default(c_layers_2_blocks_3_lifted_tensor_46);  c_layers_2_blocks_3_lifted_tensor_46 = None\n",
              "                    slice_313: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_91, 1, 0, -8)\n",
              "                    slice_314: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_313, 2, -8, -4);  slice_313 = None\n",
              "                    fill_46: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_314, clone_101);  slice_314 = clone_101 = None\n",
              "                    slice_315: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_91, 1, 0, -8)\n",
              "                    slice_scatter_92: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_315, fill_46, 2, -8, -4);  slice_315 = fill_46 = None\n",
              "                    slice_scatter_93: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_91, slice_scatter_92, 1, 0, -8);  slice_scatter_91 = slice_scatter_92 = None\n",
              "                    clone_102: \"f32[]\" = torch.ops.aten.clone.default(c_layers_2_blocks_3_lifted_tensor_47);  c_layers_2_blocks_3_lifted_tensor_47 = None\n",
              "                    slice_320: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_93, 1, 0, -8)\n",
              "                    slice_321: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_320, 2, -4, 9223372036854775807);  slice_320 = None\n",
              "                    fill_47: \"f32[1, s53 - 8, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_321, clone_102);  slice_321 = clone_102 = None\n",
              "                    slice_322: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_93, 1, 0, -8)\n",
              "                    slice_scatter_94: \"f32[1, s53 - 8, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_322, fill_47, 2, -4, 9223372036854775807);  slice_322 = fill_47 = None\n",
              "                    slice_scatter_95: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_93, slice_scatter_94, 1, 0, -8);  slice_scatter_93 = slice_scatter_94 = None\n",
              "                    clone_103: \"f32[]\" = torch.ops.aten.clone.default(c_layers_2_blocks_3_lifted_tensor_48);  c_layers_2_blocks_3_lifted_tensor_48 = None\n",
              "                    slice_327: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_95, 1, -8, -4)\n",
              "                    slice_328: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.slice.Tensor(slice_327, 2, 0, -8);  slice_327 = None\n",
              "                    fill_48: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.fill.Tensor(slice_328, clone_103);  slice_328 = clone_103 = None\n",
              "                    slice_329: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_95, 1, -8, -4)\n",
              "                    slice_scatter_96: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_329, fill_48, 2, 0, -8);  slice_329 = fill_48 = None\n",
              "                    slice_scatter_97: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_95, slice_scatter_96, 1, -8, -4);  slice_scatter_95 = slice_scatter_96 = None\n",
              "                    clone_104: \"f32[]\" = torch.ops.aten.clone.default(c_layers_2_blocks_3_lifted_tensor_49);  c_layers_2_blocks_3_lifted_tensor_49 = None\n",
              "                    slice_334: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_97, 1, -8, -4)\n",
              "                    slice_335: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_334, 2, -8, -4);  slice_334 = None\n",
              "                    fill_49: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_335, clone_104);  slice_335 = clone_104 = None\n",
              "                    slice_336: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_97, 1, -8, -4)\n",
              "                    slice_scatter_98: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_336, fill_49, 2, -8, -4);  slice_336 = fill_49 = None\n",
              "                    slice_scatter_99: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_97, slice_scatter_98, 1, -8, -4);  slice_scatter_97 = slice_scatter_98 = None\n",
              "                    clone_105: \"f32[]\" = torch.ops.aten.clone.default(c_layers_2_blocks_3_lifted_tensor_50);  c_layers_2_blocks_3_lifted_tensor_50 = None\n",
              "                    slice_341: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_99, 1, -8, -4)\n",
              "                    slice_342: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_341, 2, -4, 9223372036854775807);  slice_341 = None\n",
              "                    fill_50: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_342, clone_105);  slice_342 = clone_105 = None\n",
              "                    slice_343: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_99, 1, -8, -4)\n",
              "                    slice_scatter_100: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_343, fill_50, 2, -4, 9223372036854775807);  slice_343 = fill_50 = None\n",
              "                    slice_scatter_101: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_99, slice_scatter_100, 1, -8, -4);  slice_scatter_99 = slice_scatter_100 = None\n",
              "                    clone_106: \"f32[]\" = torch.ops.aten.clone.default(c_layers_2_blocks_3_lifted_tensor_51);  c_layers_2_blocks_3_lifted_tensor_51 = None\n",
              "                    slice_348: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_101, 1, -4, 9223372036854775807)\n",
              "                    slice_349: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.slice.Tensor(slice_348, 2, 0, -8);  slice_348 = None\n",
              "                    fill_51: \"f32[1, 4, s0 - 8, 1]\" = torch.ops.aten.fill.Tensor(slice_349, clone_106);  slice_349 = clone_106 = None\n",
              "                    slice_350: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_101, 1, -4, 9223372036854775807)\n",
              "                    slice_scatter_102: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_350, fill_51, 2, 0, -8);  slice_350 = fill_51 = None\n",
              "                    slice_scatter_103: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_101, slice_scatter_102, 1, -4, 9223372036854775807);  slice_scatter_101 = slice_scatter_102 = None\n",
              "                    clone_107: \"f32[]\" = torch.ops.aten.clone.default(c_layers_2_blocks_3_lifted_tensor_52);  c_layers_2_blocks_3_lifted_tensor_52 = None\n",
              "                    slice_355: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_103, 1, -4, 9223372036854775807)\n",
              "                    slice_356: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_355, 2, -8, -4);  slice_355 = None\n",
              "                    fill_52: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_356, clone_107);  slice_356 = clone_107 = None\n",
              "                    slice_357: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_103, 1, -4, 9223372036854775807)\n",
              "                    slice_scatter_104: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_357, fill_52, 2, -8, -4);  slice_357 = fill_52 = None\n",
              "                    slice_scatter_105: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_103, slice_scatter_104, 1, -4, 9223372036854775807);  slice_scatter_103 = slice_scatter_104 = None\n",
              "                    clone_108: \"f32[]\" = torch.ops.aten.clone.default(c_layers_2_blocks_3_lifted_tensor_53);  c_layers_2_blocks_3_lifted_tensor_53 = None\n",
              "                    slice_362: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_105, 1, -4, 9223372036854775807)\n",
              "                    slice_363: \"f32[1, 4, 4, 1]\" = torch.ops.aten.slice.Tensor(slice_362, 2, -4, 9223372036854775807);  slice_362 = None\n",
              "                    fill_53: \"f32[1, 4, 4, 1]\" = torch.ops.aten.fill.Tensor(slice_363, clone_108);  slice_363 = clone_108 = None\n",
              "                    slice_364: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice.Tensor(slice_scatter_105, 1, -4, 9223372036854775807)\n",
              "                    slice_scatter_106: \"f32[1, 4, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_364, fill_53, 2, -4, 9223372036854775807);  slice_364 = fill_53 = None\n",
              "                    slice_scatter_107: \"f32[1, s53, s0, 1]\" = torch.ops.aten.slice_scatter.default(slice_scatter_105, slice_scatter_106, 1, -4, 9223372036854775807);  slice_scatter_105 = slice_scatter_106 = None\n",
              "                    view_150: \"f32[1, (s53//8), 8, (s0//8), 8, ((s0*s53)//(64*((s0//8))*((s53//8))))]\" = torch.ops.aten.view.default(slice_scatter_107, [1, floordiv_388, 8, floordiv_389, 8, -1]);  slice_scatter_107 = None\n",
              "                    permute_61: \"f32[1, (s53//8), (s0//8), 8, 8, ((s0*s53)//(64*((s0//8))*((s53//8))))]\" = torch.ops.aten.permute.default(view_150, [0, 1, 3, 2, 4, 5]);  view_150 = None\n",
              "                    clone_109: \"f32[1, (s53//8), (s0//8), 8, 8, ((s0*s53)//(64*((s0//8))*((s53//8))))]\" = torch.ops.aten.clone.default(permute_61, memory_format = torch.contiguous_format);  permute_61 = None\n",
              "                    view_151: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 8, 8, 1]\" = torch.ops.aten.view.default(clone_109, [-1, 8, 8, 1]);  clone_109 = None\n",
              "                    view_152: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64]\" = torch.ops.aten.view.default(view_151, [mul_846, -1]);  view_151 = None\n",
              "                    unsqueeze_20: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 1, 64]\" = torch.ops.aten.unsqueeze.default(view_152, 1)\n",
              "                    unsqueeze_21: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 1]\" = torch.ops.aten.unsqueeze.default(view_152, 2);  view_152 = None\n",
              "                    sub_1273: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.sub.Tensor(unsqueeze_20, unsqueeze_21);  unsqueeze_20 = unsqueeze_21 = None\n",
              "                    scalar_tensor_default_10: \"f32[]\" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32)\n",
              "                    ne_162: \"b8[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.ne.Tensor(sub_1273, scalar_tensor_default_10);  scalar_tensor_default_10 = None\n",
              "                    masked_fill_10: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.masked_fill.Scalar(sub_1273, ne_162, -100.0);  ne_162 = None\n",
              "                    scalar_tensor_default_11: \"f32[]\" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32)\n",
              "                    eq_1941: \"b8[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.eq.Tensor(sub_1273, scalar_tensor_default_11);  sub_1273 = scalar_tensor_default_11 = None\n",
              "                    masked_fill_11: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 64, 64]\" = torch.ops.aten.masked_fill.Scalar(masked_fill_10, eq_1941, 0.0);  masked_fill_10 = eq_1941 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_44: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 768]\" = torch.ops.aten.linear.default(view_148, p_layers_2_blocks_3_attn_qkv_weight, p_layers_2_blocks_3_attn_qkv_bias);  view_148 = p_layers_2_blocks_3_attn_qkv_weight = p_layers_2_blocks_3_attn_qkv_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:42 in forward, code: qkv=self.qkv(x).view(B_, N, 3, self.n_heads,C//self.n_heads)\n",
              "                    view_153: \"f32[((s0//8))*((s53//8)), ((s0//((s0//8))))*((s53//((s53//8)))), 3, 8, 32]\" = torch.ops.aten.view.default(linear_44, [floordiv_391, 64, 3, 8, 32]);  linear_44 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:43 in forward, code: q, k, v=qkv.permute(2,0,3,1,4).contiguous()\n",
              "                    permute_62: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.permute.default(view_153, [2, 0, 3, 1, 4]);  view_153 = None\n",
              "                    clone_110: \"f32[3, ((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.clone.default(permute_62, memory_format = torch.contiguous_format);  permute_62 = None\n",
              "                    unbind_11 = torch.ops.aten.unbind.int(clone_110);  clone_110 = None\n",
              "                    getitem_33: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_11[0]\n",
              "                    getitem_34: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_11[1]\n",
              "                    getitem_35: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = unbind_11[2];  unbind_11 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:44 in forward, code: attn=(q@k.transpose(-2, -1))*self.scale\n",
              "                    transpose_11: \"f32[((s0//8))*((s53//8)), 8, 32, ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.transpose.int(getitem_34, -2, -1);  getitem_34 = None\n",
              "                    matmul_22: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.matmul.default(getitem_33, transpose_11);  getitem_33 = transpose_11 = None\n",
              "                    mul_3621: \"f32[((s0//8))*((s53//8)), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.mul.Tensor(matmul_22, 0.1767766952966369);  matmul_22 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:48 in forward, code: attn=attn.view(B_//nW, nW, self.n_heads, N, N)\n",
              "                    view_154: \"f32[((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))), ((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.view.default(mul_3621, [floordiv_393, mul_846, 8, 64, 64]);  mul_3621 = floordiv_393 = mul_846 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:49 in forward, code: attn=attn+mask.unsqueeze(1).unsqueeze(0)\n",
              "                    unsqueeze_22: \"f32[((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 1, 64, 64]\" = torch.ops.aten.unsqueeze.default(masked_fill_11, 1);  masked_fill_11 = None\n",
              "                    unsqueeze_23: \"f32[1, ((s0//8))*((s53//8))*(((s0*s53)//(64*((s0//8))*((s53//8))))), 1, 64, 64]\" = torch.ops.aten.unsqueeze.default(unsqueeze_22, 0);  unsqueeze_22 = None\n",
              "                    add_3375: \"f32[1, ((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.add.Tensor(view_154, unsqueeze_23);  view_154 = unsqueeze_23 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:50 in forward, code: attn=attn.view(B_,self.n_heads,N,N)\n",
              "                    view_155: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.view.default(add_3375, [floordiv_391, 8, 64, 64]);  add_3375 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:51 in forward, code: attn=attn.softmax(dim=-1)\n",
              "                    softmax_11: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), ((s0//((s0//8))))*((s53//((s53//8))))]\" = torch.ops.aten.softmax.int(view_155, -1);  view_155 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:52 in forward, code: attn=(attn@v).permute(0,2,1,3).contiguous().view(B_, N, C)\n",
              "                    matmul_23: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((s0//((s0//8))))*((s53//((s53//8)))), 32]\" = torch.ops.aten.matmul.default(softmax_11, getitem_35);  softmax_11 = getitem_35 = None\n",
              "                    permute_63: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.permute.default(matmul_23, [0, 2, 1, 3]);  matmul_23 = None\n",
              "                    clone_111: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 8, 32]\" = torch.ops.aten.clone.default(permute_63, memory_format = torch.contiguous_format);  permute_63 = None\n",
              "                    view_156: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.view.default(clone_111, [floordiv_391, 64, 256]);  clone_111 = floordiv_391 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_45: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), ((s0//((s0//8))))*((s53//((s53//8)))), 256]\" = torch.ops.aten.linear.default(view_156, p_layers_2_blocks_3_attn_proj_weight, p_layers_2_blocks_3_attn_proj_bias);  view_156 = p_layers_2_blocks_3_attn_proj_weight = p_layers_2_blocks_3_attn_proj_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:103 in forward, code: x_attn=x_attn.view(-1,self.window_size, self.window_size, C)\n",
              "                    view_157: \"f32[((((s0//8))*((s53//8)))//(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8))))))))), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(linear_45, [-1, 8, 8, 256]);  linear_45 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:104 in forward, code: x=window_reverse(x_attn, self.window_size, H, W)   # (B, H, W ,C)\n",
              "                    view_158: \"f32[1, (s53//8), (s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))), 8, ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.view.default(view_157, [1, floordiv_388, floordiv_389, 8, 8, -1]);  view_157 = floordiv_388 = floordiv_389 = None\n",
              "                    permute_64: \"f32[1, (s53//8), 8, (s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.permute.default(view_158, [0, 1, 3, 2, 4, 5]);  view_158 = None\n",
              "                    clone_112: \"f32[1, (s53//8), 8, (s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))), ((((s0//((s0//8))))*((s53//((s53//8)))))//8), 256]\" = torch.ops.aten.clone.default(permute_64, memory_format = torch.contiguous_format);  permute_64 = None\n",
              "                    view_159: \"f32[1, 8*((s53//8)), ((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(clone_112, [1, sym_size_int_318, sym_size_int_319, -1]);  clone_112 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:107 in forward, code: x=torch.roll(x, shifts=(self.shift_size, self.shift_size),dims=(1,2))\n",
              "                    roll_11: \"f32[1, 8*((s53//8)), ((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.roll.default(view_159, [4, 4], [1, 2]);  view_159 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:109 in forward, code: x=x.view(-1, H*W, C)\n",
              "                    view_160: \"f32[1, 8*((s0//(8*(((s77*(((4*s0*s53)//(((s0//8))*((s53//8))))))//(256*(((s0*s53)//(64*((s0//8))*((s53//8)))))))))))*((s53//8))*(((((s0//((s0//8))))*((s53//((s53//8)))))//8)), 256]\" = torch.ops.aten.view.default(roll_11, [-1, mul_471, 256]);  roll_11 = mul_471 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:110 in forward, code: x=shortcut+self.drop_path1(x)\n",
              "                    add_3448: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_3064, view_160);  add_3064 = view_160 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_23: \"f32[1, s0*s53, 256]\" = torch.ops.aten.layer_norm.default(add_3448, [256], p_layers_2_blocks_3_norm2_weight, p_layers_2_blocks_3_norm2_bias);  p_layers_2_blocks_3_norm2_weight = p_layers_2_blocks_3_norm2_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_46: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.linear.default(layer_norm_23, p_layers_2_blocks_3_mlp_0_weight, p_layers_2_blocks_3_mlp_0_bias);  layer_norm_23 = p_layers_2_blocks_3_mlp_0_weight = p_layers_2_blocks_3_mlp_0_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
              "                    gelu_11: \"f32[1, s0*s53, 1024]\" = torch.ops.aten.gelu.default(linear_46);  linear_46 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
              "                    linear_47: \"f32[1, s0*s53, 256]\" = torch.ops.aten.linear.default(gelu_11, p_layers_2_blocks_3_mlp_2_weight, p_layers_2_blocks_3_mlp_2_bias);  gelu_11 = p_layers_2_blocks_3_mlp_2_weight = p_layers_2_blocks_3_mlp_2_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:111 in forward, code: x=x+self.drop_path2(self.mlp(self.norm2(x)))\n",
              "                    add_3464: \"f32[1, s0*s53, 256]\" = torch.ops.aten.add.Tensor(add_3448, linear_47);  add_3448 = linear_47 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:135 in forward, code: x=x.view(B, H, W, C).permute(0,3,1,2).contiguous()\n",
              "                    view_161: \"f32[1, s53, s0, 256]\" = torch.ops.aten.view.default(add_3464, [sym_size_int_317, sym_size_int_318, sym_size_int_319, 256]);  add_3464 = sym_size_int_317 = sym_size_int_318 = sym_size_int_319 = None\n",
              "                    permute_65: \"f32[1, 256, s53, s0]\" = torch.ops.aten.permute.default(view_161, [0, 3, 1, 2]);  view_161 = None\n",
              "                    clone_113: \"f32[1, 256, s53, s0]\" = torch.ops.aten.clone.default(permute_65, memory_format = torch.contiguous_format);  permute_65 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
              "                    conv2d_3: \"f32[1, 256, s53, s0]\" = torch.ops.aten.conv2d.default(clone_113, p_layers_2_conv_weight, p_layers_2_conv_bias, [1, 1], [1, 1]);  clone_113 = p_layers_2_conv_weight = p_layers_2_conv_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:137 in forward, code: x=x.permute(0,2,3,1).contiguous()\n",
              "                    permute_66: \"f32[1, s53, s0, 256]\" = torch.ops.aten.permute.default(conv2d_3, [0, 2, 3, 1]);  conv2d_3 = None\n",
              "                    clone_114: \"f32[1, s53, s0, 256]\" = torch.ops.aten.clone.default(permute_66, memory_format = torch.contiguous_format);  permute_66 = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:138 in forward, code: x=shortcut+x\n",
              "                    add_3491: \"f32[1, s53, s0, 256]\" = torch.ops.aten.add.Tensor(add_2334, clone_114);  add_2334 = clone_114 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
              "                    layer_norm_24: \"f32[1, s53, s0, 256]\" = torch.ops.aten.layer_norm.default(add_3491, [256], p_norm_weight, p_norm_bias);  add_3491 = p_norm_weight = p_norm_bias = None\n",
              "            \n",
              "                     # File: /tmp/ipython-input-687436260.py:170 in forward, code: x=x.permute(0,3,1,2).contiguous()\n",
              "                    permute_67: \"f32[1, 256, s53, s0]\" = torch.ops.aten.permute.default(layer_norm_24, [0, 3, 1, 2]);  layer_norm_24 = None\n",
              "                    clone_115: \"f32[1, 256, s53, s0]\" = torch.ops.aten.clone.default(permute_67, memory_format = torch.contiguous_format);  permute_67 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
              "                    conv2d_4: \"f32[1, 256, s53, s0]\" = torch.ops.aten.conv2d.default(clone_115, p_conv_after_body_weight, p_conv_after_body_bias, [1, 1], [1, 1]);  clone_115 = p_conv_after_body_weight = p_conv_after_body_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
              "                    conv2d_5: \"f32[1, 1024, s53, s0]\" = torch.ops.aten.conv2d.default(conv2d_4, p_upsample_0_weight, p_upsample_0_bias, [1, 1], [1, 1]);  conv2d_4 = p_upsample_0_weight = p_upsample_0_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/pixelshuffle.py:62 in forward, code: return F.pixel_shuffle(input, self.upscale_factor)\n",
              "                    pixel_shuffle: \"f32[1, 256, 2*s53, 2*s0]\" = torch.ops.aten.pixel_shuffle.default(conv2d_5, 2);  conv2d_5 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
              "                    conv2d_6: \"f32[1, 1024, 2*s53, 2*s0]\" = torch.ops.aten.conv2d.default(pixel_shuffle, p_upsample_2_weight, p_upsample_2_bias, [1, 1], [1, 1]);  pixel_shuffle = p_upsample_2_weight = p_upsample_2_bias = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/pixelshuffle.py:62 in forward, code: return F.pixel_shuffle(input, self.upscale_factor)\n",
              "                    pixel_shuffle_1: \"f32[1, 256, 4*s53, 4*s0]\" = torch.ops.aten.pixel_shuffle.default(conv2d_6, 2);  conv2d_6 = None\n",
              "            \n",
              "                     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
              "                    conv2d_7: \"f32[1, 3, 4*s53, 4*s0]\" = torch.ops.aten.conv2d.default(pixel_shuffle_1, p_upsample_4_weight, p_upsample_4_bias, [1, 1], [1, 1]);  pixel_shuffle_1 = p_upsample_4_weight = p_upsample_4_bias = None\n",
              "                    return (conv2d_7,)\n",
              "            \n",
              "        Graph signature: \n",
              "            # inputs\n",
              "            p_conv_first_weight: PARAMETER target='conv_first.weight'\n",
              "            p_conv_first_bias: PARAMETER target='conv_first.bias'\n",
              "            p_layers_0_blocks_0_norm1_weight: PARAMETER target='layers.0.blocks.0.norm1.weight'\n",
              "            p_layers_0_blocks_0_norm1_bias: PARAMETER target='layers.0.blocks.0.norm1.bias'\n",
              "            p_layers_0_blocks_0_attn_qkv_weight: PARAMETER target='layers.0.blocks.0.attn.qkv.weight'\n",
              "            p_layers_0_blocks_0_attn_qkv_bias: PARAMETER target='layers.0.blocks.0.attn.qkv.bias'\n",
              "            p_layers_0_blocks_0_attn_proj_weight: PARAMETER target='layers.0.blocks.0.attn.proj.weight'\n",
              "            p_layers_0_blocks_0_attn_proj_bias: PARAMETER target='layers.0.blocks.0.attn.proj.bias'\n",
              "            p_layers_0_blocks_0_norm2_weight: PARAMETER target='layers.0.blocks.0.norm2.weight'\n",
              "            p_layers_0_blocks_0_norm2_bias: PARAMETER target='layers.0.blocks.0.norm2.bias'\n",
              "            p_layers_0_blocks_0_mlp_0_weight: PARAMETER target='layers.0.blocks.0.mlp.0.weight'\n",
              "            p_layers_0_blocks_0_mlp_0_bias: PARAMETER target='layers.0.blocks.0.mlp.0.bias'\n",
              "            p_layers_0_blocks_0_mlp_2_weight: PARAMETER target='layers.0.blocks.0.mlp.2.weight'\n",
              "            p_layers_0_blocks_0_mlp_2_bias: PARAMETER target='layers.0.blocks.0.mlp.2.bias'\n",
              "            p_layers_0_blocks_1_norm1_weight: PARAMETER target='layers.0.blocks.1.norm1.weight'\n",
              "            p_layers_0_blocks_1_norm1_bias: PARAMETER target='layers.0.blocks.1.norm1.bias'\n",
              "            p_layers_0_blocks_1_attn_qkv_weight: PARAMETER target='layers.0.blocks.1.attn.qkv.weight'\n",
              "            p_layers_0_blocks_1_attn_qkv_bias: PARAMETER target='layers.0.blocks.1.attn.qkv.bias'\n",
              "            p_layers_0_blocks_1_attn_proj_weight: PARAMETER target='layers.0.blocks.1.attn.proj.weight'\n",
              "            p_layers_0_blocks_1_attn_proj_bias: PARAMETER target='layers.0.blocks.1.attn.proj.bias'\n",
              "            p_layers_0_blocks_1_norm2_weight: PARAMETER target='layers.0.blocks.1.norm2.weight'\n",
              "            p_layers_0_blocks_1_norm2_bias: PARAMETER target='layers.0.blocks.1.norm2.bias'\n",
              "            p_layers_0_blocks_1_mlp_0_weight: PARAMETER target='layers.0.blocks.1.mlp.0.weight'\n",
              "            p_layers_0_blocks_1_mlp_0_bias: PARAMETER target='layers.0.blocks.1.mlp.0.bias'\n",
              "            p_layers_0_blocks_1_mlp_2_weight: PARAMETER target='layers.0.blocks.1.mlp.2.weight'\n",
              "            p_layers_0_blocks_1_mlp_2_bias: PARAMETER target='layers.0.blocks.1.mlp.2.bias'\n",
              "            p_layers_0_blocks_2_norm1_weight: PARAMETER target='layers.0.blocks.2.norm1.weight'\n",
              "            p_layers_0_blocks_2_norm1_bias: PARAMETER target='layers.0.blocks.2.norm1.bias'\n",
              "            p_layers_0_blocks_2_attn_qkv_weight: PARAMETER target='layers.0.blocks.2.attn.qkv.weight'\n",
              "            p_layers_0_blocks_2_attn_qkv_bias: PARAMETER target='layers.0.blocks.2.attn.qkv.bias'\n",
              "            p_layers_0_blocks_2_attn_proj_weight: PARAMETER target='layers.0.blocks.2.attn.proj.weight'\n",
              "            p_layers_0_blocks_2_attn_proj_bias: PARAMETER target='layers.0.blocks.2.attn.proj.bias'\n",
              "            p_layers_0_blocks_2_norm2_weight: PARAMETER target='layers.0.blocks.2.norm2.weight'\n",
              "            p_layers_0_blocks_2_norm2_bias: PARAMETER target='layers.0.blocks.2.norm2.bias'\n",
              "            p_layers_0_blocks_2_mlp_0_weight: PARAMETER target='layers.0.blocks.2.mlp.0.weight'\n",
              "            p_layers_0_blocks_2_mlp_0_bias: PARAMETER target='layers.0.blocks.2.mlp.0.bias'\n",
              "            p_layers_0_blocks_2_mlp_2_weight: PARAMETER target='layers.0.blocks.2.mlp.2.weight'\n",
              "            p_layers_0_blocks_2_mlp_2_bias: PARAMETER target='layers.0.blocks.2.mlp.2.bias'\n",
              "            p_layers_0_blocks_3_norm1_weight: PARAMETER target='layers.0.blocks.3.norm1.weight'\n",
              "            p_layers_0_blocks_3_norm1_bias: PARAMETER target='layers.0.blocks.3.norm1.bias'\n",
              "            p_layers_0_blocks_3_attn_qkv_weight: PARAMETER target='layers.0.blocks.3.attn.qkv.weight'\n",
              "            p_layers_0_blocks_3_attn_qkv_bias: PARAMETER target='layers.0.blocks.3.attn.qkv.bias'\n",
              "            p_layers_0_blocks_3_attn_proj_weight: PARAMETER target='layers.0.blocks.3.attn.proj.weight'\n",
              "            p_layers_0_blocks_3_attn_proj_bias: PARAMETER target='layers.0.blocks.3.attn.proj.bias'\n",
              "            p_layers_0_blocks_3_norm2_weight: PARAMETER target='layers.0.blocks.3.norm2.weight'\n",
              "            p_layers_0_blocks_3_norm2_bias: PARAMETER target='layers.0.blocks.3.norm2.bias'\n",
              "            p_layers_0_blocks_3_mlp_0_weight: PARAMETER target='layers.0.blocks.3.mlp.0.weight'\n",
              "            p_layers_0_blocks_3_mlp_0_bias: PARAMETER target='layers.0.blocks.3.mlp.0.bias'\n",
              "            p_layers_0_blocks_3_mlp_2_weight: PARAMETER target='layers.0.blocks.3.mlp.2.weight'\n",
              "            p_layers_0_blocks_3_mlp_2_bias: PARAMETER target='layers.0.blocks.3.mlp.2.bias'\n",
              "            p_layers_0_conv_weight: PARAMETER target='layers.0.conv.weight'\n",
              "            p_layers_0_conv_bias: PARAMETER target='layers.0.conv.bias'\n",
              "            p_layers_1_blocks_0_norm1_weight: PARAMETER target='layers.1.blocks.0.norm1.weight'\n",
              "            p_layers_1_blocks_0_norm1_bias: PARAMETER target='layers.1.blocks.0.norm1.bias'\n",
              "            p_layers_1_blocks_0_attn_qkv_weight: PARAMETER target='layers.1.blocks.0.attn.qkv.weight'\n",
              "            p_layers_1_blocks_0_attn_qkv_bias: PARAMETER target='layers.1.blocks.0.attn.qkv.bias'\n",
              "            p_layers_1_blocks_0_attn_proj_weight: PARAMETER target='layers.1.blocks.0.attn.proj.weight'\n",
              "            p_layers_1_blocks_0_attn_proj_bias: PARAMETER target='layers.1.blocks.0.attn.proj.bias'\n",
              "            p_layers_1_blocks_0_norm2_weight: PARAMETER target='layers.1.blocks.0.norm2.weight'\n",
              "            p_layers_1_blocks_0_norm2_bias: PARAMETER target='layers.1.blocks.0.norm2.bias'\n",
              "            p_layers_1_blocks_0_mlp_0_weight: PARAMETER target='layers.1.blocks.0.mlp.0.weight'\n",
              "            p_layers_1_blocks_0_mlp_0_bias: PARAMETER target='layers.1.blocks.0.mlp.0.bias'\n",
              "            p_layers_1_blocks_0_mlp_2_weight: PARAMETER target='layers.1.blocks.0.mlp.2.weight'\n",
              "            p_layers_1_blocks_0_mlp_2_bias: PARAMETER target='layers.1.blocks.0.mlp.2.bias'\n",
              "            p_layers_1_blocks_1_norm1_weight: PARAMETER target='layers.1.blocks.1.norm1.weight'\n",
              "            p_layers_1_blocks_1_norm1_bias: PARAMETER target='layers.1.blocks.1.norm1.bias'\n",
              "            p_layers_1_blocks_1_attn_qkv_weight: PARAMETER target='layers.1.blocks.1.attn.qkv.weight'\n",
              "            p_layers_1_blocks_1_attn_qkv_bias: PARAMETER target='layers.1.blocks.1.attn.qkv.bias'\n",
              "            p_layers_1_blocks_1_attn_proj_weight: PARAMETER target='layers.1.blocks.1.attn.proj.weight'\n",
              "            p_layers_1_blocks_1_attn_proj_bias: PARAMETER target='layers.1.blocks.1.attn.proj.bias'\n",
              "            p_layers_1_blocks_1_norm2_weight: PARAMETER target='layers.1.blocks.1.norm2.weight'\n",
              "            p_layers_1_blocks_1_norm2_bias: PARAMETER target='layers.1.blocks.1.norm2.bias'\n",
              "            p_layers_1_blocks_1_mlp_0_weight: PARAMETER target='layers.1.blocks.1.mlp.0.weight'\n",
              "            p_layers_1_blocks_1_mlp_0_bias: PARAMETER target='layers.1.blocks.1.mlp.0.bias'\n",
              "            p_layers_1_blocks_1_mlp_2_weight: PARAMETER target='layers.1.blocks.1.mlp.2.weight'\n",
              "            p_layers_1_blocks_1_mlp_2_bias: PARAMETER target='layers.1.blocks.1.mlp.2.bias'\n",
              "            p_layers_1_blocks_2_norm1_weight: PARAMETER target='layers.1.blocks.2.norm1.weight'\n",
              "            p_layers_1_blocks_2_norm1_bias: PARAMETER target='layers.1.blocks.2.norm1.bias'\n",
              "            p_layers_1_blocks_2_attn_qkv_weight: PARAMETER target='layers.1.blocks.2.attn.qkv.weight'\n",
              "            p_layers_1_blocks_2_attn_qkv_bias: PARAMETER target='layers.1.blocks.2.attn.qkv.bias'\n",
              "            p_layers_1_blocks_2_attn_proj_weight: PARAMETER target='layers.1.blocks.2.attn.proj.weight'\n",
              "            p_layers_1_blocks_2_attn_proj_bias: PARAMETER target='layers.1.blocks.2.attn.proj.bias'\n",
              "            p_layers_1_blocks_2_norm2_weight: PARAMETER target='layers.1.blocks.2.norm2.weight'\n",
              "            p_layers_1_blocks_2_norm2_bias: PARAMETER target='layers.1.blocks.2.norm2.bias'\n",
              "            p_layers_1_blocks_2_mlp_0_weight: PARAMETER target='layers.1.blocks.2.mlp.0.weight'\n",
              "            p_layers_1_blocks_2_mlp_0_bias: PARAMETER target='layers.1.blocks.2.mlp.0.bias'\n",
              "            p_layers_1_blocks_2_mlp_2_weight: PARAMETER target='layers.1.blocks.2.mlp.2.weight'\n",
              "            p_layers_1_blocks_2_mlp_2_bias: PARAMETER target='layers.1.blocks.2.mlp.2.bias'\n",
              "            p_layers_1_blocks_3_norm1_weight: PARAMETER target='layers.1.blocks.3.norm1.weight'\n",
              "            p_layers_1_blocks_3_norm1_bias: PARAMETER target='layers.1.blocks.3.norm1.bias'\n",
              "            p_layers_1_blocks_3_attn_qkv_weight: PARAMETER target='layers.1.blocks.3.attn.qkv.weight'\n",
              "            p_layers_1_blocks_3_attn_qkv_bias: PARAMETER target='layers.1.blocks.3.attn.qkv.bias'\n",
              "            p_layers_1_blocks_3_attn_proj_weight: PARAMETER target='layers.1.blocks.3.attn.proj.weight'\n",
              "            p_layers_1_blocks_3_attn_proj_bias: PARAMETER target='layers.1.blocks.3.attn.proj.bias'\n",
              "            p_layers_1_blocks_3_norm2_weight: PARAMETER target='layers.1.blocks.3.norm2.weight'\n",
              "            p_layers_1_blocks_3_norm2_bias: PARAMETER target='layers.1.blocks.3.norm2.bias'\n",
              "            p_layers_1_blocks_3_mlp_0_weight: PARAMETER target='layers.1.blocks.3.mlp.0.weight'\n",
              "            p_layers_1_blocks_3_mlp_0_bias: PARAMETER target='layers.1.blocks.3.mlp.0.bias'\n",
              "            p_layers_1_blocks_3_mlp_2_weight: PARAMETER target='layers.1.blocks.3.mlp.2.weight'\n",
              "            p_layers_1_blocks_3_mlp_2_bias: PARAMETER target='layers.1.blocks.3.mlp.2.bias'\n",
              "            p_layers_1_conv_weight: PARAMETER target='layers.1.conv.weight'\n",
              "            p_layers_1_conv_bias: PARAMETER target='layers.1.conv.bias'\n",
              "            p_layers_2_blocks_0_norm1_weight: PARAMETER target='layers.2.blocks.0.norm1.weight'\n",
              "            p_layers_2_blocks_0_norm1_bias: PARAMETER target='layers.2.blocks.0.norm1.bias'\n",
              "            p_layers_2_blocks_0_attn_qkv_weight: PARAMETER target='layers.2.blocks.0.attn.qkv.weight'\n",
              "            p_layers_2_blocks_0_attn_qkv_bias: PARAMETER target='layers.2.blocks.0.attn.qkv.bias'\n",
              "            p_layers_2_blocks_0_attn_proj_weight: PARAMETER target='layers.2.blocks.0.attn.proj.weight'\n",
              "            p_layers_2_blocks_0_attn_proj_bias: PARAMETER target='layers.2.blocks.0.attn.proj.bias'\n",
              "            p_layers_2_blocks_0_norm2_weight: PARAMETER target='layers.2.blocks.0.norm2.weight'\n",
              "            p_layers_2_blocks_0_norm2_bias: PARAMETER target='layers.2.blocks.0.norm2.bias'\n",
              "            p_layers_2_blocks_0_mlp_0_weight: PARAMETER target='layers.2.blocks.0.mlp.0.weight'\n",
              "            p_layers_2_blocks_0_mlp_0_bias: PARAMETER target='layers.2.blocks.0.mlp.0.bias'\n",
              "            p_layers_2_blocks_0_mlp_2_weight: PARAMETER target='layers.2.blocks.0.mlp.2.weight'\n",
              "            p_layers_2_blocks_0_mlp_2_bias: PARAMETER target='layers.2.blocks.0.mlp.2.bias'\n",
              "            p_layers_2_blocks_1_norm1_weight: PARAMETER target='layers.2.blocks.1.norm1.weight'\n",
              "            p_layers_2_blocks_1_norm1_bias: PARAMETER target='layers.2.blocks.1.norm1.bias'\n",
              "            p_layers_2_blocks_1_attn_qkv_weight: PARAMETER target='layers.2.blocks.1.attn.qkv.weight'\n",
              "            p_layers_2_blocks_1_attn_qkv_bias: PARAMETER target='layers.2.blocks.1.attn.qkv.bias'\n",
              "            p_layers_2_blocks_1_attn_proj_weight: PARAMETER target='layers.2.blocks.1.attn.proj.weight'\n",
              "            p_layers_2_blocks_1_attn_proj_bias: PARAMETER target='layers.2.blocks.1.attn.proj.bias'\n",
              "            p_layers_2_blocks_1_norm2_weight: PARAMETER target='layers.2.blocks.1.norm2.weight'\n",
              "            p_layers_2_blocks_1_norm2_bias: PARAMETER target='layers.2.blocks.1.norm2.bias'\n",
              "            p_layers_2_blocks_1_mlp_0_weight: PARAMETER target='layers.2.blocks.1.mlp.0.weight'\n",
              "            p_layers_2_blocks_1_mlp_0_bias: PARAMETER target='layers.2.blocks.1.mlp.0.bias'\n",
              "            p_layers_2_blocks_1_mlp_2_weight: PARAMETER target='layers.2.blocks.1.mlp.2.weight'\n",
              "            p_layers_2_blocks_1_mlp_2_bias: PARAMETER target='layers.2.blocks.1.mlp.2.bias'\n",
              "            p_layers_2_blocks_2_norm1_weight: PARAMETER target='layers.2.blocks.2.norm1.weight'\n",
              "            p_layers_2_blocks_2_norm1_bias: PARAMETER target='layers.2.blocks.2.norm1.bias'\n",
              "            p_layers_2_blocks_2_attn_qkv_weight: PARAMETER target='layers.2.blocks.2.attn.qkv.weight'\n",
              "            p_layers_2_blocks_2_attn_qkv_bias: PARAMETER target='layers.2.blocks.2.attn.qkv.bias'\n",
              "            p_layers_2_blocks_2_attn_proj_weight: PARAMETER target='layers.2.blocks.2.attn.proj.weight'\n",
              "            p_layers_2_blocks_2_attn_proj_bias: PARAMETER target='layers.2.blocks.2.attn.proj.bias'\n",
              "            p_layers_2_blocks_2_norm2_weight: PARAMETER target='layers.2.blocks.2.norm2.weight'\n",
              "            p_layers_2_blocks_2_norm2_bias: PARAMETER target='layers.2.blocks.2.norm2.bias'\n",
              "            p_layers_2_blocks_2_mlp_0_weight: PARAMETER target='layers.2.blocks.2.mlp.0.weight'\n",
              "            p_layers_2_blocks_2_mlp_0_bias: PARAMETER target='layers.2.blocks.2.mlp.0.bias'\n",
              "            p_layers_2_blocks_2_mlp_2_weight: PARAMETER target='layers.2.blocks.2.mlp.2.weight'\n",
              "            p_layers_2_blocks_2_mlp_2_bias: PARAMETER target='layers.2.blocks.2.mlp.2.bias'\n",
              "            p_layers_2_blocks_3_norm1_weight: PARAMETER target='layers.2.blocks.3.norm1.weight'\n",
              "            p_layers_2_blocks_3_norm1_bias: PARAMETER target='layers.2.blocks.3.norm1.bias'\n",
              "            p_layers_2_blocks_3_attn_qkv_weight: PARAMETER target='layers.2.blocks.3.attn.qkv.weight'\n",
              "            p_layers_2_blocks_3_attn_qkv_bias: PARAMETER target='layers.2.blocks.3.attn.qkv.bias'\n",
              "            p_layers_2_blocks_3_attn_proj_weight: PARAMETER target='layers.2.blocks.3.attn.proj.weight'\n",
              "            p_layers_2_blocks_3_attn_proj_bias: PARAMETER target='layers.2.blocks.3.attn.proj.bias'\n",
              "            p_layers_2_blocks_3_norm2_weight: PARAMETER target='layers.2.blocks.3.norm2.weight'\n",
              "            p_layers_2_blocks_3_norm2_bias: PARAMETER target='layers.2.blocks.3.norm2.bias'\n",
              "            p_layers_2_blocks_3_mlp_0_weight: PARAMETER target='layers.2.blocks.3.mlp.0.weight'\n",
              "            p_layers_2_blocks_3_mlp_0_bias: PARAMETER target='layers.2.blocks.3.mlp.0.bias'\n",
              "            p_layers_2_blocks_3_mlp_2_weight: PARAMETER target='layers.2.blocks.3.mlp.2.weight'\n",
              "            p_layers_2_blocks_3_mlp_2_bias: PARAMETER target='layers.2.blocks.3.mlp.2.bias'\n",
              "            p_layers_2_conv_weight: PARAMETER target='layers.2.conv.weight'\n",
              "            p_layers_2_conv_bias: PARAMETER target='layers.2.conv.bias'\n",
              "            p_norm_weight: PARAMETER target='norm.weight'\n",
              "            p_norm_bias: PARAMETER target='norm.bias'\n",
              "            p_conv_after_body_weight: PARAMETER target='conv_after_body.weight'\n",
              "            p_conv_after_body_bias: PARAMETER target='conv_after_body.bias'\n",
              "            p_upsample_0_weight: PARAMETER target='upsample.0.weight'\n",
              "            p_upsample_0_bias: PARAMETER target='upsample.0.bias'\n",
              "            p_upsample_2_weight: PARAMETER target='upsample.2.weight'\n",
              "            p_upsample_2_bias: PARAMETER target='upsample.2.bias'\n",
              "            p_upsample_4_weight: PARAMETER target='upsample.4.weight'\n",
              "            p_upsample_4_bias: PARAMETER target='upsample.4.bias'\n",
              "            c_layers_0_blocks_1_lifted_tensor_0: CONSTANT_TENSOR target='layers.0.blocks.1.lifted_tensor_0'\n",
              "            c_layers_0_blocks_1_lifted_tensor_1: CONSTANT_TENSOR target='layers.0.blocks.1.lifted_tensor_1'\n",
              "            c_layers_0_blocks_1_lifted_tensor_2: CONSTANT_TENSOR target='layers.0.blocks.1.lifted_tensor_2'\n",
              "            c_layers_0_blocks_1_lifted_tensor_3: CONSTANT_TENSOR target='layers.0.blocks.1.lifted_tensor_3'\n",
              "            c_layers_0_blocks_1_lifted_tensor_4: CONSTANT_TENSOR target='layers.0.blocks.1.lifted_tensor_4'\n",
              "            c_layers_0_blocks_1_lifted_tensor_5: CONSTANT_TENSOR target='layers.0.blocks.1.lifted_tensor_5'\n",
              "            c_layers_0_blocks_1_lifted_tensor_6: CONSTANT_TENSOR target='layers.0.blocks.1.lifted_tensor_6'\n",
              "            c_layers_0_blocks_1_lifted_tensor_7: CONSTANT_TENSOR target='layers.0.blocks.1.lifted_tensor_7'\n",
              "            c_layers_0_blocks_1_lifted_tensor_8: CONSTANT_TENSOR target='layers.0.blocks.1.lifted_tensor_8'\n",
              "            c_layers_0_blocks_3_lifted_tensor_9: CONSTANT_TENSOR target='layers.0.blocks.3.lifted_tensor_9'\n",
              "            c_layers_0_blocks_3_lifted_tensor_10: CONSTANT_TENSOR target='layers.0.blocks.3.lifted_tensor_10'\n",
              "            c_layers_0_blocks_3_lifted_tensor_11: CONSTANT_TENSOR target='layers.0.blocks.3.lifted_tensor_11'\n",
              "            c_layers_0_blocks_3_lifted_tensor_12: CONSTANT_TENSOR target='layers.0.blocks.3.lifted_tensor_12'\n",
              "            c_layers_0_blocks_3_lifted_tensor_13: CONSTANT_TENSOR target='layers.0.blocks.3.lifted_tensor_13'\n",
              "            c_layers_0_blocks_3_lifted_tensor_14: CONSTANT_TENSOR target='layers.0.blocks.3.lifted_tensor_14'\n",
              "            c_layers_0_blocks_3_lifted_tensor_15: CONSTANT_TENSOR target='layers.0.blocks.3.lifted_tensor_15'\n",
              "            c_layers_0_blocks_3_lifted_tensor_16: CONSTANT_TENSOR target='layers.0.blocks.3.lifted_tensor_16'\n",
              "            c_layers_0_blocks_3_lifted_tensor_17: CONSTANT_TENSOR target='layers.0.blocks.3.lifted_tensor_17'\n",
              "            c_layers_1_blocks_1_lifted_tensor_18: CONSTANT_TENSOR target='layers.1.blocks.1.lifted_tensor_18'\n",
              "            c_layers_1_blocks_1_lifted_tensor_19: CONSTANT_TENSOR target='layers.1.blocks.1.lifted_tensor_19'\n",
              "            c_layers_1_blocks_1_lifted_tensor_20: CONSTANT_TENSOR target='layers.1.blocks.1.lifted_tensor_20'\n",
              "            c_layers_1_blocks_1_lifted_tensor_21: CONSTANT_TENSOR target='layers.1.blocks.1.lifted_tensor_21'\n",
              "            c_layers_1_blocks_1_lifted_tensor_22: CONSTANT_TENSOR target='layers.1.blocks.1.lifted_tensor_22'\n",
              "            c_layers_1_blocks_1_lifted_tensor_23: CONSTANT_TENSOR target='layers.1.blocks.1.lifted_tensor_23'\n",
              "            c_layers_1_blocks_1_lifted_tensor_24: CONSTANT_TENSOR target='layers.1.blocks.1.lifted_tensor_24'\n",
              "            c_layers_1_blocks_1_lifted_tensor_25: CONSTANT_TENSOR target='layers.1.blocks.1.lifted_tensor_25'\n",
              "            c_layers_1_blocks_1_lifted_tensor_26: CONSTANT_TENSOR target='layers.1.blocks.1.lifted_tensor_26'\n",
              "            c_layers_1_blocks_3_lifted_tensor_27: CONSTANT_TENSOR target='layers.1.blocks.3.lifted_tensor_27'\n",
              "            c_layers_1_blocks_3_lifted_tensor_28: CONSTANT_TENSOR target='layers.1.blocks.3.lifted_tensor_28'\n",
              "            c_layers_1_blocks_3_lifted_tensor_29: CONSTANT_TENSOR target='layers.1.blocks.3.lifted_tensor_29'\n",
              "            c_layers_1_blocks_3_lifted_tensor_30: CONSTANT_TENSOR target='layers.1.blocks.3.lifted_tensor_30'\n",
              "            c_layers_1_blocks_3_lifted_tensor_31: CONSTANT_TENSOR target='layers.1.blocks.3.lifted_tensor_31'\n",
              "            c_layers_1_blocks_3_lifted_tensor_32: CONSTANT_TENSOR target='layers.1.blocks.3.lifted_tensor_32'\n",
              "            c_layers_1_blocks_3_lifted_tensor_33: CONSTANT_TENSOR target='layers.1.blocks.3.lifted_tensor_33'\n",
              "            c_layers_1_blocks_3_lifted_tensor_34: CONSTANT_TENSOR target='layers.1.blocks.3.lifted_tensor_34'\n",
              "            c_layers_1_blocks_3_lifted_tensor_35: CONSTANT_TENSOR target='layers.1.blocks.3.lifted_tensor_35'\n",
              "            c_layers_2_blocks_1_lifted_tensor_36: CONSTANT_TENSOR target='layers.2.blocks.1.lifted_tensor_36'\n",
              "            c_layers_2_blocks_1_lifted_tensor_37: CONSTANT_TENSOR target='layers.2.blocks.1.lifted_tensor_37'\n",
              "            c_layers_2_blocks_1_lifted_tensor_38: CONSTANT_TENSOR target='layers.2.blocks.1.lifted_tensor_38'\n",
              "            c_layers_2_blocks_1_lifted_tensor_39: CONSTANT_TENSOR target='layers.2.blocks.1.lifted_tensor_39'\n",
              "            c_layers_2_blocks_1_lifted_tensor_40: CONSTANT_TENSOR target='layers.2.blocks.1.lifted_tensor_40'\n",
              "            c_layers_2_blocks_1_lifted_tensor_41: CONSTANT_TENSOR target='layers.2.blocks.1.lifted_tensor_41'\n",
              "            c_layers_2_blocks_1_lifted_tensor_42: CONSTANT_TENSOR target='layers.2.blocks.1.lifted_tensor_42'\n",
              "            c_layers_2_blocks_1_lifted_tensor_43: CONSTANT_TENSOR target='layers.2.blocks.1.lifted_tensor_43'\n",
              "            c_layers_2_blocks_1_lifted_tensor_44: CONSTANT_TENSOR target='layers.2.blocks.1.lifted_tensor_44'\n",
              "            c_layers_2_blocks_3_lifted_tensor_45: CONSTANT_TENSOR target='layers.2.blocks.3.lifted_tensor_45'\n",
              "            c_layers_2_blocks_3_lifted_tensor_46: CONSTANT_TENSOR target='layers.2.blocks.3.lifted_tensor_46'\n",
              "            c_layers_2_blocks_3_lifted_tensor_47: CONSTANT_TENSOR target='layers.2.blocks.3.lifted_tensor_47'\n",
              "            c_layers_2_blocks_3_lifted_tensor_48: CONSTANT_TENSOR target='layers.2.blocks.3.lifted_tensor_48'\n",
              "            c_layers_2_blocks_3_lifted_tensor_49: CONSTANT_TENSOR target='layers.2.blocks.3.lifted_tensor_49'\n",
              "            c_layers_2_blocks_3_lifted_tensor_50: CONSTANT_TENSOR target='layers.2.blocks.3.lifted_tensor_50'\n",
              "            c_layers_2_blocks_3_lifted_tensor_51: CONSTANT_TENSOR target='layers.2.blocks.3.lifted_tensor_51'\n",
              "            c_layers_2_blocks_3_lifted_tensor_52: CONSTANT_TENSOR target='layers.2.blocks.3.lifted_tensor_52'\n",
              "            c_layers_2_blocks_3_lifted_tensor_53: CONSTANT_TENSOR target='layers.2.blocks.3.lifted_tensor_53'\n",
              "            x: USER_INPUT\n",
              "    \n",
              "            # outputs\n",
              "            conv2d_7: USER_OUTPUT\n",
              "    \n",
              "        Range constraints: {s77: VR[0, int_oo], s53: VR[8, int_oo], s0: VR[8, int_oo]}\n",
              "\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 경량화\n",
        "ORIGINAL_MODEL='swinir_best.pth'\n",
        "FP32_MODEL='swinir_x4.onnx'\n",
        "FP32_DATA='swinir_x4.onnx.data'\n",
        "INT8_MODEL='swinir_x4_int8.onnx'\n",
        "INT8_DATA='swinir_x4_int8.onnx.data'\n",
        "\n",
        "\n",
        "model_onnx=onnx.load(FP32_MODEL)\n",
        "onnx.checker.check_model(model_onnx)\n",
        "\n",
        "# quantize_dynamic(\n",
        "#     model_input=FP32_MODEL,\n",
        "#     model_output=INT8_MODEL,\n",
        "#     weight_type=QuantType.QInt8\n",
        "# )\n",
        "quantize_dynamic(\n",
        "    model_input=FP32_MODEL,\n",
        "    model_output=INT8_MODEL,\n",
        "    weight_type=QuantType.QInt8,\n",
        "    op_types_to_quantize=[\"MatMul\", \"Gemm\"],  # Conv는 빼고 양자화\n",
        ")\n",
        "\n",
        "def safe_size(path):\n",
        "  return os.path.getsize(path) if os.path.exists(path) else 0\n",
        "\n",
        "origin_size=safe_size(ORIGINAL_MODEL)/(1024*1024)\n",
        "fp32_size=(safe_size(FP32_MODEL)+safe_size(FP32_DATA))/(1024*1024)\n",
        "int8_size=(safe_size(INT8_MODEL)+safe_size(INT8_DATA))/(1024*1024)\n",
        "\n",
        "print(\"======= Result of Model size =======\")\n",
        "print(f\"Original File Size: {origin_size:.2f} MB\")\n",
        "print(f\"FP32: {fp32_size:.2f} MB\")\n",
        "print(f\"INT8: {int8_size:.2f} MB\")\n",
        "print(f\"압축율: {(1-int8_size/fp32_size)*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60RNJ0knA6qC",
        "outputId": "57fc1e45-4242-4a2b-f5cb-bdbaa2793d7a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Result of Model size =======\n",
            "Original File Size: 189.85 MB\n",
            "FP32: 64.17 MB\n",
            "INT8: 37.10 MB\n",
            "압축율: 42.19%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile engine.h\n",
        "#pragma once\n",
        "#include <string>\n",
        "#include <vector>\n",
        "#include <onnxruntime_cxx_api.h>\n",
        "\n",
        "class InferenceEngine {\n",
        "  public:\n",
        "    InferenceEngine(const std::string& model_path, int threads);\n",
        "    std::vector<float> infer(const std::vector<float>& input);\n",
        "\n",
        "  private:\n",
        "    Ort::Env env;\n",
        "    Ort::Session session;\n",
        "    Ort::MemoryInfo mem_info;\n",
        "    std::vector<int64_t> input_shape;\n",
        "};"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFRRLzFyA6lB",
        "outputId": "d2a27a23-28a1-4645-ab8b-e6aae7468b83"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing engine.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile engine.cpp\n",
        "#include \"engine.h\"\n",
        "\n",
        "InferenceEngine::InferenceEngine(\n",
        "    const std::string& model_path,\n",
        "    int threads\n",
        ") : env(ORT_LOGGING_LEVEL_WARNING, \"Engine\"),\n",
        "    session(nullptr),\n",
        "    mem_info(Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault))\n",
        "{\n",
        "    Ort::SessionOptions opts;\n",
        "    opts.SetIntraOpNumThreads(threads);\n",
        "    opts.SetInterOpNumThreads(threads);\n",
        "    opts.SetGraphOptimizationLevel(\n",
        "        GraphOptimizationLevel::ORT_ENABLE_ALL);\n",
        "\n",
        "    session = Ort::Session(env, model_path.c_str(), opts);\n",
        "    input_shape = {1, 3, 64, 64};\n",
        "}\n",
        "\n",
        "std::vector<float> InferenceEngine::infer(const std::vector<float>& input) {\n",
        "    auto input_tensor = Ort::Value::CreateTensor<float>(\n",
        "        mem_info,\n",
        "        const_cast<float*>(input.data()),\n",
        "        input.size(),\n",
        "        input_shape.data(),\n",
        "        input_shape.size()\n",
        "    );\n",
        "\n",
        "    const char* input_names[] = {\"input\"};\n",
        "    const char* output_names[] = {\"output\"};\n",
        "\n",
        "    auto outputs = session.Run(\n",
        "        Ort::RunOptions{nullptr},\n",
        "        input_names, &input_tensor, 1,\n",
        "        output_names, 1\n",
        "    );\n",
        "\n",
        "    float* out = outputs[0].GetTensorMutableData<float>();\n",
        "    size_t out_size = outputs[0]\n",
        "        .GetTensorTypeAndShapeInfo()\n",
        "        .GetElementCount();\n",
        "\n",
        "    return std::vector<float>(out, out + out_size);\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vyy7KffoA6ih",
        "outputId": "8e21233d-321c-417d-933f-b32c592f9872"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing engine.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cpp\n",
        "#include <iostream>\n",
        "#include <chrono>\n",
        "#include \"engine.h\"\n",
        "\n",
        "double benchmark(InferenceEngine& engine, const char* tag) {\n",
        "    std::vector<float> input(1*3*64*64, 0.5f);\n",
        "\n",
        "    // 워밍업\n",
        "    engine.infer(input);\n",
        "\n",
        "    auto t0 = std::chrono::high_resolution_clock::now();\n",
        "    auto out = engine.infer(input);\n",
        "    auto t1 = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    std::chrono::duration<double, std::milli> ms = t1 - t0;\n",
        "\n",
        "    std::cout << \"[\" << tag << \"] Time: \"\n",
        "              << ms.count()\n",
        "              << \" ms / First: \"\n",
        "              << out[0] << std::endl;\n",
        "\n",
        "    return ms.count();\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    InferenceEngine fp32(\"swinir_x4.onnx\", 4);\n",
        "    InferenceEngine int8(\"swinir_x4_int8.onnx\", 4);\n",
        "\n",
        "    double t_fp32 = benchmark(fp32, \"FP32\");\n",
        "    double t_int8 = benchmark(int8, \"INT8\");\n",
        "\n",
        "    std::cout << \"Result: \" << \"Faster \" << (1-t_int8/t_fp32)*100 << \"% than Before\";\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSk3NIQDBtG3",
        "outputId": "81f33636-2206-4798-9961-243e41472ca6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ main.cpp engine.cpp \\\n",
        "  -I /content/onnxruntime-linux-x64-1.23.2/include \\\n",
        "  -L /content/onnxruntime-linux-x64-1.23.2/lib \\\n",
        "  -lonnxruntime \\\n",
        "  -std=c++17 -O3 -march=native -o app\n",
        "\n",
        "!LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/content/onnxruntime-linux-x64-1.23.2/lib ./app\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17IsF4tPBwQ9",
        "outputId": "f9c22edd-2730-4a57-a395-29e030158e0c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FP32] Time: 85827.6 ms / First: 0.486768\n",
            "[INT8] Time: 6320.48 ms / First: 0.486675\n",
            "Result: Faster 92.6358% than Before"
          ]
        }
      ]
    }
  ]
}